# ğŸ“ ã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ - Phase 1-6 å­¦ç¿’è¨˜éŒ²è©³ç´°

## ğŸŒŸ å­¦ç¿’æˆæœã‚µãƒãƒªãƒ¼

### **æŠ€è¡“ç¿’å¾—ã®æ·±åº¦**
- **Pythoné«˜åº¦é–‹ç™º**: æ‰‹ç¶šãå‹â†’ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘â†’è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨ã¸ã®æ®µéšçš„æˆé•·
- **Google Cloud Platform**: APIåŸºæœ¬ä½¿ç”¨â†’åˆ¶ç´„ç†è§£â†’å›é¿æŠ€è¡“ç¿’å¾—ã¸ã®æ·±åŒ–
- **ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: å˜ç™ºå‡¦ç†â†’ETLè¨­è¨ˆâ†’å¤§è¦æ¨¡é‹ç”¨ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ç™ºå±•
- **æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ**: ç†è«–å­¦ç¿’â†’å®Ÿè¨¼åˆ†æâ†’æˆ¦ç•¥çš„åˆ¤æ–­ã¸ã®æˆç†Ÿ
- **å•é¡Œè§£æ±ºèƒ½åŠ›**: ãƒˆãƒ©ã‚¤ã‚¢ãƒ³ãƒ‰ã‚¨ãƒ©ãƒ¼â†’ä½“ç³»çš„èª¿æŸ»â†’äºˆé˜²çš„è¨­è¨ˆã¸ã®é€²åŒ–

---

## ğŸ“š Phaseåˆ¥å­¦ç¿’è©³ç´°ãƒ»å¤±æ•—ãƒ»æˆåŠŸä½“é¨“

### **Phase 1: PythonåŸºç¤â†’å¿œç”¨é–‹ç™ºã¸ã®è»¢æ›**

#### **1-1: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘è¨­è¨ˆã®ç†è§£éç¨‹**

**æœ€åˆã®æ··ä¹±ãƒ»ã‚¨ãƒ©ãƒ¼ä½“é¨“**ï¼š
```python
# æœ€åˆã«æ›¸ã„ãŸã‚³ãƒ¼ãƒ‰ï¼ˆæ‰‹ç¶šãå‹ï¼‰
def download_data():
    base_url = "https://www.tepco.co.jp/forecast/html/images"
    # ã™ã¹ã¦ã®å‡¦ç†ã‚’1ã¤ã®é–¢æ•°ã«è©°ã‚è¾¼ã¿
    for month in months:
        url = f"{base_url}/{month}_power_usage.zip"
        # é•·å¤§ãªå‡¦ç†...

# ã‚¨ãƒ©ãƒ¼: å†åˆ©ç”¨ä¸å¯ã€ãƒ†ã‚¹ãƒˆå›°é›£ã€ä¿å®ˆæ€§çš†ç„¡
```

**å­¦ç¿’è»¢æ›ç‚¹**ï¼š
- **ã‚¨ãƒ©ãƒ¼**: åŒã˜å‡¦ç†ã‚’è¤‡æ•°ç®‡æ‰€ã§æ›¸ãç¾½ç›®ã«ãªã‚‹
- **æ°—ã¥ã**: ã€Œä½•åº¦ã‚‚åŒã˜ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã„ã‚‹ã€
- **èª¿æŸ»**: "python class design patterns" ã§ã‚°ã‚°ã‚‹
- **å‚è€ƒ**: Real Python ã® "Object-Oriented Programming (OOP) in Python 3"

**æ”¹è‰¯å¾Œã®ã‚³ãƒ¼ãƒ‰**ï¼š
```python
# å­¦ç¿’å¾Œï¼ˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ï¼‰
class PowerDataDownloader:
    BASE_URL = "https://www.tepco.co.jp/forecast/html/images"
    
    def __init__(self, base_dir="data/raw"):
        self.base_dir = Path(base_dir)
        logger.info(f"PowerDataDownloader initialized with base_dir: {self.base_dir}")
    
    def get_required_months(self, days=5):
        # è²¬ä»»åˆ†é›¢ã•ã‚ŒãŸå°ã•ãªãƒ¡ã‚½ãƒƒãƒ‰
        today = datetime.today()
        dates = [today - timedelta(days=i) for i in range(days + 1)]
        months = {date.strftime('%Y%m') for date in dates}
        return months
    
    def download_month_data(self, yyyymm):
        # å˜ä¸€è²¬ä»»ã®ãƒ¡ã‚½ãƒƒãƒ‰
        url = f"{self.BASE_URL}/{yyyymm}_power_usage.zip"
        # å…·ä½“çš„ãªå‡¦ç†...
        return True
```

**å…·ä½“çš„ãªå­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
1. **æœ€åˆã®ã‚¨ãƒ©ãƒ¼**: é–¢æ•°ãŒ100è¡Œã‚’è¶…ãˆã¦èª­ã‚ãªã„
2. **ã‚°ã‚°ã‚Šæ–¹**: "python function too long how to split"
3. **ç™ºè¦‹**: å˜ä¸€è²¬ä»»åŸå‰‡ã€ã‚¯ãƒ©ã‚¹è¨­è¨ˆã®æ¦‚å¿µ
4. **è©¦è¡ŒéŒ¯èª¤**: ä½•ã‚’ã‚¯ãƒ©ã‚¹ã«ã—ã¦ä½•ã‚’é–¢æ•°ã«ã™ã‚‹ã‹æ‚©ã‚€
5. **åŸºæº–ç¢ºç«‹**: ã€ŒçŠ¶æ…‹ã‚’æŒã¤ã‚‚ã®ã€ã€Œå†åˆ©ç”¨ã™ã‚‹ã‚‚ã®ã€ã¯ã‚¯ãƒ©ã‚¹åŒ–

#### **1-2: self ã®æ¦‚å¿µç†è§£ãƒ»æ··ä¹±ãƒ»ç¿’å¾—**

**æœ€åˆã®æ··ä¹±**ï¼š
```python
# ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚³ãƒ¼ãƒ‰
class MyClass:
    def my_method(self, param):
        return param
    
    def another_method(self):
        # ã‚¨ãƒ©ãƒ¼: self ã‚’å¿˜ã‚Œã¦my_methodã‚’å‘¼ã³å‡ºã—
        result = my_method("test")  # NameError: name 'my_method' is not defined
```

**ã‚¨ãƒ©ãƒ¼è§£æ±ºãƒ—ãƒ­ã‚»ã‚¹**ï¼š
- **ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**: `NameError: name 'my_method' is not defined`
- **æœ€åˆã®èª¤è§£**: "é–¢æ•°åãŒé–“é•ã£ã¦ã„ã‚‹ï¼Ÿ"
- **ã‚°ã‚°ã‚Š**: "python class method not defined error"
- **ç™ºè¦‹**: selfã‚’ä½¿ã£ã¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã¶å¿…è¦ãŒã‚ã‚‹
- **ä¿®æ­£**: `result = self.my_method("test")`

**selfç†è§£ã®æ®µéšçš„æ·±åŒ–**ï¼š
```python
# Stage 1: selfã‚’æ©Ÿæ¢°çš„ã«æ›¸ãï¼ˆç†è§£ä¸ååˆ†ï¼‰
def __init__(self, value):
    self.value = value  # ãªãœselfãŒå¿…è¦ãªã®ã‹åˆ†ã‹ã‚‰ãªã„

# Stage 2: selfã®å½¹å‰²ç†è§£
def __init__(self, value):
    self.value = value  # ã“ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å°‚ç”¨ã®å¤‰æ•°ã¨ç†è§£

# Stage 3: ã‚¯ãƒ©ã‚¹å¤‰æ•°ã¨ã®ä½¿ã„åˆ†ã‘ç†è§£
class PowerDataDownloader:
    BASE_URL = "https://..."  # ã‚¯ãƒ©ã‚¹å¤‰æ•°ï¼ˆå…¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å…±é€šï¼‰
    
    def __init__(self, base_dir):
        self.base_dir = base_dir  # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ï¼ˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å›ºæœ‰ï¼‰
```

**å®Ÿéš›ã«è©¦ã—ãŸå®Ÿé¨“ã‚³ãƒ¼ãƒ‰**ï¼š
```python
# self ã®æŒ™å‹•ç¢ºèªå®Ÿé¨“
class TestSelf:
    class_var = "å…±é€š"
    
    def __init__(self, name):
        self.instance_var = name
    
    def show_vars(self):
        print(f"ã‚¯ãƒ©ã‚¹å¤‰æ•°: {TestSelf.class_var}")
        print(f"ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°: {self.instance_var}")

# å®Ÿé¨“å®Ÿè¡Œ
obj1 = TestSelf("ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ1")
obj2 = TestSelf("ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ2")
obj1.show_vars()  # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ãŒé•ã†ã“ã¨ã‚’ç¢ºèª
obj2.show_vars()
```

#### **1-3: argparseå­¦ç¿’ãƒ»CLIè¨­è¨ˆã®è©¦è¡ŒéŒ¯èª¤**

**æœ€åˆã®å®Ÿè£…ï¼ˆå•é¡Œã‚ã‚Šï¼‰**ï¼š
```python
# æœ€åˆã®ç´ æœ´ãªå®Ÿè£…
import sys

if len(sys.argv) > 1:
    if sys.argv[1] == "--days":
        days = int(sys.argv[2])
    elif sys.argv[1] == "--month":
        month = sys.argv[2]
    # ã‚¨ãƒ©ãƒ¼: å¼•æ•°ãƒ‘ãƒ¼ã‚¹åœ°ç„ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãªã—
```

**å•é¡Œç™ºç”Ÿãƒ»å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
- **å•é¡Œ1**: å¼•æ•°ã®é †ç•ªãŒé•ã†ã¨ã‚¨ãƒ©ãƒ¼
- **å•é¡Œ2**: ãƒ˜ãƒ«ãƒ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒãªã„
- **å•é¡Œ3**: å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼ã®å‡¦ç†ãªã—
- **ã‚°ã‚°ã‚Š**: "python command line arguments best practice"
- **ç™ºè¦‹**: argparseæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

**argparseå­¦ç¿’ã®æ®µéšçš„å®Ÿè£…**ï¼š
```python
# Stage 1: åŸºæœ¬çš„ãªargparse
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--days', type=int)
args = parser.parse_args()
print(args.days)

# å•é¡Œ: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®æ‰±ã„ãŒåˆ†ã‹ã‚‰ãªã„
```

```python
# Stage 2: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãƒ»ãƒ˜ãƒ«ãƒ—è¿½åŠ 
parser = argparse.ArgumentParser(description='æ±äº¬é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼')
parser.add_argument('--days', type=int, default=5, 
                   help='ä»Šæ—¥ã‹ã‚‰é¡ã‚‹æ—¥æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)')
args = parser.parse_args()

# å•é¡Œ: è¤‡æ•°å¼•æ•°ã®æ’ä»–åˆ¶å¾¡æ–¹æ³•ãŒåˆ†ã‹ã‚‰ãªã„
```

```python
# Stage 3: æ’ä»–ãƒã‚§ãƒƒã‚¯å®Ÿè£…ï¼ˆæœ€åˆã®æ–¹æ³•ï¼‰
args = parser.parse_args()

# ç´ æœ´ãªæ’ä»–ãƒã‚§ãƒƒã‚¯ï¼ˆå¾Œã§æ”¹è‰¯ï¼‰
if args.month and args.date:
    print("ã‚¨ãƒ©ãƒ¼: --month ã¨ --date ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
    sys.exit(1)
```

```python
# Stage 4: ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸæ’ä»–ãƒã‚§ãƒƒã‚¯
specified_args = [
    bool(args.month),
    bool(args.date),
    args.days != 5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä»¥å¤–ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
]

if sum(specified_args) > 1:
    print("âŒ ã‚¨ãƒ©ãƒ¼: --days, --month, --date ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
    return

# å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ: sum()ã¨bool()ã®çµ„ã¿åˆã‚ã›ã§æ’ä»–åˆ¶å¾¡
```

**CLIè¨­è¨ˆã§å­¦ã‚“ã æ€æƒ³**ï¼š
- **ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£**: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åˆ†ã‹ã‚Šã‚„ã™ã
- **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œ**: å¼•æ•°ãªã—ã§ã‚‚æ„å‘³ã®ã‚ã‚‹å‹•ä½œ
- **æ‹¡å¼µæ€§**: å°†æ¥ã®å¼•æ•°è¿½åŠ ã‚’è€ƒæ…®ã—ãŸè¨­è¨ˆ

#### **1-4: Pathã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå­¦ç¿’ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã®è¿‘ä»£åŒ–**

**å¤ã„æ–¹æ³•ï¼ˆæœ€åˆã«æ›¸ã„ãŸã‚³ãƒ¼ãƒ‰ï¼‰**ï¼š
```python
import os

# å¤ã„os.pathæ–¹å¼ï¼ˆæœ€åˆã®å®Ÿè£…ï¼‰
base_dir = "data/raw"
month_dir = os.path.join(base_dir, "202505")
if not os.path.exists(month_dir):
    os.makedirs(month_dir)

zip_path = os.path.join(month_dir, "202505.zip")

# å•é¡Œ: ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ä¾å­˜ã€å¯èª­æ€§ä½ä¸‹
```

**Pathã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç™ºè¦‹ãƒ»å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
- **ãã£ã‹ã‘**: Windowsã¨Linuxã§ãƒ‘ã‚¹åŒºåˆ‡ã‚Šæ–‡å­—ãŒé•ã†ã‚¨ãƒ©ãƒ¼
- **ã‚¨ãƒ©ãƒ¼**: `FileNotFoundError` (Windowsç’°å¢ƒã§ã®\ã¨/æ··åœ¨)
- **ã‚°ã‚°ã‚Š**: "python cross platform file path"
- **ç™ºè¦‹**: pathlibãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- **å­¦ç¿’ã‚½ãƒ¼ã‚¹**: Pythonå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ + Real Python pathlib tutorial

**Pathã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ®µéšçš„ç¿’å¾—**ï¼š
```python
# Stage 1: åŸºæœ¬çš„ãªç½®ãæ›ãˆ
from pathlib import Path

base_dir = Path("data/raw")
month_dir = base_dir / "202505"  # ã‚¹ãƒ©ãƒƒã‚·ãƒ¥æ¼”ç®—å­ã«æ„Ÿå‹•
month_dir.mkdir(parents=True, exist_ok=True)

# Stage 2: ä¾¿åˆ©ãƒ¡ã‚½ãƒƒãƒ‰ç™ºè¦‹
zip_path = month_dir / "202505.zip"
if zip_path.exists():  # os.path.exists ã‚ˆã‚Šèª­ã¿ã‚„ã™ã„
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {zip_path.stat().st_size}")

# Stage 3: globæ©Ÿèƒ½æ´»ç”¨
csv_files = month_dir.glob("*.csv")
for csv_file in csv_files:
    print(f"å‡¦ç†ä¸­: {csv_file.name}")  # .nameã§è‡ªå‹•çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿
```

**å®Ÿéš›ã«è©¦ã—ã¦å­¦ã‚“ã Pathæ©Ÿèƒ½**ï¼š
```python
# å®Ÿé¨“ã—ã¦å­¦ã‚“ã ä¾¿åˆ©æ©Ÿèƒ½
path = Path("data/raw/202505/20250501_power_usage.csv")

print(f"è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {path.parent}")        # data/raw/202505
print(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {path.name}")               # 20250501_power_usage.csv
print(f"æ‹¡å¼µå­ãªã—: {path.stem}")               # 20250501_power_usage
print(f"æ‹¡å¼µå­: {path.suffix}")                 # .csv
print(f"çµ¶å¯¾ãƒ‘ã‚¹: {path.absolute()}")

# è¤‡æ•°æ‹¡å¼µå­ã®å ´åˆ
tar_path = Path("backup.tar.gz")
print(f"æœ€å¾Œã®æ‹¡å¼µå­: {tar_path.suffix}")      # .gz
print(f"å…¨æ‹¡å¼µå­: {tar_path.suffixes}")        # ['.tar', '.gz']
```

#### **1-5: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¨­è¨ˆæ€æƒ³ã®ç™ºé”**

**æœ€åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆå•é¡Œã‚ã‚Šï¼‰**ï¼š
```python
# æœ€åˆã®ç´ æœ´ãªå®Ÿè£…
try:
    response = requests.get(url)
    with open(file_path, 'wb') as f:
        f.write(response.content)
except:
    print("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")  # å•é¡Œ: ä½•ã®ã‚¨ãƒ©ãƒ¼ã‹åˆ†ã‹ã‚‰ãªã„
```

**ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å­¦ç¿’ã®è»¢æ›ç‚¹**ï¼š
- **å•é¡Œç™ºç”Ÿ**: HTTP 404ã‚¨ãƒ©ãƒ¼ã¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚’åŒºåˆ¥ã§ããªã„
- **å®Ÿéš›ã®ã‚¨ãƒ©ãƒ¼**: `requests.exceptions.HTTPError: 404 Client Error`
- **å­¦ç¿’**: ä¾‹å¤–ã®ç¨®é¡ã¨éšå±¤æ§‹é€ 
- **èª¿æŸ»**: Pythonå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ "Built-in Exceptions"

**æ®µéšçš„æ”¹è‰¯ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
```python
# Stage 1: å…·ä½“çš„ãªä¾‹å¤–ã‚­ãƒ£ãƒƒãƒ
try:
    response = requests.get(url)
    response.raise_for_status()  # HTTPã‚¨ãƒ©ãƒ¼ã‚’ä¾‹å¤–ã«å¤‰æ›
except requests.exceptions.HTTPError as e:
    print(f"HTTPã‚¨ãƒ©ãƒ¼: {e}")
except requests.exceptions.ConnectionError as e:
    print(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
```

```python
# Stage 2: ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã«å¿œã˜ãŸåˆ†å²
try:
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    return True
except requests.exceptions.HTTPError as e:
    if e.response.status_code == 404:
        logger.warning(f"Data for {yyyymm} not yet available (404)")
        return False  # 404ã¯æ­£å¸¸ãªã‚±ãƒ¼ã‚¹ï¼ˆãƒ‡ãƒ¼ã‚¿æœªå…¬é–‹ï¼‰
    else:
        logger.error(f"HTTP error downloading {yyyymm}: {e}")
        raise  # ãã®ä»–ã®HTTPã‚¨ãƒ©ãƒ¼ã¯å†ç™ºç”Ÿ
except requests.exceptions.Timeout:
    logger.error(f"Timeout downloading {yyyymm}")
    raise
except requests.exceptions.ConnectionError as e:
    logger.error(f"Connection error downloading {yyyymm}: {e}")
    raise
```

```python
# Stage 3: ãƒ­ã‚°çµ±åˆãƒ»é‹ç”¨è€ƒæ…®
try:
    logger.info(f"Downloading: {url}")
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    
    with open(zip_path, 'wb') as f:
        f.write(response.content)
    
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(month_dir)
    
    logger.info(f"Successfully downloaded and extracted {yyyymm} data")
    return True
    
except requests.exceptions.HTTPError as e:
    if e.response.status_code == 404:
        logger.warning(f"Data for {yyyymm} not yet available (404)")
        return False
    else:
        logger.error(f"HTTP error downloading {yyyymm}: {e}")
        raise
except zipfile.BadZipFile as e:
    logger.error(f"Corrupted ZIP file for {yyyymm}: {e}")
    if zip_path.exists():
        zip_path.unlink()  # ç ´æZIPãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    raise
except Exception as e:
    logger.error(f"Unexpected error downloading {yyyymm}: {e}")
    raise
```

**ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¨­è¨ˆã§å­¦ã‚“ã åŸå‰‡**ï¼š
1. **ä¾‹å¤–ã®ç²’åº¦**: é©åˆ‡ãªãƒ¬ãƒ™ãƒ«ã§ã‚­ãƒ£ãƒƒãƒ
2. **ãƒ­ã‚°ã¨ã®é€£æº**: ã‚¨ãƒ©ãƒ¼ã‚‚é‡è¦ãªæƒ…å ±
3. **å¾©æ—§å¯èƒ½æ€§**: ç¶šè¡Œå¯èƒ½ãªã‚¨ãƒ©ãƒ¼ã¨è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã®åŒºåˆ¥
4. **ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£**: æŠ€è¡“çš„ã‚¨ãƒ©ãƒ¼ã‚’åˆ†ã‹ã‚Šã‚„ã™ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¤‰æ›

---

### **Phase 2: Google Cloud Platformçµ±åˆã§ã®åˆ¶ç´„ã¨ã®æ ¼é—˜**

#### **2-1: BigQuery APIåˆå­¦ç¿’ãƒ»èªè¨¼ã®å£**

**æœ€åˆã®èªè¨¼ã‚¨ãƒ©ãƒ¼åœ°ç„**ï¼š
```python
# æœ€åˆã®ã‚³ãƒ¼ãƒ‰ï¼ˆèªè¨¼ãªã—ï¼‰
from google.cloud import bigquery

client = bigquery.Client()  # ã‚¨ãƒ©ãƒ¼: DefaultCredentialsError
```

**èªè¨¼ã‚¨ãƒ©ãƒ¼è§£æ±ºãƒ—ãƒ­ã‚»ã‚¹**ï¼š
- **ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**: `google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials`
- **æœ€åˆã®èª¤è§£**: "ã‚³ãƒ¼ãƒ‰ãŒé–“é•ã£ã¦ã„ã‚‹ï¼Ÿ"
- **ã‚°ã‚°ã‚Š1**: "bigquery DefaultCredentialsError"
- **ç™ºè¦‹**: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãŒå¿…è¦
- **ã‚°ã‚°ã‚Š2**: "google cloud service account key setup"
- **å®Ÿè¡Œ**: GCPã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆãƒ»ã‚­ãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

**èªè¨¼è¨­å®šã®è©¦è¡ŒéŒ¯èª¤**ï¼š
```python
# è©¦è¡Œ1: ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆæ‰‹å‹•ï¼‰
import os
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/key.json'

# å•é¡Œ: ãƒ‘ã‚¹ã®æ›¸ãæ–¹ã§ãƒãƒã‚‹ï¼ˆWindowsã®ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ï¼‰
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\Users\...\key.json'  # ã‚¨ãƒ©ãƒ¼
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\Users\\...\\key.json'  # æˆåŠŸ

# è©¦è¡Œ2: ã‚³ãƒ¼ãƒ‰å†…ã§ã®ç›´æ¥æŒ‡å®š
client = bigquery.Client.from_service_account_json('path/to/key.json')

# è©¦è¡Œ3: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDæ˜ç¤ºæŒ‡å®šï¼ˆæœ€çµ‚çš„ãªå®Ÿè£…ï¼‰
client = bigquery.Client(project='energy-env')
```

**æ¨©é™ã‚¨ãƒ©ãƒ¼ã¨ã®æ ¼é—˜**ï¼š
```python
# æ¨©é™ã‚¨ãƒ©ãƒ¼ä¾‹
# google.api_core.exceptions.Forbidden: 403 Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.

# è§£æ±ºãƒ—ãƒ­ã‚»ã‚¹:
# 1. IAMã§BigQueryç®¡ç†è€…ãƒ­ãƒ¼ãƒ«è¿½åŠ 
# 2. BigQuery APIã®æœ‰åŠ¹åŒ–ç¢ºèª
# 3. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®å†ä½œæˆ
```

#### **2-2: BigQueryå‹ã‚·ã‚¹ãƒ†ãƒ ç†è§£ãƒ»åˆ¶ç´„ç™ºè¦‹**

**æœ€åˆã®å‹ã‚¨ãƒ©ãƒ¼**ï¼š
```sql
-- æœ€åˆã«æ›¸ã„ãŸã‚¯ã‚¨ãƒªï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
SELECT *
FROM energy_data_hourly e
JOIN weather_data w
  ON e.date = w.date 
  AND e.hour = w.hour  -- ã‚¨ãƒ©ãƒ¼: Cannot compare INT64 with STRING
```

**å‹ã‚¨ãƒ©ãƒ¼è§£æ±ºã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
- **ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**: `No matching signature for operator = for argument types: INT64, STRING`
- **æœ€åˆã®æ··ä¹±**: "åŒã˜hourãªã®ã«ãªãœã‚¨ãƒ©ãƒ¼ï¼Ÿ"
- **èª¿æŸ»**: BigQueryã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒç¢ºèª
- **ç™ºè¦‹**: energy_data.hour ã¯ INTEGERã€weather_data.hour ã¯ STRING
- **ã‚°ã‚°ã‚Š**: "bigquery join different types cast"

**å‹å¤‰æ›å­¦ç¿’ã®æ®µéšçš„é€²åŒ–**ï¼š
```sql
-- Stage 1: åŸºæœ¬çš„ãªå‹å¤‰æ›
SELECT *
FROM energy_data_hourly e
JOIN weather_data w
  ON e.date = w.date 
  AND e.hour = CAST(w.hour AS INT64)  -- å‹å¤‰æ›ã§è§£æ±º

-- Stage 2: ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå‹å¤‰æ›
SELECT *
FROM energy_data_hourly e
JOIN weather_data w
  ON e.date = w.date 
  AND CAST(e.hour AS STRING) = w.hour  -- INTEGERã‚’STRINGã«å¤‰æ›

-- Stage 3: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è€ƒæ…®
-- çµè«–: INTEGERã‚’STRINGã«å¤‰æ›ã™ã‚‹ã‚ˆã‚Šã€STRINGã‚’INTEGERã«å¤‰æ›ã™ã‚‹æ–¹ãŒåŠ¹ç‡çš„
```

**LOAD DATAå‹æ¨è«–ã®ç™ºè¦‹**ï¼š
```sql
-- æ‰‹å‹•INSERTï¼ˆå‹ã‚’æ˜ç¤ºæŒ‡å®šï¼‰
CREATE TABLE test_table (
  date DATE,
  hour INTEGER,
  value FLOAT64
);

-- LOAD DATAï¼ˆè‡ªå‹•å‹æ¨è«–ï¼‰
LOAD DATA INTO `energy-env.dev_energy_data.energy_data_hourly`
FROM FILES (
  format = 'CSV',
  uris = ['gs://bucket/*.csv'],
  skip_leading_rows = 1
);
-- BigQueryãŒè‡ªå‹•çš„ã«æœ€é©ãªå‹ã‚’é¸æŠ
```

**å‹çµ±ä¸€å•é¡Œã®æ ¹æœ¬è§£æ±ºæˆ¦ç•¥**ï¼š
- **è¨­è¨ˆæ–¹é‡**: æ•°å€¤ã¯ INTEGER ã§çµ±ä¸€ä¿å­˜
- **JOINæ™‚å¤‰æ›**: `CAST(string_column AS INTEGER)`
- **è¡¨ç¤ºæ™‚å¤‰æ›**: `LPAD(CAST(integer_column AS STRING), 2, '0')`
- **åˆ©ç‚¹**: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŠ¹ç‡ + è¨ˆç®—åŠ¹ç‡ + è¡¨ç¤ºæŸ”è»Ÿæ€§

#### **2-3: BigQueryåˆ¶ç´„ç™ºè¦‹ãƒ»å›é¿æŠ€è¡“ç¿’å¾—**

**è¤‡æ•°ã‚«ãƒ©ãƒ INå¥åˆ¶ç´„ã¨ã®é­é‡**ï¼š
```sql
-- æœ€åˆã«æ›¸ã„ãŸã‚¯ã‚¨ãƒªï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
DELETE FROM weather_data 
WHERE (prefecture, date, hour) IN (
    SELECT prefecture, date, hour 
    FROM temp_weather_external
);
-- ã‚¨ãƒ©ãƒ¼: Subquery with multiple columns is not supported in IN predicate
```

**åˆ¶ç´„å›é¿ã®è©¦è¡ŒéŒ¯èª¤ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
```sql
-- è©¦è¡Œ1: EXISTSæ–¹å¼
DELETE FROM weather_data w1
WHERE EXISTS (
    SELECT 1 
    FROM temp_weather_external w2
    WHERE w1.prefecture = w2.prefecture
      AND w1.date = w2.date
      AND w1.hour = w2.hour
);
-- çµæœ: å‹•ä½œã™ã‚‹ãŒã€è¤‡é›‘

-- è©¦è¡Œ2: JOINæ–¹å¼
DELETE w1
FROM weather_data w1
JOIN temp_weather_external w2
  ON w1.prefecture = w2.prefecture
     AND w1.date = w2.date
     AND w1.hour = w2.hour;
-- ã‚¨ãƒ©ãƒ¼: DELETE with JOIN is not supported

-- è©¦è¡Œ3: CONCATæ–¹å¼ï¼ˆæœ€çµ‚è§£æ±ºï¼‰
DELETE FROM weather_data 
WHERE CONCAT(prefecture, '|', CAST(date AS STRING), '|', hour) IN (
    SELECT CONCAT(prefecture, '|', CAST(date AS STRING), '|', hour)
    FROM temp_weather_external
);
-- æˆåŠŸ: è¤‡åˆã‚­ãƒ¼ã‚’æ–‡å­—åˆ—çµåˆã§è¡¨ç¾
```

**CONCATæ–¹å¼é¸æŠã®å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ**ï¼š
- **åŒºåˆ‡ã‚Šæ–‡å­—é¸æŠ**: '|' ã‚’é¸æŠï¼ˆãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„æ–‡å­—ï¼‰
- **å‹çµ±ä¸€**: `CAST(date AS STRING)` ã§çµ±ä¸€
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: æ–‡å­—åˆ—çµåˆã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯ã‚ã‚‹ãŒã€æ©Ÿèƒ½æ€§ã‚’å„ªå…ˆ

**WITHå¥+UPDATEåˆ¶ç´„ã®ç™ºè¦‹**ï¼š
```sql
-- BigQueryã§è©¦ã—ãŸã‚¯ã‚¨ãƒªï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
WITH business_days AS (
  SELECT date, ROW_NUMBER() OVER (ORDER BY date) as number
  FROM calendar_data WHERE is_holiday = false
)
UPDATE calendar_data 
SET business_day_number = bd.number
FROM business_days bd 
WHERE calendar_data.date = bd.date;
-- ã‚¨ãƒ©ãƒ¼: WITH clause is not supported in UPDATE statements
```

**åˆ¶ç´„å›é¿: ç”ŸSQLå®Ÿè¡Œ**ï¼š
```python
# Python APIã®åˆ¶ç´„å›é¿
sql_string = """
UPDATE `energy-env.dev_energy_data.calendar_data`
SET business_day_number = (
  SELECT COUNT(*)
  FROM `energy-env.dev_energy_data.calendar_data` c2
  WHERE c2.is_holiday = false 
    AND c2.is_weekend = false
    AND c2.date <= calendar_data.date
)
WHERE is_holiday = false AND is_weekend = false;
"""

job = client.query(sql_string)  # ç”ŸSQLæ–‡å­—åˆ—å®Ÿè¡Œ
job.result()
```

**MERGEæ–‡ã®ç™ºè¦‹ãƒ»æ´»ç”¨**ï¼š
```sql
-- è¤‡é›‘ãªUPDATEæ–‡ã®ä»£æ›¿: MERGEæ–‡
MERGE `energy-env.dev_energy_data.calendar_data` AS target
USING (
  SELECT 
    date,
    ROW_NUMBER() OVER (ORDER BY date) as business_day_number
  FROM `energy-env.dev_energy_data.calendar_data`
  WHERE is_holiday = false AND is_weekend = false
) AS source
ON target.date = source.date
WHEN MATCHED THEN
  UPDATE SET business_day_number = source.business_day_number;
-- BigQueryã§æ¨å¥¨ã•ã‚Œã‚‹åŠ¹ç‡çš„ãªæ›´æ–°æ–¹æ³•
```

#### **2-4: EXTERNAL TABLE vs ç‰©ç†ãƒ†ãƒ¼ãƒ–ãƒ«è¨­è¨ˆåˆ¤æ–­**

**EXTERNAL TABLEå­¦ç¿’ã®çµŒç·¯**ï¼š
- **æœ€åˆã®æ–¹æ³•**: ç›´æ¥INSERTæ–‡ã§ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
- **å•é¡Œ**: å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½ä½ä¸‹
- **èª¿æŸ»**: "bigquery bulk insert performance"
- **ç™ºè¦‹**: EXTERNAL TABLE â†’ ç‰©ç†ãƒ†ãƒ¼ãƒ–ãƒ«æŠ•å…¥ãƒ‘ã‚¿ãƒ¼ãƒ³

**EXTERNAL TABLEæ´»ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…**ï¼š
```sql
-- Step 1: EXTERNAL TABLEä½œæˆ
CREATE OR REPLACE EXTERNAL TABLE `energy-env.dev_energy_data.temp_weather_external`
(
    prefecture STRING,
    date STRING,
    hour STRING,
    temperature_2m FLOAT64,
    relative_humidity_2m FLOAT64,
    precipitation FLOAT64,
    weather_code INT64
)
OPTIONS (
    format = 'CSV',
    uris = ['gs://energy-env-data/weather_processed/historical/*.csv'],
    skip_leading_rows = 1
);

-- Step 2: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
SELECT COUNT(*), MIN(date), MAX(date)
FROM `energy-env.dev_energy_data.temp_weather_external`;

-- Step 3: ç‰©ç†ãƒ†ãƒ¼ãƒ–ãƒ«ã«æŠ•å…¥
INSERT INTO `energy-env.dev_energy_data.weather_data`
SELECT 
    prefecture,
    PARSE_DATE('%Y-%m-%d', date) as date,
    CAST(hour AS INTEGER) as hour,
    temperature_2m,
    relative_humidity_2m,
    precipitation,
    weather_code,
    CURRENT_TIMESTAMP() as created_at
FROM `energy-env.dev_energy_data.temp_weather_external`;

-- Step 4: EXTERNAL TABLEå‰Šé™¤
DROP TABLE `energy-env.dev_energy_data.temp_weather_external`;
```

**è¨­è¨ˆåˆ¤æ–­ã®å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ**ï¼š
- **é–‹ç™ºæ™‚**: EXTERNAL TABLEï¼ˆæŸ”è»Ÿæ€§é‡è¦–ï¼‰
- **æœ¬ç•ªé‹ç”¨**: ç‰©ç†ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ€§èƒ½é‡è¦–ï¼‰
- **ETLãƒ—ãƒ­ã‚»ã‚¹**: EXTERNAL TABLE â†’ å¤‰æ› â†’ ç‰©ç†ãƒ†ãƒ¼ãƒ–ãƒ«

---

### **Phase 3: ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ã®æ ¼é—˜**

#### **3-1: Shift-JISåœ°ç„ã¨ã®æ ¼é—˜**

**æœ€åˆã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼**ï¼š
```python
# æœ€åˆã®ã‚³ãƒ¼ãƒ‰ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
with open(csv_path, 'r') as f:
    content = f.read()
# UnicodeDecodeError: 'utf-8' codec can't decode byte 0x82 in position 15: invalid start byte
```

**ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œè§£æ±ºã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
- **ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**: `UnicodeDecodeError: 'utf-8' codec can't decode byte 0x82`
- **æœ€åˆã®æ¨æ¸¬**: "ãƒ•ã‚¡ã‚¤ãƒ«ãŒå£Šã‚Œã¦ã„ã‚‹ï¼Ÿ"
- **ã‚°ã‚°ã‚Š1**: "python UnicodeDecodeError 0x82"
- **ç™ºè¦‹**: æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œ
- **ã‚°ã‚°ã‚Š2**: "æ±äº¬é›»åŠ› CSV ãƒ•ã‚¡ã‚¤ãƒ« ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"
- **æ¨æ¸¬**: Shift-JISã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å¯èƒ½æ€§

**ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã®è©¦è¡ŒéŒ¯èª¤**ï¼š
```python
# è©¦è¡Œ1: Shift-JISæŒ‡å®š
with open(csv_path, 'r', encoding='shift-jis') as f:
    content = f.read()
# æˆåŠŸ! æ—¥æœ¬èªãŒæ­£ã—ãè¡¨ç¤º

# è©¦è¡Œ2: è‡ªå‹•æ¤œå‡ºï¼ˆchardetãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰
import chardet

with open(csv_path, 'rb') as f:
    raw_data = f.read()
    
detected = chardet.detect(raw_data)
print(f"æ¤œå‡ºã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected['encoding']}")  # shift_jis
print(f"ç¢ºä¿¡åº¦: {detected['confidence']}")              # 0.99

# è©¦è¡Œ3: è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œ
def read_csv_with_encoding(file_path):
    encodings = ['shift-jis', 'cp932', 'utf-8', 'iso-2022-jp']
    
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                return f.read(), encoding
        except UnicodeDecodeError:
            continue
    
    raise ValueError(f"Could not decode {file_path} with any encoding")
```

**æœ€çµ‚çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†å®Ÿè£…**ï¼š
```python
def _process_raw_csv_to_hourly(self, input_csv_path):
    try:
        # ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³: Shift-JIS
        with open(input_csv_path, 'r', encoding='shift-jis') as f:
            content = f.read()
    except UnicodeDecodeError:
        try:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯1: CP932
            with open(input_csv_path, 'r', encoding='cp932') as f:
                content = f.read()
        except UnicodeDecodeError:
            try:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯2: UTF-8
                with open(input_csv_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError as e:
                logger.error(f"Could not decode {input_csv_path}: {e}")
                raise
    
    # UTF-8ã§å‡ºåŠ›ï¼ˆçµ±ä¸€ï¼‰
    with open(output_csv_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(output_lines))
```

#### **3-2: æ±é›»CSVæ§‹é€ è§£æãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜å­¦ç¿’**

**æœ€åˆã®CSVè§£æï¼ˆå¤±æ•—ï¼‰**ï¼š
```python
# æœ€åˆã®ç´ æœ´ãªå®Ÿè£…
with open(csv_path, 'r', encoding='shift-jis') as f:
    lines = f.readlines()
    for line in lines:
        if 'DATE' in line:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¦‹ã¤ã‘ãŸï¼
            data = line.split(',')
            # å•é¡Œ: ãƒ‡ãƒ¼ã‚¿è¡Œã¨ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®åŒºåˆ¥ãŒã§ããªã„
```

**æ±é›»CSVæ§‹é€ åˆ†æã®æ®µéšçš„ç†è§£**ï¼š
```python
# Stage 1: ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã®æ‰‹å‹•èª¿æŸ»
# å®Ÿéš›ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ã§é–‹ã„ã¦æ§‹é€ åˆ†æ

# ç™ºè¦‹ã—ãŸæ§‹é€ :
# - æœ€åˆã®æ•°è¡Œ: ãƒ¡ã‚¿æƒ…å ±ï¼ˆä¼šç¤¾åã€ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ç­‰ï¼‰
# - ä¸­é–“: ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ "DATE,TIME,å½“æ—¥å®Ÿç¸¾(ä¸‡kW),äºˆæ¸¬å€¤(ä¸‡kW),..."
# - ãã®å¾Œ: 24è¡Œã®ãƒ‡ãƒ¼ã‚¿ï¼ˆ00:00-23:00ï¼‰

# Stage 2: ãƒ˜ãƒƒãƒ€ãƒ¼æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³æ”¹è‰¯
header_patterns = [
    'DATE,TIME,å½“æ—¥å®Ÿç¸¾(ä¸‡kW)',      # é€šå¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³
    'DATE,TIME,å½“æ—¥å®Ÿç¸¾ï¼ˆä¸‡kWï¼‰',    # å…¨è§’æ‹¬å¼§ãƒ‘ã‚¿ãƒ¼ãƒ³
    'DATE,TIME,å®Ÿç¸¾(ä¸‡kW)',          # çŸ­ç¸®ãƒ‘ã‚¿ãƒ¼ãƒ³
]

header_line_index = -1
for i, line in enumerate(lines):
    for pattern in header_patterns:
        if pattern in line:
            header_line_index = i
            break
    if header_line_index != -1:
        break

# Stage 3: ãƒ‡ãƒ¼ã‚¿è¡Œã®ç¯„å›²ç‰¹å®š
# ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®æ¬¡è¡Œã‹ã‚‰24è¡ŒãŒãƒ‡ãƒ¼ã‚¿ã¨åˆ¤æ˜
data_start = header_line_index + 1
data_end = min(data_start + 24, len(lines))

for i in range(data_start, data_end):
    line = lines[i].strip()
    if not line:  # ç©ºè¡Œã‚¹ã‚­ãƒƒãƒ—
        continue
    
    parts = line.split(',')
    if len(parts) >= 6:  # æœ€ä½é™ã®åˆ—æ•°ãƒã‚§ãƒƒã‚¯
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†...
```

**æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤šæ§˜æ€§ã¨ã®æ ¼é—˜**ï¼š
```python
# ç™ºè¦‹ã—ãŸæ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ç¨®é¡
date_formats = [
    "2025/4/1",      # æœˆæ—¥ãŒ1æ¡
    "2025/04/01",    # æœˆæ—¥ãŒ2æ¡
    "2025-04-01",    # ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Š
    "20250401",      # åŒºåˆ‡ã‚Šãªã—8æ¡
]

# çµ±ä¸€å‡¦ç†ã®å®Ÿè£…
def normalize_date_format(date_str):
    date_str = date_str.strip()
    
    if '/' in date_str:
        # "2025/4/1" â†’ "2025-04-01"
        parts = date_str.split('/')
        if len(parts) == 3:
            year = parts[0]
            month = parts[1].zfill(2)  # 1æ¡â†’2æ¡å¤‰æ›
            day = parts[2].zfill(2)
            return f"{year}-{month}-{day}"
    
    elif '-' in date_str:
        # "2025-04-01" â†’ ãã®ã¾ã¾
        return date_str
    
    elif len(date_str) == 8 and date_str.isdigit():
        # "20250401" â†’ "2025-04-01"
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
    
    else:
        # ãã®ä»–ã®å½¢å¼ã¯ãã®ã¾ã¾
        return date_str
```

**æ™‚åˆ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ­£è¦åŒ–ã®å­¦ç¿’**ï¼š
```python
# æ™‚åˆ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ç¨®é¡ã¨å‡¦ç†
def normalize_time_format(time_str):
    time_str = time_str.strip()
    
    if ':' in time_str:
        # "13:00" â†’ 13
        hour = int(time_str.split(':')[0])
    elif time_str.isdigit():
        # "13" â†’ 13
        hour = int(time_str)
    else:
        raise ValueError(f"Unknown time format: {time_str}")
    
    # 2æ¡ã‚¼ãƒ­åŸ‹ã‚æ–‡å­—åˆ—ã«å¤‰æ›
    return str(hour).zfill(2)

# å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
test_cases = [
    ("13:00", "13"),
    ("9:00", "09"),
    ("23:00", "23"),
    ("0:00", "00"),
]

for input_time, expected in test_cases:
    result = normalize_time_format(input_time)
    assert result == expected, f"Failed: {input_time} â†’ {result}, expected {expected}"
```

#### **3-3: æ•°å€¤ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»ç•°å¸¸å€¤æ¤œå‡ºå­¦ç¿’**

**æœ€åˆã®æ•°å€¤å¤‰æ›ã‚¨ãƒ©ãƒ¼**ï¼š
```python
# æœ€åˆã®ã‚³ãƒ¼ãƒ‰ï¼ˆã‚¨ãƒ©ãƒ¼å¤šç™ºï¼‰
actual_power = float(parts[2])  # ValueError: could not convert string to float: '---'
supply_capacity = float(parts[5])  # ValueError: could not convert string to float: ''
```

**ç•°å¸¸å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç™ºè¦‹ãƒ»å¯¾å¿œå­¦ç¿’**ï¼š
```python
# ç™ºè¦‹ã—ãŸç•°å¸¸å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
abnormal_patterns = [
    '---',      # ãƒ‡ãƒ¼ã‚¿ãªã—è¡¨ç¾
    '',         # ç©ºæ–‡å­—åˆ—
    'N/A',      # æ¬ æå€¤è¡¨ç¾
    '999999',   # æ˜ã‚‰ã‹ã«ç•°å¸¸ãªå€¤
    '-1',       # è² ã®å€¤ï¼ˆé›»åŠ›ãƒ‡ãƒ¼ã‚¿ã§ã¯ç•°å¸¸ï¼‰
]

# ç•°å¸¸å€¤å¯¾å¿œã®æ®µéšçš„å®Ÿè£…
def safe_float_conversion(value_str, column_name, line_number):
    value_str = value_str.strip()
    
    # Stage 1: åŸºæœ¬çš„ãªç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
    if value_str in ['---', '', 'N/A', 'null']:
        logger.warning(f"Missing value in {column_name} at line {line_number}: '{value_str}'")
        return None
    
    try:
        value = float(value_str)
    except ValueError as e:
        logger.warning(f"Cannot convert to float in {column_name} at line {line_number}: '{value_str}'")
        return None
    
    # Stage 2: ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯æ¤œè¨¼
    if column_name in ['actual_power', 'supply_capacity']:
        if value < 0:
            logger.warning(f"Negative {column_name} at line {line_number}: {value}")
            return None
        
        if value > 100000:  # 10ä¸‡kWè¶…ã¯ç•°å¸¸
            logger.warning(f"Abnormally high {column_name} at line {line_number}: {value}")
            return None
    
    return value

# å®Ÿéš›ã®ä½¿ç”¨ä¾‹
try:
    actual_power = safe_float_conversion(parts[2], 'actual_power', i)
    supply_capacity = safe_float_conversion(parts[5], 'supply_capacity', i)
    
    # Noneå€¤ï¼ˆç•°å¸¸å€¤ï¼‰ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if actual_power is None or supply_capacity is None:
        continue
    
    # ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯æ¤œè¨¼
    if actual_power > supply_capacity * 1.1:  # 10%ãƒãƒ¼ã‚¸ãƒ³
        logger.warning(f"Actual power exceeds supply at line {i}: {actual_power} > {supply_capacity}")
        # ç¶šè¡Œï¼ˆè­¦å‘Šã®ã¿ï¼‰
    
    # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã¿å‡ºåŠ›
    output_line = f"{formatted_date},{hour_str},{actual_power},{supply_capacity}"
    output_lines.append(output_line)
    processed_count += 1
    
except Exception as e:
    logger.error(f"Error processing line {i}: {line} - {e}")
    continue
```

---

### **Phase 4: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»APIçµ±åˆå­¦ç¿’**

#### **4-1: Open-Meteo APIç†è§£ãƒ»åˆ¶é™ã¨ã®æ ¼é—˜**

**æœ€åˆã®APIå‘¼ã³å‡ºã—ï¼ˆå¤±æ•—ï¼‰**ï¼š
```python
# æœ€åˆã®ç´ æœ´ãªå®Ÿè£…
import requests

url = "https://api.open-meteo.com/v1/forecast"
params = {
    'latitude': 35.6762,
    'longitude': 139.6503,
    'hourly': 'temperature_2m,relative_humidity_2m,precipitation,weather_code'
}

response = requests.get(url, params=params)
data = response.json()

# å•é¡Œ1: æœŸé–“æŒ‡å®šãªã—ã§7æ—¥åˆ†ã—ã‹å–å¾—ã§ããªã„
# å•é¡Œ2: éå»ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„ï¼ˆforecastã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼‰
```

**APIåˆ¶é™ç™ºè¦‹ãƒ»å›é¿ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
- **å•é¡Œç™ºè¦‹**: 1å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å…¨æœŸé–“å–å¾—ä¸å¯
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª¿æŸ»**: Open-Meteo API documentation
- **ç™ºè¦‹1**: historical ã¨ forecast ã®åˆ¥ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
- **ç™ºè¦‹2**: æœŸé–“åˆ¶é™ã‚ã‚Šï¼ˆhistorical: æœ€å¤§1å¹´ã€forecast: 16æ—¥ï¼‰

**APIåˆ¶é™å›é¿ã®æ®µéšçš„å®Ÿè£…**ï¼š
```python
# Stage 1: ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆåˆ†é›¢
def get_historical_weather(lat, lon, start_date, end_date):
    url = "https://archive-api.open-meteo.com/v1/archive"
    params = {
        'latitude': lat,
        'longitude': lon,
        'start_date': start_date,  # 'YYYY-MM-DD'
        'end_date': end_date,
        'hourly': 'temperature_2m,relative_humidity_2m,precipitation,weather_code'
    }
    return requests.get(url, params=params)

def get_forecast_weather(lat, lon):
    url = "https://api.open-meteo.com/v1/forecast"
    params = {
        'latitude': lat,
        'longitude': lon,
        'hourly': 'temperature_2m,relative_humidity_2m,precipitation,weather_code'
    }
    return requests.get(url, params=params)

# Stage 2: æœŸé–“åˆ†å‰²å‡¦ç†
def download_weather_data_with_pagination(lat, lon, start_date, end_date):
    from datetime import datetime, timedelta
    import calendar
    
    current_date = datetime.strptime(start_date, '%Y-%m-%d')
    end_date_dt = datetime.strptime(end_date, '%Y-%m-%d')
    
    all_data = []
    
    while current_date <= end_date_dt:
        # æœˆå˜ä½ã§åˆ†å‰²ï¼ˆAPIåˆ¶é™å›é¿ï¼‰
        year = current_date.year
        month = current_date.month
        
        # æœˆã®æœ€çµ‚æ—¥å–å¾—
        last_day = calendar.monthrange(year, month)[1]
        month_end_date = datetime(year, month, last_day)
        
        # æœŸé–“è¨­å®š
        period_start = current_date.strftime('%Y-%m-%d')
        period_end = min(month_end_date, end_date_dt).strftime('%Y-%m-%d')
        
        # APIå‘¼ã³å‡ºã—
        response = get_historical_weather(lat, lon, period_start, period_end)
        if response.status_code == 200:
            all_data.append(response.json())
        else:
            logger.error(f"API error for {period_start} to {period_end}: {response.status_code}")
        
        # æ¬¡ã®æœˆã¸
        current_date = month_end_date + timedelta(days=1)
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼ˆ1ç§’å¾…æ©Ÿï¼‰
        time.sleep(1)
    
    return all_data
```

**ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å­¦ç¿’**ï¼š
```python
# Stage 3: æœ¬æ ¼çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
import time
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

def create_robust_session():
    session = requests.Session()
    
    # ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥
    retry_strategy = Retry(
        total=3,                    # æœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤
        status_forcelist=[429, 500, 502, 503, 504],  # ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        method_whitelist=["HEAD", "GET", "OPTIONS"],
        backoff_factor=1            # 1ç§’ã€2ç§’ã€4ç§’ã®é–“éš”
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

def download_with_retry(session, url, params, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = session.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:  # Too Many Requests
                wait_time = 2 ** attempt  # Exponential backoff
                logger.warning(f"Rate limited, waiting {wait_time} seconds...")
                time.sleep(wait_time)
                continue
            else:
                response.raise_for_status()
                
        except requests.exceptions.Timeout:
            logger.warning(f"Timeout on attempt {attempt + 1}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            else:
                raise
        except requests.exceptions.ConnectionError:
            logger.warning(f"Connection error on attempt {attempt + 1}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            else:
                raise
    
    raise Exception(f"Failed to download after {max_retries} attempts")
```

#### **4-2: åº§æ¨™ç®¡ç†ãƒ»éƒ½çœŒãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–å­¦ç¿’**

**éƒ½çœŒåº§æ¨™ã®èª¿æŸ»ãƒ»ç®¡ç†å­¦ç¿’**ï¼š
```python
# æœ€åˆã®åº§æ¨™å–å¾—ï¼ˆæ‰‹å‹•èª¿æŸ»ï¼‰
# ã‚°ã‚°ã‚Š: "æ±äº¬éƒ½ ç·¯åº¦çµŒåº¦", "ç¥å¥ˆå·çœŒ ç·¯åº¦çµŒåº¦" ã‚’1ã¤ãšã¤èª¿æŸ»

# Stage 1: æ‰‹å‹•åº§æ¨™ãƒªã‚¹ãƒˆ
PREFECTURE_COORDINATES = {
    'tokyo': (35.6762, 139.6503),      # æ±äº¬é§…
    'kanagawa': (35.4478, 139.6425),   # æ¨ªæµœé§…
    # ... æ‰‹å‹•ã§9éƒ½çœŒåˆ†èª¿æŸ»
}

# Stage 2: åº§æ¨™ã®å¦¥å½“æ€§æ¤œè¨¼
def validate_coordinates():
    for prefecture, (lat, lon) in PREFECTURE_COORDINATES.items():
        # ç·¯åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¥æœ¬ã®ç¯„å›²ï¼‰
        if not (24 <= lat <= 46):
            raise ValueError(f"Invalid latitude for {prefecture}: {lat}")
        
        # çµŒåº¦ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¥æœ¬ã®ç¯„å›²ï¼‰
        if not (123 <= lon <= 146):
            raise ValueError(f"Invalid longitude for {prefecture}: {lon}")
        
        print(f"{prefecture}: lat={lat}, lon={lon} - OK")

# Stage 3: ä»£è¡¨åœ°ç‚¹é¸æŠã®å­¦ç¿’
# å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ: éƒ½åºœçœŒåºæ‰€åœ¨åœ° vs äººå£é‡å¿ƒ vs åœ°ç†çš„ä¸­å¿ƒ
# æ±ºå®š: çœŒåºæ‰€åœ¨åœ°ã‚’åŸºæº–ï¼ˆå…¬å¼ãƒ‡ãƒ¼ã‚¿ã¨ã®æ•´åˆæ€§é‡è¦–ï¼‰
```

**9éƒ½çœŒåŒæ™‚å‡¦ç†ã®ä¸¦è¡ŒåŒ–å­¦ç¿’**ï¼š
```python
# æœ€åˆã®å®Ÿè£…ï¼ˆé€æ¬¡å‡¦ç†ï¼‰
def download_all_prefectures_sequential(date_range):
    results = {}
    for prefecture, coordinates in PREFECTURE_COORDINATES.items():
        lat, lon = coordinates
        data = download_weather_data(lat, lon, date_range)
        results[prefecture] = data
        time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
    return results

# å•é¡Œ: 9éƒ½çœŒ Ã— æœŸé–“åˆ†å‰² = é•·æ™‚é–“å‡¦ç†

# Stage 1: ä¸¦è¡Œå‡¦ç†ï¼ˆthreadingï¼‰
import threading
import queue

def download_prefecture_worker(prefecture, coordinates, date_range, result_queue):
    try:
        lat, lon = coordinates
        data = download_weather_data(lat, lon, date_range)
        result_queue.put((prefecture, data, None))
    except Exception as e:
        result_queue.put((prefecture, None, e))

def download_all_prefectures_parallel(date_range):
    result_queue = queue.Queue()
    threads = []
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰èµ·å‹•
    for prefecture, coordinates in PREFECTURE_COORDINATES.items():
        thread = threading.Thread(
            target=download_prefecture_worker,
            args=(prefecture, coordinates, date_range, result_queue)
        )
        thread.start()
        threads.append(thread)
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼ˆå°‘ã—é–“éš”ã‚’ã‚ã‘ã¦èµ·å‹•ï¼‰
        time.sleep(0.5)
    
    # çµæœåé›†
    results = {}
    errors = {}
    
    for thread in threads:
        thread.join()
    
    while not result_queue.empty():
        prefecture, data, error = result_queue.get()
        if error:
            errors[prefecture] = error
        else:
            results[prefecture] = data
    
    if errors:
        logger.warning(f"Errors occurred for: {list(errors.keys())}")
        for prefecture, error in errors.items():
            logger.error(f"{prefecture}: {error}")
    
    return results, errors

# Stage 2: ã‚ˆã‚Šå®Ÿç”¨çš„ãªå®Ÿè£…ï¼ˆã‚¨ãƒ©ãƒ¼è€æ€§å‘ä¸Šï¼‰
def download_with_fallback(prefecture, coordinates, date_range, max_retries=3):
    lat, lon = coordinates
    
    for attempt in range(max_retries):
        try:
            data = download_weather_data(lat, lon, date_range)
            logger.info(f"Successfully downloaded {prefecture} data (attempt {attempt + 1})")
            return data
        except Exception as e:
            logger.warning(f"Failed to download {prefecture} data (attempt {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2  # 2ç§’ã€4ç§’ã€6ç§’
                time.sleep(wait_time)
            else:
                logger.error(f"Giving up on {prefecture} after {max_retries} attempts")
                raise
```

---

### **Phase 5: BigQueryé«˜åº¦è¨­è¨ˆãƒ»åˆ¶ç´„å…‹æœã®æ·±å±¤å­¦ç¿’**

#### **5-1: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­è¨ˆæ€æƒ³å­¦ç¿’**

**æœ€åˆã®ãƒ†ãƒ¼ãƒ–ãƒ«è¨­è¨ˆï¼ˆéåŠ¹ç‡ï¼‰**ï¼š
```sql
-- æœ€åˆã®ç´ æœ´ãªãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE energy_data_hourly (
    date DATE,
    hour INTEGER,
    actual_power FLOAT64,
    supply_capacity FLOAT64,
    created_at TIMESTAMP
);
-- å•é¡Œ: æ—¥ä»˜ç¯„å›²ã‚¯ã‚¨ãƒªãŒé…ã„ã€æ™‚é–“åˆ¥é›†è¨ˆãŒé…ã„
```

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œç™ºè¦‹ãƒ»å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
- **å•é¡Œç™ºç”Ÿ**: æœˆåˆ¥é›†è¨ˆã‚¯ã‚¨ãƒªãŒ30ç§’ä»¥ä¸Šã‹ã‹ã‚‹
- **ã‚°ã‚°ã‚Š**: "bigquery slow query optimization"
- **ç™ºè¦‹**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¦‚å¿µ
- **å­¦ç¿’ã‚½ãƒ¼ã‚¹**: BigQueryå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ "Partitioned tables"

**ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­è¨ˆã®æ®µéšçš„ç†è§£**ï¼š
```sql
-- Stage 1: æ—¥ä»˜ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¿½åŠ 
CREATE TABLE energy_data_hourly (
    date DATE,
    hour INTEGER,
    actual_power FLOAT64,
    supply_capacity FLOAT64,
    created_at TIMESTAMP
)
PARTITION BY date;

-- åŠ¹æœ: æ—¥ä»˜ç¯„å›²ã‚¯ã‚¨ãƒªãŒé«˜é€ŸåŒ–ï¼ˆãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³å›é¿ï¼‰

-- Stage 2: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¿½åŠ 
CREATE TABLE energy_data_hourly (
    date DATE,
    hour INTEGER,
    actual_power FLOAT64,
    supply_capacity FLOAT64,
    created_at TIMESTAMP
)
PARTITION BY date
CLUSTER BY hour;

-- åŠ¹æœ: æ™‚é–“åˆ¥åˆ†æã‚¯ã‚¨ãƒªã‚‚é«˜é€ŸåŒ–
```

**è¨­è¨ˆåˆ¤æ–­ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
```sql
-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å€™è£œã®æ¤œè¨
-- å€™è£œ1: date (æ—¥ä»˜) â†’ æ¡ç”¨
-- ç†ç”±: å¤§éƒ¨åˆ†ã®ã‚¯ã‚¨ãƒªãŒæ—¥ä»˜ç¯„å›²æŒ‡å®š

-- å€™è£œ2: created_at (ä½œæˆæ—¥æ™‚) â†’ ä¸æ¡ç”¨
-- ç†ç”±: ãƒ“ã‚¸ãƒã‚¹ã‚¯ã‚¨ãƒªã§ã¯ä½¿ç”¨é »åº¦ä½ã„

-- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å€™è£œã®æ¤œè¨
-- å€™è£œ1: hour (æ™‚é–“) â†’ æ¡ç”¨
-- ç†ç”±: æ™‚é–“åˆ¥åˆ†æãŒå¤šã„

-- å€™è£œ2: actual_power (é›»åŠ›å€¤) â†’ ä¸æ¡ç”¨
-- ç†ç”±: ç¯„å›²æ¤œç´¢ã‚ˆã‚Šå®Œå…¨ä¸€è‡´ãŒå¤šã„

-- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
SELECT 
    hour,
    AVG(actual_power) as avg_power
FROM energy_data_hourly
WHERE date BETWEEN '2025-01-01' AND '2025-01-31'
GROUP BY hour
ORDER BY hour;

-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å‰: 8.2ç§’ã€115MBå‡¦ç†
-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å¾Œ: 1.1ç§’ã€12MBå‡¦ç†ï¼ˆç´„7å€é«˜é€ŸåŒ–ï¼‰
```

#### **5-2: å‹çµ±ä¸€å•é¡Œã®æ ¹æœ¬è§£æ±ºå­¦ç¿’**

**å‹ã‚¨ãƒ©ãƒ¼ã®æ ¹æœ¬åŸå› åˆ†æ**ï¼š
```sql
-- å•é¡Œã®ã‚¯ã‚¨ãƒª
SELECT *
FROM energy_data_hourly e
JOIN weather_data w
  ON e.date = w.date 
  AND e.hour = w.hour  -- ã‚¨ãƒ©ãƒ¼: INT64 vs STRING
```

**å‹çµ±ä¸€æˆ¦ç•¥ã®æ®µéšçš„æ¤œè¨**ï¼š
```sql
-- Stage 1: ãã®å ´ã—ã®ãã®å‹å¤‰æ›
SELECT *
FROM energy_data_hourly e
JOIN weather_data w
  ON e.date = w.date 
  AND e.hour = CAST(w.hour AS INT64)

-- å•é¡Œ: æ¯å›CASTãŒå¿…è¦ã€å¯èª­æ€§ä½ä¸‹

-- Stage 2: ã©ã¡ã‚‰ã‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿®æ­£ã™ã‚‹æ¤œè¨
-- é¸æŠè‚¢A: weather_dataã®hourã‚’INTEGERã«å¤‰æ›´
-- é¸æŠè‚¢B: energy_dataã®hourã‚’STRINGã«å¤‰æ›´

-- æ¤œè¨çµæœ: é¸æŠè‚¢Aæ¡ç”¨
-- ç†ç”±: INTEGERã®æ–¹ãŒã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŠ¹ç‡ãƒ»è¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„
```

**LOAD DATAå‹æ¨è«–ã®æ´»ç”¨å­¦ç¿’**ï¼š
```sql
-- å¾“æ¥ã®æ–¹æ³•: æ‰‹å‹•å‹æŒ‡å®š
CREATE TABLE weather_data (
    prefecture STRING,
    date DATE,
    hour STRING,  -- æ‰‹å‹•ã§STRINGæŒ‡å®šï¼ˆå¾Œã§å•é¡Œã«ï¼‰
    temperature_2m FLOAT64,
    ...
);

-- æ”¹è‰¯ã—ãŸæ–¹æ³•: LOAD DATAè‡ªå‹•æ¨è«–
LOAD DATA INTO `energy-env.dev_energy_data.weather_data`
FROM FILES (
    format = 'CSV',
    uris = ['gs://bucket/weather_processed/*.csv'],
    skip_leading_rows = 1
);

-- BigQueryãŒè‡ªå‹•çš„ã«é©åˆ‡ãªå‹ã‚’é¸æŠ
-- houråˆ—ãŒæ•°å€¤ã®ã¿ã®å ´åˆ â†’ INTEGERè‡ªå‹•é¸æŠ
-- çµæœ: å‹çµ±ä¸€å•é¡Œã®æ ¹æœ¬è§£æ±º
```

**å‹å¤‰æ›æœ€é©åŒ–ã®å­¦ç¿’**ï¼š
```sql
-- è¡¨ç¤ºæ™‚å‹å¤‰æ›ã®åŠ¹ç‡åŒ–
-- Before: JOINã”ã¨ã«CAST
SELECT 
    e.date,
    LPAD(CAST(e.hour AS STRING), 2, '0') as hour_display,
    e.actual_power,
    w.temperature_2m
FROM energy_data_hourly e
JOIN weather_data w 
  ON e.date = w.date 
  AND e.hour = w.hour  -- ã©ã¡ã‚‰ã‚‚INTEGERåŒå£«

-- After: ãƒ“ãƒ¥ãƒ¼ã§å‹å¤‰æ›ã‚’éš è”½
CREATE VIEW energy_data_display AS
SELECT 
    date,
    LPAD(CAST(hour AS STRING), 2, '0') as hour_display,
    hour as hour_raw,  -- JOINç”¨
    actual_power,
    supply_capacity
FROM energy_data_hourly;

-- åˆ©ç‚¹: ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã¨è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ã®åˆ†é›¢
```

#### **5-3: BigQueryåˆ¶ç´„ã®ä½“ç³»çš„ç†è§£ãƒ»å›é¿æŠ€è¡“ç¿’å¾—**

**åˆ¶ç´„ç™ºè¦‹ã®ä½“ç³»çš„ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
```sql
-- åˆ¶ç´„1: è¤‡æ•°ã‚«ãƒ©ãƒ INå¥
-- ç™ºè¦‹çµŒç·¯: é‡è¤‡å‰Šé™¤ã‚¯ã‚¨ãƒªå®Ÿè£…æ™‚

-- è©¦è¡Œã—ãŸã‚¯ã‚¨ãƒªï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
WHERE (prefecture, date, hour) IN (
    SELECT prefecture, date, hour FROM external_table
);
-- ã‚¨ãƒ©ãƒ¼: Subquery with multiple columns is not supported

-- å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹:
-- 1. BigQueryå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª¿æŸ»
-- 2. Stack Overflowæ¤œç´¢: "bigquery multiple column in subquery"
-- 3. ä»£æ›¿æ‰‹æ³•ã®ç™ºè¦‹ãƒ»å®Ÿè£…

-- è§£æ±ºç­–: CONCATæ–¹å¼
WHERE CONCAT(prefecture, '|', CAST(date AS STRING), '|', hour) IN (
    SELECT CONCAT(prefecture, '|', CAST(date AS STRING), '|', hour)
    FROM external_table
);
```

**åˆ¶ç´„å›é¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’ãƒ»ä½“ç³»åŒ–**ï¼š
```python
# BigQueryåˆ¶ç´„å›é¿ãƒ‘ã‚¿ãƒ¼ãƒ³é›†
bigquery_workarounds = {
    'multiple_column_in': 'CONCATæ–¹å¼ã«ã‚ˆã‚‹è¤‡åˆã‚­ãƒ¼',
    'delete_with_join': 'EXISTSæ–¹å¼ã«ã‚ˆã‚‹çµåˆå‰Šé™¤',
    'with_clause_update': 'ç”ŸSQLå®Ÿè¡Œã«ã‚ˆã‚‹å›é¿',
    'window_function_in_where': 'ã‚µãƒ–ã‚¯ã‚¨ãƒªåŒ–ã«ã‚ˆã‚‹å›é¿',
    'recursive_cte': 'UNION ALLåå¾©ã«ã‚ˆã‚‹å›é¿',
}

# å®Ÿéš›ã®å›é¿å®Ÿè£…ä¾‹
def generate_multi_column_in_query(table, key_columns, subquery):
    # è¤‡æ•°ã‚«ãƒ©ãƒ INå¥ã®å®‰å…¨ãªä»£æ›¿ç”Ÿæˆ
    concat_expr = ' || "|" || '.join([
        f'CAST({col} AS STRING)' if col_type != 'STRING' else col
        for col, col_type in key_columns
    ])
    
    return f"""
    WHERE {concat_expr} IN (
        SELECT {concat_expr}
        FROM ({subquery})
    )
    """

# ä½¿ç”¨ä¾‹
key_columns = [('prefecture', 'STRING'), ('date', 'DATE'), ('hour', 'INTEGER')]
subquery = "SELECT prefecture, date, hour FROM external_table"
safe_query = generate_multi_column_in_query('weather_data', key_columns, subquery)
```

---

### **Phase 6: æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆãƒ»æˆ¦ç•¥æ€è€ƒã®ç™ºé”**

#### **6-1: æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»çµ±è¨ˆæ€è€ƒã®æ·±åŒ–**

**æœ€åˆã®åˆ†æï¼ˆè¡¨é¢çš„ï¼‰**ï¼š
```python
# æœ€åˆã®ç´ æœ´ãªåˆ†æ
import matplotlib.pyplot as plt

# å˜ç´”ãªæ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
df = client.query("SELECT date, hour, actual_power FROM ml_features ORDER BY date, hour").to_dataframe()
plt.plot(df['actual_power'])
plt.title('é›»åŠ›éœ€è¦æ¨ç§»')
plt.show()

# å•é¡Œ: ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ãˆãªã„ã€æ´å¯ŸãŒå¾—ã‚‰ã‚Œãªã„
```

**åˆ†ææ‰‹æ³•ã®æ®µéšçš„é«˜åº¦åŒ–**ï¼š
```python
# Stage 1: æœˆåˆ¥ãƒ»æ™‚é–“åˆ¥ã®äºŒæ¬¡å…ƒåˆ†æ
query = """
SELECT 
    EXTRACT(MONTH FROM date) as month,
    hour,
    AVG(actual_power) as avg_power
FROM `energy-env.dev_energy_data.power_weather_integrated`
GROUP BY month, hour
ORDER BY month, hour
"""

df = client.query(query).to_dataframe()

# ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
pivot_data = df.pivot(index='hour', columns='month', values='avg_power')

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å¯è¦–åŒ–
import seaborn as sns
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_data, annot=True, fmt='.0f', cmap='YlOrRd')
plt.title('æœˆåˆ¥ãƒ»æ™‚é–“åˆ¥ é›»åŠ›éœ€è¦ãƒ‘ã‚¿ãƒ¼ãƒ³')
plt.xlabel('æœˆ')
plt.ylabel('æ™‚é–“')
plt.show()

# ç™ºè¦‹: 7-9æœˆã€12-2æœˆã®éœ€è¦ãŒé«˜ã„ â†’ å†·æš–æˆ¿éœ€è¦
```

**Ridge Plotåˆ†æã®å­¦ç¿’ãƒ»å®Ÿè£…**ï¼š
```python
# Stage 2: éƒ½çœŒåˆ¥åˆ†å¸ƒæ¯”è¼ƒï¼ˆRidge Plotæ¦‚å¿µå­¦ç¿’ï¼‰

# Ridge Plotå­¦ç¿’ã®çµŒç·¯:
# 1. éƒ½çœŒåˆ¥ã®äºˆæ¸¬é›£æ˜“åº¦ã‚’å®šé‡åŒ–ã—ãŸã„
# 2. ã‚°ã‚°ã‚Š: "compare distributions multiple groups python"
# 3. ç™ºè¦‹: Ridge Plotï¼ˆåˆ†å¸ƒã®é‡ã­åˆã‚ã›è¡¨ç¤ºï¼‰
# 4. å®Ÿè£…: seaborn + matplotlibçµ„ã¿åˆã‚ã›

def create_ridge_plot_analysis():
    # éƒ½çœŒåˆ¥é›»åŠ›éœ€è¦åˆ†å¸ƒå–å¾—
    query = """
    SELECT 
        w.prefecture,
        e.actual_power,
        c.is_weekend,
        c.is_holiday
    FROM `energy-env.dev_energy_data.weather_data` w
    JOIN `energy-env.dev_energy_data.energy_data_hourly` e
        ON w.date = e.date AND CAST(w.hour AS INTEGER) = e.hour
    JOIN `energy-env.dev_energy_data.calendar_data` c
        ON w.date = c.date
    WHERE w.date >= '2025-01-01'
    """
    
    df = client.query(query).to_dataframe()
    
    # å¹³æ—¥ãƒ»ä¼‘æ—¥åˆ¥åˆ†æ
    weekday_df = df[~df['is_weekend'] & ~df['is_holiday']]
    weekend_df = df[df['is_weekend'] | df['is_holiday']]
    
    # å„éƒ½çœŒã®åˆ†å¸ƒçµ±è¨ˆè¨ˆç®—
    prefecture_stats = {}
    
    for prefecture in df['prefecture'].unique():
        weekday_power = weekday_df[weekday_df['prefecture'] == prefecture]['actual_power']
        weekend_power = weekend_df[weekend_df['prefecture'] == prefecture]['actual_power']
        
        prefecture_stats[prefecture] = {
            'weekday_q75_q25': weekday_power.quantile(0.75) - weekday_power.quantile(0.25),
            'weekend_q75_q25': weekend_power.quantile(0.75) - weekend_power.quantile(0.25),
            'weekday_mean': weekday_power.mean(),
            'weekend_mean': weekend_power.mean(),
            'weekday_std': weekday_power.std(),
            'weekend_std': weekend_power.std()
        }
    
    # äºˆæ¸¬å¯èƒ½æ€§è©•ä¾¡ï¼ˆãƒ¬ãƒ³ã‚¸å¹…ãŒå°ã•ã„ = äºˆæ¸¬ã—ã‚„ã™ã„ï¼‰
    for prefecture, stats in prefecture_stats.items():
        total_predictability = (
            1 / stats['weekday_q75_q25'] + 1 / stats['weekend_q75_q25']
        ) / 2
        stats['predictability_score'] = total_predictability
    
    # çµæœè¡¨ç¤º
    sorted_prefectures = sorted(
        prefecture_stats.items(), 
        key=lambda x: x[1]['predictability_score'], 
        reverse=True
    )
    
    print("éƒ½çœŒåˆ¥äºˆæ¸¬å¯èƒ½æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆãƒ¬ãƒ³ã‚¸å¹…ãƒ™ãƒ¼ã‚¹ï¼‰:")
    for i, (prefecture, stats) in enumerate(sorted_prefectures):
        print(f"{i+1}. {prefecture}: "
              f"å¹³æ—¥ãƒ¬ãƒ³ã‚¸{stats['weekday_q75_q25']:.0f}, "
              f"ä¼‘æ—¥ãƒ¬ãƒ³ã‚¸{stats['weekend_q75_q25']:.0f}")
    
    return prefecture_stats

# å®Ÿè¡Œçµæœä¾‹ï¼š
# 1. chiba: å¹³æ—¥ãƒ¬ãƒ³ã‚¸2856, ä¼‘æ—¥ãƒ¬ãƒ³ã‚¸1994
# 2. kanagawa: å¹³æ—¥ãƒ¬ãƒ³ã‚¸2901, ä¼‘æ—¥ãƒ¬ãƒ³ã‚¸2043
# ...
# â†’ åƒè‘‰ãŒæœ€ã‚‚äºˆæ¸¬ã—ã‚„ã™ã„ï¼ˆãƒ¬ãƒ³ã‚¸å¹…æœ€å°ï¼‰ã¨åˆ¤æ˜
```

**çµ±è¨ˆçš„æ€è€ƒã®æ·±åŒ–å­¦ç¿’**ï¼š
```python
# Stage 3: çµ±è¨ˆçš„æœ‰æ„æ€§ vs å®Ÿè³ªçš„æ„å‘³ã®å­¦ç¿’

# å•é¡Œ: å¤§ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ä½•ã§ã‚‚æœ‰æ„ã«ãªã‚‹
from scipy import stats

# åƒè‘‰ vs ä»–éƒ½çœŒã®é›»åŠ›éœ€è¦å·®æ¤œå®š
chiba_power = df[df['prefecture'] == 'chiba']['actual_power']
tokyo_power = df[df['prefecture'] == 'tokyo']['actual_power']

# tæ¤œå®šå®Ÿè¡Œ
t_stat, p_value = stats.ttest_ind(chiba_power, tokyo_power)
print(f"tæ¤œå®šçµæœ: t={t_stat:.3f}, p={p_value:.3e}")
# çµæœ: p < 0.001ï¼ˆçµ±è¨ˆçš„æœ‰æ„ï¼‰

# å•é¡Œ: ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå¤šã„ãŸã‚ã€å®Ÿç”¨çš„ã§ãªã„å°ã•ãªå·®ã‚‚æœ‰æ„ã«ãªã‚‹

# åŠ¹æœé‡è¨ˆç®—ï¼ˆCohen's dï¼‰
def cohens_d(group1, group2):
    n1, n2 = len(group1), len(group2)
    pooled_std = np.sqrt(((n1-1)*group1.var() + (n2-1)*group2.var()) / (n1+n2-2))
    return (group1.mean() - group2.mean()) / pooled_std

effect_size = cohens_d(chiba_power, tokyo_power)
print(f"åŠ¹æœé‡ï¼ˆCohen's dï¼‰: {effect_size:.3f}")

# è§£é‡ˆåŸºæº–å­¦ç¿’:
# 0.2: å°ã•ã„åŠ¹æœ
# 0.5: ä¸­ç¨‹åº¦ã®åŠ¹æœ  
# 0.8: å¤§ãã„åŠ¹æœ

if abs(effect_size) < 0.2:
    print("å®Ÿç”¨çš„ã«ã¯å·®ãŒãªã„")
elif abs(effect_size) < 0.5:
    print("å°ã•ãªå·®")
else:
    print("å®Ÿç”¨çš„ã«æ„å‘³ã®ã‚ã‚‹å·®")

# å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ: çµ±è¨ˆçš„æœ‰æ„æ€§ã‚ˆã‚ŠåŠ¹æœé‡ãƒ»å®Ÿç”¨çš„æ„å‘³ã‚’é‡è¦–
```

#### **6-2: æ©Ÿæ¢°å­¦ç¿’æˆ¦ç•¥æ±ºå®šãƒ»æ‰‹æ³•é¸æŠå­¦ç¿’**

**æœ€åˆã®æ‰‹æ³•æ¤œè¨ï¼ˆç†è«–å…ˆè¡Œï¼‰**ï¼š
```python
# æœ€åˆã®æ¤œè¨ï¼ˆæ™‚ç³»åˆ—æ‰‹æ³•ã«åé‡ï¼‰
ml_approaches = {
    'prophet': 'æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãªã®ã§ProphetãŒé©ã—ã¦ã„ã‚‹',
    'arima': 'å¾“æ¥ã®æ™‚ç³»åˆ—åˆ†ææ‰‹æ³•',
    'lstm': 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹æ™‚ç³»åˆ—äºˆæ¸¬'
}

# å•é¡Œ: ãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªã‚’ç†è§£ã›ãšã«æ‰‹æ³•å…ˆè¡Œã§é¸æŠ
```

**é›»åŠ›éœ€è¦ãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªç†è§£ãƒ—ãƒ­ã‚»ã‚¹**ï¼š
```python
# Stage 1: ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§åˆ†æ
# 1. å‘¨æœŸæ€§ã®ç¢ºèª
query = """
SELECT 
    EXTRACT(DAYOFWEEK FROM date) as day_of_week,
    hour,
    AVG(actual_power) as avg_power
FROM `energy-env.dev_energy_data.power_weather_integrated`
GROUP BY day_of_week, hour
ORDER BY day_of_week, hour
"""

df = client.query(query).to_dataframe()

# ç™ºè¦‹1: æ˜ç¢ºãªé€±æ¬¡å‘¨æœŸæ€§ï¼ˆå¹³æ—¥vsé€±æœ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
# ç™ºè¦‹2: æ—¥æ¬¡å‘¨æœŸæ€§ï¼ˆæœå¤•ãƒ”ãƒ¼ã‚¯ï¼‰

# 2. å¤–éƒ¨è¦å› ã®å½±éŸ¿åˆ†æ
query = """
SELECT 
    temperature_2m,
    actual_power,
    is_holiday
FROM `energy-env.dev_energy_data.power_weather_integrated`
WHERE temperature_2m IS NOT NULL
"""

df = client.query(query).to_dataframe()

# æ°—æ¸©ã¨é›»åŠ›éœ€è¦ã®é–¢ä¿‚åˆ†æ
import numpy as np
correlation = np.corrcoef(df['temperature_2m'], df['actual_power'])[0,1]
print(f"æ°—æ¸©ã¨é›»åŠ›éœ€è¦ã®ç›¸é–¢: {correlation:.3f}")

# ç¥æ—¥åŠ¹æœã®åˆ†æ
holiday_power = df[df['is_holiday'] == True]['actual_power']
normal_power = df[df['is_holiday'] == False]['actual_power']

print(f"ç¥æ—¥å¹³å‡éœ€è¦: {holiday_power.mean():.0f}")
print(f"å¹³æ—¥å¹³å‡éœ€è¦: {normal_power.mean():.0f}")
print(f"ç¥æ—¥åŠ¹æœ: {(normal_power.mean() - holiday_power.mean()):.0f}ä¸‡kWå‰Šæ¸›")

# ç™ºè¦‹3: æ°—æ¸©ãƒ»ç¥æ—¥ãƒ»æ›œæ—¥ãŒè¤‡åˆçš„ã«å½±éŸ¿
```

**æ‰‹æ³•é¸æŠã®å®Ÿè¨¼çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒå­¦ç¿’**ï¼š
```python
# Stage 2: ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãæ‰‹æ³•å†æ¤œè¨

# é›»åŠ›éœ€è¦ã®ç‰¹å¾´:
# - è¤‡æ•°ã®å‘¨æœŸæ€§ï¼ˆæ—¥æ¬¡ãƒ»é€±æ¬¡ãƒ»å¹´æ¬¡ï¼‰
# - å¤–éƒ¨è¦å› ï¼ˆæ°—æ¸©ãƒ»ç¥æ—¥ãƒ»æ›œæ—¥ï¼‰ã®è¤‡åˆçš„å½±éŸ¿
# - éç·šå½¢ãªç›¸äº’ä½œç”¨ï¼ˆå¤ã®å¹³æ—¥æ˜¼é–“ã¯ç‰¹ã«é«˜éœ€è¦ï¼‰

# æ‰‹æ³•ã®å†è©•ä¾¡:
ml_method_evaluation = {
    'prophet': {
        'pros': ['å‘¨æœŸæ€§è‡ªå‹•æ¤œå‡º', 'ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ', 'ç¥æ—¥åŠ¹æœ'],
        'cons': ['å¤–éƒ¨å¤‰æ•°åˆ¶é™', 'éç·šå½¢ç›¸äº’ä½œç”¨å¼±ã„'],
        'fit_score': 6
    },
    'xgboost': {
        'pros': ['éç·šå½¢ç›¸äº’ä½œç”¨', 'ç‰¹å¾´é‡é‡è¦åº¦', 'éå­¦ç¿’è€æ€§'],
        'cons': ['å‘¨æœŸæ€§æ‰‹å‹•è¨­è¨ˆ', 'è§£é‡ˆæ€§ã‚„ã‚„åŠ£ã‚‹'],
        'fit_score': 8
    },
    'linear_regression': {
        'pros': ['è§£é‡ˆæ€§é«˜ã„', 'é«˜é€Ÿ'],
        'cons': ['éç·šå½¢é–¢ä¿‚æ‰ãˆã‚‰ã‚Œãªã„', 'ç›¸äº’ä½œç”¨æ‰‹å‹•è¨­è¨ˆ'],
        'fit_score': 4
    }
}

# æ±ºå®š: XGBoost + ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
# ç†ç”±: 
# 1. æ°—æ¸©Ã—æ™‚é–“Ã—æ›œæ—¥ã®è¤‡é›‘ãªç›¸äº’ä½œç”¨ã‚’å­¦ç¿’å¯èƒ½
# 2. Feature Importanceã§è¦å› åˆ†æå¯èƒ½
# 3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§å‘¨æœŸæ€§ã‚‚è¡¨ç¾å¯èƒ½
```

**ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æˆ¦ç•¥ã®å­¦ç¿’**ï¼š
```python
# Stage 3: XGBoostå‘ã‘ç‰¹å¾´é‡è¨­è¨ˆå­¦ç¿’

# å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹:
# 1. "xgboost feature engineering best practices" ã‚°ã‚°ã‚Š
# 2. Kaggleæ™‚ç³»åˆ—ã‚³ãƒ³ãƒšè§£æ³•èª¿æŸ»
# 3. é›»åŠ›éœ€è¦äºˆæ¸¬è«–æ–‡èª¿æŸ»

feature_engineering_strategy = {
    'calendar_features': {
        'importance': 'HIGH',
        'reason': 'ç¥æ—¥åŠ¹æœãŒ2000ä¸‡kWï¼ˆå…¨ä½“ã®30%ï¼‰ã¨å·¨å¤§',
        'implementation': [
            'is_holiday', 'is_weekend', 'day_of_week',
            'month', 'is_month_end', 'is_quarter_end'
        ]
    },
    'lag_features': {
        'importance': 'HIGH', 
        'reason': 'å‰æ—¥ãƒ»å‰é€±ã®åŒæ™‚åˆ»ã¯å¼·ã„äºˆæ¸¬å› å­',
        'implementation': [
            'lag_1_day', 'lag_7_day', 'lag_1_business_day',
            'moving_average_7_days', 'moving_average_30_days'
        ]
    },
    'weather_features': {
        'importance': 'MEDIUM',
        'reason': 'å†·æš–æˆ¿éœ€è¦ã®åŸºæœ¬è¦å› ã ãŒé™å®šçš„',
        'implementation': [
            'temperature_2m', 'humidity', 
            'temperature_squared', 'cooling_degree_days'
        ]
    },
    'time_features': {
        'importance': 'MEDIUM',
        'reason': '24æ™‚é–“å‘¨æœŸæ€§ã®è¡¨ç¾',
        'implementation': [
            'hour_sin', 'hour_cos',
            'is_peak_hour', 'time_of_day_category'
        ]
    }
}

# å®Ÿè£…å„ªå…ˆé †ä½æ±ºå®š:
# 1. ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´é‡ï¼ˆç¥æ—¥åŠ¹æœæœ€å¤§ï¼‰
# 2. ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆæ™‚ç³»åˆ—ã®æœ¬è³ªï¼‰
# 3. æ°—è±¡ç‰¹å¾´é‡ï¼ˆåŸºæœ¬çš„å½±éŸ¿ï¼‰
# 4. æ™‚é–“ç‰¹å¾´é‡ï¼ˆå‘¨æœŸæ€§è£œå®Œï¼‰
```

#### **6-3: å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡è¨­è¨ˆãƒ»å®Ÿè£…ã®æ·±å±¤å­¦ç¿’**

**æœ€åˆã®å–¶æ¥­æ—¥æ¦‚å¿µå­¦ç¿’**ï¼š
```sql
-- æœ€åˆã®ç´ æœ´ãªç†è§£
-- å–¶æ¥­æ—¥ = å¹³æ—¥ï¼ˆæœˆã€œé‡‘ï¼‰

SELECT 
    date,
    EXTRACT(DAYOFWEEK FROM date) as day_of_week,
    CASE 
        WHEN EXTRACT(DAYOFWEEK FROM date) BETWEEN 2 AND 6 THEN true
        ELSE false
    END as is_business_day
FROM calendar_data;

-- å•é¡Œ: ç¥æ—¥ã‚’è€ƒæ…®ã—ã¦ã„ãªã„
-- ä¾‹: æœˆæ›œæ—¥ãŒç¥æ—¥ã§ã‚‚å–¶æ¥­æ—¥ã¨ã—ã¦æ‰±ã£ã¦ã—ã¾ã†
```

**å–¶æ¥­æ—¥å®šç¾©ã®ç²¾ç·»åŒ–å­¦ç¿’**ï¼š
```sql
-- Stage 1: ç¥æ—¥è€ƒæ…®ã®å–¶æ¥­æ—¥å®šç¾©
SELECT 
    date,
    is_weekend,
    is_holiday,
    CASE 
        WHEN is_weekend = true OR is_holiday = true THEN false
        ELSE true
    END as is_business_day
FROM calendar_data;

-- Stage 2: å®Ÿéš›ã®é›»åŠ›éœ€è¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œè¨¼
SELECT 
    CASE 
        WHEN is_weekend = true OR is_holiday = true THEN 'non_business'
        ELSE 'business'
    END as day_type,
    AVG(actual_power) as avg_power,
    STDDEV(actual_power) as std_power,
    COUNT(*) as sample_count
FROM power_weather_integrated
GROUP BY day_type;

-- çµæœä¾‹:
-- business: avg=4500ä¸‡kW, std=800ä¸‡kW
-- non_business: avg=3200ä¸‡kW, std=600ä¸‡kW
-- â†’ å–¶æ¥­æ—¥ã¨éå–¶æ¥­æ—¥ã§æ˜ç¢ºã«ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª
```

**business_day_numberæ¦‚å¿µã®æ¤œè¨ãƒ»ç ´æ£„å­¦ç¿’**ï¼š
```sql
-- æœ€åˆã®ã‚¢ã‚¤ãƒ‡ã‚¢: å–¶æ¥­æ—¥ã«é€£ç•ªã‚’æŒ¯ã‚‹
-- 2025-01-06(æœˆ) â†’ business_day_number: 1
-- 2025-01-07(ç«) â†’ business_day_number: 2
-- 2025-01-08(æ°´) â†’ business_day_number: 3
-- 2025-01-11(åœŸ) â†’ business_day_number: NULLï¼ˆåœŸæ›œæ—¥ï¼‰
-- 2025-01-13(æœˆ) â†’ business_day_number: 4

-- å®Ÿè£…è©¦è¡Œ1: ROW_NUMBER()ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
ALTER TABLE calendar_data 
ADD COLUMN business_day_number INTEGER;

UPDATE calendar_data
SET business_day_number = (
    SELECT ROW_NUMBER() OVER (ORDER BY date)
    FROM calendar_data c2
    WHERE c2.is_holiday = false 
      AND c2.is_weekend = false
      AND c2.date <= calendar_data.date
)
WHERE is_holiday = false AND is_weekend = false;

-- å•é¡Œç™ºè¦‹1: ä¼‘æ—¥æœŸé–“ã®ç‰¹å¾´é‡æ¬ æ
-- åœŸæ—¥ç¥æ—¥ã®è¡Œã«ã¯business_day_numberãŒNULL
-- â†’ ä¼‘æ—¥ã®äºˆæ¸¬æ™‚ã«å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚°ç‰¹å¾´é‡ãŒå…¨ã¦æ¬ æ

-- å•é¡Œç™ºè¦‹2: è¤‡é›‘æ€§å¢—å¤§
-- å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚° = éå»ã®å–¶æ¥­æ—¥ç•ªå·ã‹ã‚‰è©²å½“è¡Œã‚’æ¤œç´¢
-- â†’ JOINãŒè¤‡é›‘åŒ–ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–

-- å•é¡Œç™ºè¦‹3: ä¿å®ˆæ€§ä½ä¸‹
-- ç¥æ—¥è¿½åŠ æ™‚ã®ç•ªå·æŒ¯ã‚Šç›´ã—å‡¦ç†ãŒè¤‡é›‘
```

**å–¶æ¥­æ—¥æŠ½å‡ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¸ã®è»¢æ›å­¦ç¿’**ï¼š
```python
# å­¦ç¿’è»¢æ›ç‚¹: 
# "è¤‡é›‘ãªè¨­è¨ˆã¯é–“é•ã„ã®ã‚µã‚¤ãƒ³"ã¨ã„ã†è¨­è¨ˆåŸå‰‡ã‚’æ€ã„å‡ºã™

# æ–°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: å–¶æ¥­æ—¥ã®ã¿æŠ½å‡º â†’ ã‚·ãƒ³ãƒ—ãƒ«ãªLAGé©ç”¨
simplified_approach = """
1. å–¶æ¥­æ—¥ã®ã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆWHEREæ¡ä»¶ã§çµã‚Šè¾¼ã¿ï¼‰
2. å–¶æ¥­æ—¥ãƒ‡ãƒ¼ã‚¿å†…ã§LAGé–¢æ•°é©ç”¨
3. LAG(actual_power, 24) = 1å–¶æ¥­æ—¥å‰ã®åŒæ™‚åˆ»
4. LAG(actual_power, 120) = 5å–¶æ¥­æ—¥å‰ã®åŒæ™‚åˆ»
"""

# å®Ÿè£…ä¾‹
business_days_query = """
CREATE TABLE business_days_lag AS
SELECT 
    date,
    hour,
    actual_power,
    -- å–¶æ¥­æ—¥ã®ã¿ã®ãƒ‡ãƒ¼ã‚¿ãªã®ã§ã€LAG(24) = 1å–¶æ¥­æ—¥å‰
    LAG(actual_power, 24) OVER (ORDER BY date, hour) as lag_1_business_day,
    LAG(actual_power, 120) OVER (ORDER BY date, hour) as lag_5_business_day
FROM power_weather_integrated
WHERE is_holiday = false AND is_weekend = false
ORDER BY date, hour;
"""

# åˆ©ç‚¹ã®ç¢ºèª:
# 1. ã‚·ãƒ³ãƒ—ãƒ«: WHEREæ¡ä»¶ã®ã¿ã§å–¶æ¥­æ—¥æŠ½å‡º
# 2. ç›´æ„Ÿçš„: LAG(24) = 1å–¶æ¥­æ—¥å‰ï¼ˆå–¶æ¥­æ—¥ãƒ‡ãƒ¼ã‚¿å†…ï¼‰
# 3. ä¿å®ˆæ€§: ç¥æ—¥è¿½åŠ æ™‚ã‚‚è‡ªå‹•å¯¾å¿œ
# 4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: ã‚·ãƒ³ãƒ—ãƒ«ãªLAGé–¢æ•°ã®ã¿
```

**LAGé–¢æ•°åˆ¶ç´„ç™ºè¦‹ãƒ»å¯¾å¿œå­¦ç¿’**ï¼š
```sql
-- BigQuery LAGé–¢æ•°ã®åˆ¶ç´„ç™ºè¦‹

-- è©¦è¡Œã‚¨ãƒ©ãƒ¼1: è¨ˆç®—å¼ä½¿ç”¨
LAG(actual_power, 24*1) OVER (ORDER BY date, hour)
-- ã‚¨ãƒ©ãƒ¼: Argument 2 to LAG must be a literal

-- è©¦è¡Œã‚¨ãƒ©ãƒ¼2: å¤‰æ•°ä½¿ç”¨  
DECLARE lag_offset INT64 DEFAULT 24;
LAG(actual_power, lag_offset) OVER (ORDER BY date, hour)
-- ã‚¨ãƒ©ãƒ¼: Argument 2 to LAG must be a literal

-- è§£æ±º: ãƒªãƒ†ãƒ©ãƒ«å€¤ã®ã¿ä½¿ç”¨å¯èƒ½
LAG(actual_power, 24) OVER (ORDER BY date, hour) as lag_1_business_day,
LAG(actual_power, 48) OVER (ORDER BY date, hour) as lag_2_business_day,
LAG(actual_power, 72) OVER (ORDER BY date, hour) as lag_3_business_day,
-- ... 30æ—¥åˆ†æ‰‹å‹•è¨˜è¿°

-- å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ: BigQueryã®åˆ¶ç´„ã‚’ç†è§£ã—ã¦è¨­è¨ˆé©å¿œ
```

**å¤‰åŒ–ç‡è¨ˆç®—ã®NULLå®‰å…¨æ€§å­¦ç¿’**ï¼š
```sql
-- æœ€åˆã®å¤‰åŒ–ç‡è¨ˆç®—ï¼ˆå±é™ºï¼‰
(actual_power - LAG(actual_power, 24) OVER (...)) / LAG(actual_power, 24) OVER (...) * 100

-- å•é¡Œ1: åˆ†æ¯ãŒ0ã®å ´åˆã®ã‚¼ãƒ­é™¤ç®—ã‚¨ãƒ©ãƒ¼
-- å•é¡Œ2: LAGãŒNULLã®å ´åˆã®å‡¦ç†

-- Stage 1: NULLIFä½¿ç”¨
(actual_power - LAG(actual_power, 24) OVER (...)) / 
NULLIF(LAG(actual_power, 24) OVER (...), 0) * 100

-- åˆ©ç‚¹: ã‚¼ãƒ­é™¤ç®—ã®å ´åˆã¯NULLè¿”å´ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ï¼‰

-- Stage 2: ç•°å¸¸å€¤æ¤œå‡ºè¿½åŠ 
CASE 
    WHEN LAG(actual_power, 24) OVER (...) IS NULL THEN NULL
    WHEN LAG(actual_power, 24) OVER (...) = 0 THEN NULL
    WHEN ABS((actual_power - LAG(actual_power, 24) OVER (...)) / 
             LAG(actual_power, 24) OVER (...) * 100) > 100 THEN NULL  -- 100%è¶…å¤‰åŒ–ã¯ç•°å¸¸
    ELSE (actual_power - LAG(actual_power, 24) OVER (...)) / 
         LAG(actual_power, 24) OVER (...) * 100
END as change_rate_1_business_day

-- å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ: ãƒ‡ãƒ¼ã‚¿å“è³ªã¨ã‚¨ãƒ©ãƒ¼è€æ€§ã®ä¸¡ç«‹
```

---

## ğŸš€ Phase 7å®Ÿè£…æº–å‚™: å®Ÿç”¨çš„MLè¨­è¨ˆæ€æƒ³ã®ç¢ºç«‹

### **ç‰¹å¾´é‡é¸æŠãƒ»é‡è¦åº¦è©•ä¾¡ã®äº‹å‰å­¦ç¿’**

**Random Forestäº‹å‰è©•ä¾¡ã®å®Ÿè£…å­¦ç¿’**ï¼š
```python
# XGBoostå®Ÿè£…å‰ã®äº‹å‰è©•ä¾¡æ‰‹æ³•å­¦ç¿’

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error

def preliminary_feature_evaluation():
    # BigQueryã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—
    query = """
    SELECT 
        actual_power,
        temperature_2m,
        CAST(is_weekend AS INT64) as is_weekend,
        CAST(is_holiday AS INT64) as is_holiday,
        hour,
        hour_sin,
        hour_cos,
        lag_1_day,
        lag_7_day,
        change_rate_1_day
    FROM `energy-env.dev_energy_data.ml_features`
    WHERE temperature_2m IS NOT NULL 
      AND lag_1_day IS NOT NULL
      AND RAND() < 0.1  -- 10%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆè¨ˆç®—åŠ¹ç‡ï¼‰
    """
    
    df = client.query(query).to_dataframe()
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†é›¢
    feature_cols = [col for col in df.columns if col != 'actual_power']
    X = df[feature_cols].fillna(0)  # æ¬ æå€¤å¯¾å¿œ
    y = df['actual_power']
    
    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # Random Forestå­¦ç¿’
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    
    # äºˆæ¸¬ãƒ»è©•ä¾¡
    y_pred = rf.predict(X_test)
    mape = mean_absolute_percentage_error(y_test, y_pred) * 100
    r2 = rf.score(X_test, y_test)
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    importance_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"Random Forest ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:")
    print(f"MAPE: {mape:.2f}%")
    print(f"RÂ²: {r2:.4f}")
    print("\nç‰¹å¾´é‡é‡è¦åº¦:")
    print(importance_df.to_string(index=False))
    
    return rf, importance_df

# å®Ÿè¡Œçµæœä¾‹ï¼ˆæœŸå¾…å€¤ï¼‰:
# MAPE: 15.3%
# RÂ²: 0.82
# 
# ç‰¹å¾´é‡é‡è¦åº¦:
#     feature  importance
#    lag_1_day      0.35  # å‰æ—¥åŒæ™‚åˆ»ãŒæœ€é‡è¦
#   is_holiday      0.22  # ç¥æ—¥åŠ¹æœãŒå¤§ãã„
#        hour       0.18  # æ™‚é–“å¸¯é‡è¦
# temperature_2m    0.12  # æ°—æ¸©ã‚‚ä¸€å®šã®å½±éŸ¿
#   is_weekend      0.08  # é€±æœ«åŠ¹æœ
#     hour_sin      0.03  # å¾ªç’°ç‰¹å¾´é‡ã¯è£œåŠ©çš„
#     hour_cos      0.02

# å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ: 
# 1. ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ + ãƒ©ã‚°ç‰¹å¾´é‡ã§å…¨ä½“ã®65%ã‚’å ã‚ã‚‹
# 2. æ°—è±¡ç‰¹å¾´é‡ã¯12%ç¨‹åº¦ï¼ˆè£œåŠ©çš„å½¹å‰²ï¼‰
# 3. XGBoostã§ã¯ã•ã‚‰ãªã‚‹ç²¾åº¦å‘ä¸ŠãŒæœŸå¾…ã§ãã‚‹
```

**XGBoostå®Ÿè£…è¨­è¨ˆã®äº‹å‰å­¦ç¿’**ï¼š
```python
# XGBoostå®Ÿè£…ã«å‘ã‘ãŸè¨­è¨ˆæ€æƒ³å­¦ç¿’

# å­¦ç¿’ã‚½ãƒ¼ã‚¹:
# 1. XGBoostå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
# 2. Kaggleæ™‚ç³»åˆ—ã‚³ãƒ³ãƒšä¸Šä½è§£æ³•
# 3. é›»åŠ›éœ€è¦äºˆæ¸¬ç ”ç©¶è«–æ–‡

xgboost_implementation_plan = {
    'model_architecture': {
        'algorithm': 'XGBRegressor',
        'objective': 'reg:squarederror',  # å›å¸°å•é¡Œ
        'eval_metric': ['rmse', 'mae'],   # è©•ä¾¡æŒ‡æ¨™
        'early_stopping': True,          # éå­¦ç¿’é˜²æ­¢
    },
    
    'hyperparameter_strategy': {
        'learning_rate': [0.01, 0.05, 0.1],     # å­¦ç¿’ç‡
        'max_depth': [3, 6, 9],                  # æœ¨ã®æ·±ã•
        'n_estimators': [100, 500, 1000],       # æœ¨ã®æ•°
        'subsample': [0.8, 0.9, 1.0],          # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡
        'colsample_bytree': [0.8, 0.9, 1.0],   # ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        'reg_alpha': [0, 0.1, 1.0],             # L1æ­£å‰‡åŒ–
        'reg_lambda': [1, 1.5, 2.0],            # L2æ­£å‰‡åŒ–
    },
    
    'cross_validation_design': {
        'method': 'TimeSeriesSplit',
        'n_splits': 5,
        'test_size': 30,  # 30æ—¥åˆ†ã‚’ãƒ†ã‚¹ãƒˆæœŸé–“
        'gap': 7,         # å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆé–“ã®æœŸé–“
    },
    
    'feature_engineering_priority': [
        'calendar_features',    # æœ€å„ªå…ˆï¼ˆç¥æ—¥åŠ¹æœå¤§ï¼‰
        'lag_features',         # é«˜å„ªå…ˆï¼ˆæ™‚ç³»åˆ—æœ¬è³ªï¼‰
        'moving_averages',      # ä¸­å„ªå…ˆï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
        'weather_features',     # ä¸­å„ªå…ˆï¼ˆåŸºæœ¬è¦å› ï¼‰
        'circular_features',    # ä½å„ªå…ˆï¼ˆå‘¨æœŸæ€§è£œå®Œï¼‰
        'interaction_features', # æœ€ä½å„ªå…ˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    ],
    
    'evaluation_strategy': {
        'primary_metric': 'MAPE',  # ãƒ“ã‚¸ãƒã‚¹è§£é‡ˆã—ã‚„ã™ã„
        'target_mape': '8-12%',    # å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ç›®æ¨™
        'condition_based_eval': {
            'peak_hours': 'hour IN (10, 11, 14, 15)',
            'weekdays': 'is_holiday = false AND is_weekend = false',
            'weekends': 'is_holiday = true OR is_weekend = true',
            'summer': 'month IN (7, 8, 9)',
            'winter': 'month IN (12, 1, 2)',
        }
    }
}

# å®Ÿè£…é †åºè¨ˆç”»:
# Week 1: åŸºæœ¬XGBoostå®Ÿè£… + ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´é‡
# Week 2: ãƒ©ã‚°ç‰¹å¾´é‡è¿½åŠ  + ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
# Week 3: å…¨ç‰¹å¾´é‡çµ±åˆ + ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
# Week 4: æ¡ä»¶åˆ¥è©•ä¾¡ + æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ç¢ºå®š
```

---

## ğŸ¯ Phase 7å®Ÿè£…ã«å‘ã‘ãŸå­¦ç¿’åŸºç›¤ç¢ºç«‹

### **æŠ€è¡“çš„è‡ªä¿¡ã®ç²å¾—éç¨‹**

#### **Pythoné«˜åº¦é–‹ç™ºã®ç¿’ç†Ÿ**
- **ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘è¨­è¨ˆ**: æ‰‹ç¶šãå‹â†’ã‚¯ãƒ©ã‚¹è¨­è¨ˆâ†’è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨
- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: try-exceptåŸºæœ¬â†’éšå±¤çš„ä¾‹å¤–å‡¦ç†â†’é‹ç”¨è€ƒæ…®è¨­è¨ˆ
- **CLI/ãƒ©ã‚¤ãƒ–ãƒ©ãƒªçµ±åˆ**: å˜æ©Ÿèƒ½ã‚¹ã‚¯ãƒªãƒ—ãƒˆâ†’ä¸¡å¯¾å¿œè¨­è¨ˆâ†’å†åˆ©ç”¨å¯èƒ½ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

#### **BigQuery/GCPçµ±åˆã®æ·±å±¤ç†è§£**
- **åˆ¶ç´„ç™ºè¦‹ãƒ»å›é¿**: ã‚¨ãƒ©ãƒ¼é­é‡â†’èª¿æŸ»ãƒ»ç†è§£â†’å›é¿æŠ€è¡“ä½“ç³»åŒ–
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**: åŸºæœ¬ã‚¯ã‚¨ãƒªâ†’ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­è¨ˆâ†’å‹çµ±ä¸€æˆ¦ç•¥
- **é‹ç”¨è¨­è¨ˆ**: é–‹ç™ºåŠ¹ç‡â†’æœ¬ç•ªæ€§èƒ½â†’ä¿å®ˆæ€§ã®ä¸‰ä½ä¸€ä½“è¨­è¨ˆ

#### **ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»çµ±è¨ˆæ€è€ƒã®æˆç†Ÿ**
- **æ¢ç´¢çš„åˆ†æ**: å˜ç´”é›†è¨ˆâ†’å¤šæ¬¡å…ƒåˆ†æâ†’Ridge Plotå®šé‡è©•ä¾¡
- **çµ±è¨ˆçš„åˆ¤æ–­**: på€¤é‡è¦–â†’åŠ¹æœé‡é‡è¦–â†’å®Ÿç”¨æ€§é‡è¦–
- **å®Ÿè¨¼çš„æ„æ€æ±ºå®š**: ç†è«–å…ˆè¡Œâ†’ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼â†’å®Ÿè¨¼çš„é¸æŠ

### **å•é¡Œè§£æ±ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºç«‹**

#### **å­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«ã®æœ€é©åŒ–**
1. **å•é¡Œç™ºç”Ÿ**: ã‚¨ãƒ©ãƒ¼ãƒ»åˆ¶ç´„ãƒ»éåŠ¹ç‡æ€§ã®èªè­˜
2. **ä»®èª¬å½¢æˆ**: åŸå› æ¨æ¸¬ãƒ»è§£æ±ºç­–å€™è£œã®åˆ—æŒ™
3. **èª¿æŸ»ãƒ»æ¤œè¨¼**: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»StackOverflowãƒ»å®Ÿé¨“
4. **å®Ÿè£…ãƒ»æ¤œè¨¼**: æ®µéšçš„å®Ÿè£…ãƒ»ãƒ†ã‚¹ãƒˆãƒ»æ”¹è‰¯
5. **ä½“ç³»åŒ–**: ãƒ‘ã‚¿ãƒ¼ãƒ³åŒ–ãƒ»å†åˆ©ç”¨å¯èƒ½ãªçŸ¥è­˜ã¨ã—ã¦è“„ç©

#### **èª¿æŸ»ãƒ»å­¦ç¿’æ‰‹æ³•ã®ç¢ºç«‹**
```python
# åŠ¹æœçš„ãªèª¿æŸ»æ‰‹æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³
investigation_patterns = {
    'error_resolution': [
        'ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ­£ç¢ºãªè§£èª­',
        'å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã®åˆ¶ç´„ç¢ºèª',
        'Stack Overflowé¡ä¼¼ä¾‹èª¿æŸ»',
        'æ®µéšçš„è§£æ±ºç­–å®Ÿè£…ãƒ»æ¤œè¨¼'
    ],
    'performance_optimization': [
        'ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šï¼ˆå®Ÿæ¸¬ãƒ»ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ï¼‰',
        'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹èª¿æŸ»',
        'æ®µéšçš„æ”¹å–„ãƒ»åŠ¹æœæ¸¬å®š',
        'é‹ç”¨è² è·vsåŠ¹æœã®ãƒãƒ©ãƒ³ã‚¹è©•ä¾¡'
    ],
    'design_decision': [
        'è¦ä»¶ãƒ»åˆ¶ç´„ã®æ˜ç¢ºåŒ–',
        'è¤‡æ•°é¸æŠè‚¢ã®å®¢è¦³çš„æ¯”è¼ƒ',
        'å®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãè©•ä¾¡',
        'å°†æ¥æ‹¡å¼µæ€§ãƒ»ä¿å®ˆæ€§è€ƒæ…®'
    ]
}
```

### **Phase 7æˆåŠŸã®ãŸã‚ã®å­¦ç¿’åŸºç›¤**

#### **XGBoostå®Ÿè£…ã«å¿…è¦ãªçŸ¥è­˜ä½“ç³»**
- âœ… **Python MLåŸºç¤**: scikit-learnã€pandasã€numpyæ“ä½œ
- âœ… **BigQueryçµ±åˆ**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»å‰å‡¦ç†
- âœ… **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: æ™‚ç³»åˆ—ãƒ»ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ»æ°—è±¡ç‰¹å¾´é‡è¨­è¨ˆ
- âœ… **è©•ä¾¡ãƒ»æ¤œè¨¼**: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»æ¡ä»¶åˆ¥è©•ä¾¡è¨­è¨ˆ
- âœ… **é‹ç”¨è€ƒæ…®**: ãƒ¢ãƒ‡ãƒ«æ›´æ–°ãƒ»äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ

#### **å®Ÿè£…ä¸Šã®è‡ªä¿¡ç²å¾—äº‹é …**
- âœ… **åˆ¶ç´„å›é¿èƒ½åŠ›**: BigQueryåˆ¶ç´„ã®ä½“ç³»çš„ç†è§£ãƒ»å›é¿æŠ€è¡“
- âœ… **ã‚¨ãƒ©ãƒ¼å¯¾å¿œèƒ½åŠ›**: éšå±¤çš„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°çµ±åˆ
- âœ… **è¨­è¨ˆåˆ¤æ–­èƒ½åŠ›**: å®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæŠ€è¡“é¸æŠ
- âœ… **å“è³ªä¿è¨¼èƒ½åŠ›**: ãƒ†ã‚¹ãƒˆè¨­è¨ˆãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
- âœ… **çµ±åˆé–‹ç™ºèƒ½åŠ›**: è¤‡æ•°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åŠ¹ç‡çš„çµ±åˆ

#### **ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤å‰µå‡ºã¸ã®æº–å‚™**
- âœ… **å®Ÿç”¨æ€§é‡è¦–**: ç†è«–çš„ç¾ã—ã•ã‚ˆã‚Šå®Ÿç”¨æ€§å„ªå…ˆã®åˆ¤æ–­åŸºæº–
- âœ… **æ®µéšçš„å®Ÿè£…**: ç¢ºå®Ÿãªç†è§£ã«åŸºã¥ãå …å®Ÿãªé–‹ç™ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- âœ… **é‹ç”¨è€ƒæ…®**: æ—¥å¸¸åˆ©ç”¨ã‚’æƒ³å®šã—ãŸä½¿ã„ã‚„ã™ã•ã®è¿½æ±‚
- âœ… **æ‹¡å¼µæ€§è¨­è¨ˆ**: å°†æ¥ã®æ©Ÿèƒ½è¿½åŠ ãƒ»æ”¹è‰¯ã¸ã®å¯¾å¿œåŠ›

---

## ğŸš€ ãƒ¡ã‚¿å­¦ç¿’èƒ½åŠ›ã®ç²å¾—

### **å­¦ç¿’ã®å­¦ç¿’ï¼ˆLearning How to Learnï¼‰**

#### **åŠ¹ç‡çš„ãªå­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºç«‹**
```python
# å­¦ç¿’åŠ¹ç‡åŒ–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
efficient_learning_patterns = {
    'error_driven_learning': {
        'trigger': 'ã‚¨ãƒ©ãƒ¼é­é‡ãƒ»åˆ¶ç´„ç™ºè¦‹',
        'process': [
            '1. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ­£ç¢ºãªç†è§£',
            '2. æ ¹æœ¬åŸå› ã®ä»®èª¬å½¢æˆ',
            '3. å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆâ†’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£â†’å®Ÿé¨“ã®é †ã§èª¿æŸ»',
            '4. æ®µéšçš„è§£æ±ºãƒ»æ¤œè¨¼',
            '5. ãƒ‘ã‚¿ãƒ¼ãƒ³åŒ–ãƒ»ä½“ç³»åŒ–'
        ],
        'examples': [
            'BigQueryè¤‡æ•°ã‚«ãƒ©ãƒ INå¥åˆ¶ç´„ â†’ CONCATæ–¹å¼ç™ºè¦‹',
            'Shift-JISã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œ â†’ å¤šæ®µéšãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…',
            'LAGé–¢æ•°åˆ¶ç´„ â†’ ãƒªãƒ†ãƒ©ãƒ«å€¤ã®ã¿åˆ¶ç´„ç†è§£'
        ]
    },
    
    'design_pattern_learning': {
        'trigger': 'è¨­è¨ˆã®è¤‡é›‘åŒ–ãƒ»ä¿å®ˆæ€§ä½ä¸‹',
        'process': [
            '1. è¤‡é›‘æ€§ã®åŸå› åˆ†æ',
            '2. è¨­è¨ˆåŸå‰‡ã®é©ç”¨æ¤œè¨',
            '3. ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…',
            '4. æ”¹å–„åŠ¹æœã®æ¸¬å®š',
            '5. è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã—ã¦ä½“ç³»åŒ–'
        ],
        'examples': [
            'business_day_numberè¤‡é›‘åŒ– â†’ å–¶æ¥­æ—¥æŠ½å‡ºã‚·ãƒ³ãƒ—ãƒ«åŒ–',
            'æ‰‹ç¶šãå‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ â†’ ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘è¨­è¨ˆ',
            'å€‹åˆ¥ã‚¨ãƒ©ãƒ¼å‡¦ç† â†’ éšå±¤çš„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°'
        ]
    },
    
    'data_driven_decision': {
        'trigger': 'æ‰‹æ³•é¸æŠãƒ»è¨­è¨ˆåˆ¤æ–­ã®å¿…è¦æ€§',
        'process': [
            '1. åˆ¤æ–­åŸºæº–ã®æ˜ç¢ºåŒ–',
            '2. å®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿ã®åé›†ãƒ»åˆ†æ',
            '3. å®šé‡çš„æ¯”è¼ƒè©•ä¾¡',
            '4. å®Ÿç”¨æ€§ãƒ»é‹ç”¨æ€§è€ƒæ…®',
            '5. å®Ÿè¨¼ã«åŸºã¥ãæ±ºå®š'
        ],
        'examples': [
            'åƒè‘‰æ°—æ¸©ãƒ‡ãƒ¼ã‚¿æ¡ç”¨ï¼ˆRidge Plotåˆ†æåŸºæº–ï¼‰',
            'XGBoosté¸æŠï¼ˆé›»åŠ›éœ€è¦ç‰¹æ€§åˆ†æåŸºæº–ï¼‰',
            'EXTERNAL TABLE vs ç‰©ç†ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–ï¼‰'
        ]
    }
}
```

#### **æŠ€è¡“ç¿’å¾—ã®åŠ é€Ÿãƒ‘ã‚¿ãƒ¼ãƒ³**
```python
# å­¦ç¿’åŠ é€Ÿã®ãƒ¡ã‚¿æŠ€è¡“
learning_acceleration_techniques = {
    'progressive_complexity': {
        'principle': 'æ®µéšçš„è¤‡é›‘åŒ–ã«ã‚ˆã‚‹ç¢ºå®Ÿãªç†è§£',
        'implementation': [
            'Stage 1: æœ€å°å‹•ä½œä¾‹ã®å®Ÿè£…',
            'Stage 2: ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ãƒ»å¢ƒç•Œå€¤å¯¾å¿œ',
            'Stage 3: é‹ç”¨ãƒ»ä¿å®ˆæ€§è€ƒæ…®',
            'Stage 4: æœ€é©åŒ–ãƒ»æ‹¡å¼µæ€§å‘ä¸Š'
        ],
        'concrete_example': {
            'argparseå­¦ç¿’': [
                'Stage 1: åŸºæœ¬å¼•æ•°1å€‹',
                'Stage 2: è¤‡æ•°å¼•æ•°ãƒ»å‹å¤‰æ›',
                'Stage 3: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãƒ»ãƒ˜ãƒ«ãƒ—',
                'Stage 4: æ’ä»–åˆ¶å¾¡ãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³'
            ]
        }
    },
    
    'failure_as_teacher': {
        'principle': 'å¤±æ•—ã‹ã‚‰æœ€å¤§ã®å­¦ç¿’åŠ¹æœã‚’å¾—ã‚‹',
        'practice': [
            'å¤±æ•—ã®æ ¹æœ¬åŸå› åˆ†æï¼ˆæŠ€è¡“çš„ãƒ»è¨­è¨ˆçš„ãƒ»èªè­˜çš„ï¼‰',
            'åŒæ§˜ã®å¤±æ•—é˜²æ­¢ç­–ã®ä½“ç³»åŒ–',
            'å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰äºˆé˜²çš„è¨­è¨ˆåŸå‰‡æŠ½å‡º',
            'å¤±æ•—äº‹ä¾‹ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹åŒ–'
        ],
        'learned_principles': [
            'ã€Œè¤‡é›‘ãªè¨­è¨ˆã¯é–“é•ã„ã®ã‚µã‚¤ãƒ³ã€',
            'ã€Œå®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåˆ¤æ–­ã€',
            'ã€Œåˆ¶ç´„ç†è§£ãŒè¨­è¨ˆå“è³ªã‚’æ±ºã‚ã‚‹ã€',
            'ã€Œæ®µéšçš„å®Ÿè£…ãŒæœ€ã‚‚ç¢ºå®Ÿã€'
        ]
    },
    
    'tool_mastery_strategy': {
        'principle': 'ãƒ„ãƒ¼ãƒ«ã®æœ¬è³ªç†è§£ã«ã‚ˆã‚‹åŠ¹ç‡çš„ç¿’å¾—',
        'approach': [
            '1. ãƒ„ãƒ¼ãƒ«ã®è¨­è¨ˆæ€æƒ³ãƒ»åˆ¶ç´„ã®ç†è§£',
            '2. åŸºæœ¬æ“ä½œã®å®Œå…¨ç¿’å¾—',
            '3. åˆ¶ç´„ãƒ»é™ç•Œã®ä½“é¨“çš„å­¦ç¿’',
            '4. å¿œç”¨ãƒ»çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¿’å¾—',
            '5. é‹ç”¨ãƒ»ä¿å®ˆè¦³ç‚¹ã®ç¿’å¾—'
        ],
        'mastered_tools': {
            'BigQuery': 'åˆ¶ç´„ç†è§£ â†’ å›é¿æŠ€è¡“ â†’ æœ€é©åŒ–è¨­è¨ˆ',
            'Python': 'åŸºæœ¬æ–‡æ³• â†’ OOP â†’ è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³',
            'Git': 'åŸºæœ¬æ“ä½œ â†’ ãƒ–ãƒ©ãƒ³ãƒæˆ¦ç•¥ â†’ ãƒãƒ¼ãƒ é–‹ç™º',
            'GCS': 'APIæ“ä½œ â†’ è‡ªå‹•åŒ– â†’ ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†'
        }
    }
}
```

### **çŸ¥è­˜ã®ä½“ç³»åŒ–ãƒ»å†åˆ©ç”¨èƒ½åŠ›**

#### **ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ãƒ»æŠ½è±¡åŒ–èƒ½åŠ›ã®ç™ºé”**
```python
# æŠ½è±¡åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºç«‹
abstraction_patterns = {
    'technical_patterns': {
        'error_handling': {
            'pattern': 'éšå±¤çš„ä¾‹å¤–å‡¦ç† + ãƒ­ã‚°çµ±åˆ + ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£',
            'reusable_template': '''
            try:
                logger.info(f"Starting {operation_name}")
                result = risky_operation()
                logger.info(f"Successfully completed {operation_name}")
                return result
            except SpecificExpectedException as e:
                logger.warning(f"Expected issue in {operation_name}: {e}")
                return handle_expected_case(e)
            except Exception as e:
                logger.error(f"Unexpected error in {operation_name}: {e}")
                raise
            '''
        },
        
        'data_validation': {
            'pattern': 'äº‹å‰ãƒã‚§ãƒƒã‚¯ + æ®µéšçš„æ¤œè¨¼ + ç•°å¸¸å€¤å¯¾å¿œ',
            'reusable_template': '''
            def validate_and_process(data, validation_rules):
                # Stage 1: åŸºæœ¬å½¢å¼ãƒã‚§ãƒƒã‚¯
                basic_validation(data)
                
                # Stage 2: ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼
                business_validation(data, validation_rules)
                
                # Stage 3: ç•°å¸¸å€¤æ¤œå‡ºãƒ»å¯¾å¿œ
                cleaned_data = handle_anomalies(data)
                
                return cleaned_data
            '''
        },
        
        'incremental_implementation': {
            'pattern': 'æœ€å°å‹•ä½œä¾‹ â†’ æ®µéšçš„æ‹¡å¼µ â†’ æœ€çµ‚çµ±åˆ',
            'application_areas': [
                'APIçµ±åˆ', 'ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³', 
                'MLç‰¹å¾´é‡è¨­è¨ˆ', 'ãƒ†ã‚¹ãƒˆå®Ÿè£…'
            ]
        }
    },
    
    'design_patterns': {
        'constraint_driven_design': {
            'principle': 'åˆ¶ç´„ç†è§£ãŒè¨­è¨ˆå“è³ªã‚’æ±ºã‚ã‚‹',
            'application_examples': [
                'BigQueryåˆ¶ç´„ â†’ CONCATæ–¹å¼ãƒ»MERGEæ–‡æ´»ç”¨',
                'LAGé–¢æ•°åˆ¶ç´„ â†’ ãƒªãƒ†ãƒ©ãƒ«å€¤è¨­è¨ˆ',
                'APIåˆ¶é™ â†’ æœŸé–“åˆ†å‰²ãƒ»ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥'
            ]
        },
        
        'evidence_based_architecture': {
            'principle': 'å®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæŠ€è¡“é¸æŠ',
            'decision_framework': [
                '1. åˆ¤æ–­åŸºæº–ã®å®šé‡åŒ–',
                '2. å®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿ã®åé›†',
                '3. å®¢è¦³çš„æ¯”è¼ƒè©•ä¾¡',
                '4. å®Ÿç”¨æ€§ãƒ»é‹ç”¨æ€§è€ƒæ…®',
                '5. ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ±ºå®š'
            ]
        },
        
        'pragmatic_over_perfect': {
            'principle': 'ç†è«–çš„å®Œç’§æ€§ã‚ˆã‚Šå®Ÿç”¨çš„ä¾¡å€¤',
            'examples': [
                'åƒè‘‰æ°—æ¸©æ¡ç”¨ï¼ˆä»£è¡¨æ€§ < äºˆæ¸¬ç²¾åº¦ï¼‰',
                'EXTERNAL TABLEæ´»ç”¨ï¼ˆæ­£è¦åŒ– < é–‹ç™ºåŠ¹ç‡ï¼‰',
                'NULLå€¤è¨±å®¹ï¼ˆå®Œå…¨æ€§ < å®Ÿè£…ã‚·ãƒ³ãƒ—ãƒ«æ€§ï¼‰'
            ]
        }
    }
}
```

#### **æŠ€è¡“çŸ¥è­˜ã®æ§‹é€ åŒ–ãƒ»æ¤œç´¢èƒ½åŠ›**
```python
# æŠ€è¡“çŸ¥è­˜ã®ä½“ç³»çš„ç®¡ç†
knowledge_management_system = {
    'error_database': {
        'structure': {
            'error_signature': 'ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ»ç’°å¢ƒæƒ…å ±',
            'root_cause': 'æ ¹æœ¬åŸå› ã®åˆ†æçµæœ',
            'solution_path': 'è§£æ±ºã«è‡³ã‚‹ãƒ—ãƒ­ã‚»ã‚¹',
            'prevention_strategy': 'å†ç™ºé˜²æ­¢ç­–',
            'related_patterns': 'é¡ä¼¼å•é¡Œãƒ»è§£æ±ºãƒ‘ã‚¿ãƒ¼ãƒ³'
        },
        'examples': {
            'bigquery_multiple_column_in': {
                'signature': 'Subquery with multiple columns not supported in IN',
                'root_cause': 'BigQueryã®INè¿°èªåˆ¶ç´„',
                'solution': 'CONCATæ–¹å¼ã«ã‚ˆã‚‹è¤‡åˆã‚­ãƒ¼è¡¨ç¾',
                'prevention': 'äº‹å‰åˆ¶ç´„èª¿æŸ»ãƒ»ä»£æ›¿æ‰‹æ³•æº–å‚™',
                'related': ['DELETE JOINåˆ¶ç´„', 'WITH UPDATEåˆ¶ç´„']
            }
        }
    },
    
    'design_decision_database': {
        'structure': {
            'decision_context': 'åˆ¤æ–­ãŒå¿…è¦ã«ãªã£ãŸèƒŒæ™¯',
            'evaluation_criteria': 'åˆ¤æ–­åŸºæº–ãƒ»è©•ä¾¡è»¸',
            'options_analysis': 'é¸æŠè‚¢ã®æ¯”è¼ƒåˆ†æ',
            'final_decision': 'æœ€çµ‚æ±ºå®šãƒ»ãã®æ ¹æ‹ ',
            'outcome_validation': 'çµæœæ¤œè¨¼ãƒ»å­¦ç¿’'
        },
        'examples': {
            'chiba_weather_adoption': {
                'context': '9éƒ½çœŒæ°—æ¸©ãƒ‡ãƒ¼ã‚¿ã®ä»£è¡¨å€¤é¸æŠ',
                'criteria': 'äºˆæ¸¬å¯èƒ½æ€§ï¼ˆãƒ¬ãƒ³ã‚¸å¹…æœ€å°åŒ–ï¼‰',
                'analysis': 'Ridge Plotå®šé‡æ¯”è¼ƒ',
                'decision': 'åƒè‘‰æ¡ç”¨ï¼ˆå®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿åŸºæº–ï¼‰',
                'validation': 'äºˆæ¸¬ç²¾åº¦å‘ä¸Šç¢ºèªäºˆå®š'
            }
        }
    },
    
    'implementation_pattern_library': {
        'categories': [
            'data_processing', 'error_handling', 'api_integration',
            'bigquery_optimization', 'testing_strategy', 'logging_design'
        ],
        'pattern_template': {
            'problem_description': 'è§£æ±ºã™ã‚‹å•é¡Œãƒ»èª²é¡Œ',
            'solution_approach': 'è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒãƒ»è¨­è¨ˆæ€æƒ³',
            'implementation_code': 'å®Ÿè£…ã‚³ãƒ¼ãƒ‰ä¾‹',
            'usage_guidelines': 'ä½¿ç”¨å ´é¢ãƒ»æ³¨æ„ç‚¹',
            'variation_examples': 'ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»å¿œç”¨ä¾‹'
        }
    }
}
```

---

## ğŸ¯ Phase 7ä»¥é™ã§ã®å­¦ç¿’ç¶™ç¶šæˆ¦ç•¥

### **å­¦ç¿’ã®ç¶™ç¶šãƒ»æ·±åŒ–æ–¹é‡**

#### **æŠ€è¡“çš„æ·±åŒ–ã®æ–¹å‘æ€§**
```python
learning_continuation_strategy = {
    'immediate_next_phase': {
        'focus': 'XGBoostå®Ÿè£…ãƒ»MLé‹ç”¨ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰',
        'learning_goals': [
            'ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ä½“ç³»çš„æ‰‹æ³•',
            'ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®æ·±å±¤ç†è§£',
            'Feature Importanceåˆ†æãƒ»è§£é‡ˆæŠ€è¡“',
            'MLOpsãƒ»ãƒ¢ãƒ‡ãƒ«é‹ç”¨ã®å®Ÿç”¨è¨­è¨ˆ',
            'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰æŠ€è¡“'
        ],
        'learning_approach': [
            'å®Ÿè£… â†’ è©•ä¾¡ â†’ æ”¹è‰¯ã®ã‚µã‚¤ã‚¯ãƒ«é«˜é€ŸåŒ–',
            'å®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ€é©åŒ–',
            'é‹ç”¨ãƒ»ä¿å®ˆæ€§ã‚’è€ƒæ…®ã—ãŸè¨­è¨ˆ',
            'ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤æœ€å¤§åŒ–ã®è¦³ç‚¹çµ±åˆ'
        ]
    },
    
    'medium_term_expansion': {
        'focus': 'ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ»MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°çµ±åˆ',
        'target_areas': [
            'Apache Airflowãƒ»ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è‡ªå‹•åŒ–',
            'Dockerãƒ»Kubernetesãƒ»ã‚³ãƒ³ãƒ†ãƒŠé‹ç”¨',
            'Terraformãƒ»Infrastructure as Code',
            'ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»SREå®Ÿè·µ',
            'ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ'
        ],
        'learning_principle': 'å®Ÿç”¨æ€§é‡è¦–ãƒ»æ®µéšçš„ç¿’å¾—ç¶™ç¶š'
    },
    
    'long_term_vision': {
        'focus': 'ã‚·ãƒ‹ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢é ˜åŸŸ',
        'target_capabilities': [
            'ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆãƒ»æŠ€è¡“é¸æŠãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—',
            'ãƒãƒ¼ãƒ é–‹ç™ºãƒ»æŠ€è¡“æŒ‡å°èƒ½åŠ›',
            'ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤å‰µå‡ºãƒ»ROIæœ€å¤§åŒ–',
            'æ–°æŠ€è¡“è©•ä¾¡ãƒ»å°å…¥åˆ¤æ–­èƒ½åŠ›',
            'ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆæ€è€ƒãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¾¡å€¤é‡è¦–'
        ]
    }
}
```

#### **å­¦ç¿’åŠ¹ç‡åŒ–ãƒ»å“è³ªå‘ä¸Šã®ç¶™ç¶š**
```python
# å­¦ç¿’å“è³ªå‘ä¸Šã®ãƒ¡ã‚¿æˆ¦ç•¥
learning_quality_improvement = {
    'depth_over_breadth': {
        'principle': 'åºƒãæµ…ãã‚ˆã‚Šã€ç‹­ãæ·±ãç†è§£',
        'practice': [
            'é¸æŠã—ãŸæŠ€è¡“ã®æœ¬è³ªãƒ»åˆ¶ç´„ã¾ã§ç†è§£',
            'å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã¾ã§ã®ç¢ºå®Ÿãªç¿’å¾—',
            'å¿œç”¨ãƒ»çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä½“å¾—',
            'ä»–è€…ã«æ•™ãˆã‚‰ã‚Œã‚‹ãƒ¬ãƒ™ãƒ«ã¾ã§æ·±åŒ–'
        ]
    },
    
    'documentation_as_learning': {
        'principle': 'èª¬æ˜ã§ãã‚‹ã“ã¨ãŒç†è§£ã®è¨¼æ˜',
        'practice': [
            'å­¦ç¿’éç¨‹ã®è©³ç´°è¨˜éŒ²',
            'å¤±æ•—ãƒ»æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä½“ç³»åŒ–',
            'å®Ÿè£…ã‚³ãƒ¼ãƒ‰ãƒ»è¨­è¨ˆåˆ¤æ–­ã®è§£èª¬',
            'ä»–è€…ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç²å¾—'
        ]
    },
    
    'continuous_reflection': {
        'principle': 'æŒ¯ã‚Šè¿”ã‚ŠãŒæ¬¡ã®æˆé•·ã‚’ç”Ÿã‚€',
        'practice': [
            'å®šæœŸçš„ãªå­¦ç¿’åŠ¹æœæ¸¬å®š',
            'å®Ÿè£…å“è³ªãƒ»è¨­è¨ˆåˆ¤æ–­ã®äº‹å¾Œè©•ä¾¡',
            'å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ”¹å–„ãƒ»æœ€é©åŒ–',
            'çŸ¥è­˜ä½“ç³»ã®æ•´ç†ãƒ»å†æ§‹ç¯‰'
        ]
    }
}
```

### **å®Ÿç”¨çš„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ã—ã¦ã®æˆé•·æŒ‡æ¨™**

#### **æŠ€è¡“åŠ›æŒ‡æ¨™ã®è¨­å®š**
```python
technical_growth_metrics = {
    'problem_solving_speed': {
        'phase1_6_baseline': 'ã‚¨ãƒ©ãƒ¼è§£æ±º: å¹³å‡2-3æ™‚é–“',
        'target_improvement': 'ã‚¨ãƒ©ãƒ¼è§£æ±º: å¹³å‡30åˆ†-1æ™‚é–“',
        'measurement': 'åŒç¨®å•é¡Œã®è§£æ±ºæ™‚é–“çŸ­ç¸®',
        'improvement_strategy': 'ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ãƒ»çŸ¥è­˜ä½“ç³»åŒ–'
    },
    
    'design_quality': {
        'phase1_6_baseline': 'å®Ÿè£…å¾Œãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°é »ç™º',
        'target_improvement': 'åˆå›å®Ÿè£…ã§é‹ç”¨å“è³ªé”æˆ',
        'measurement': 'ã‚³ãƒ¼ãƒ‰å¤‰æ›´é »åº¦ãƒ»ä¿å®ˆæ€§æŒ‡æ¨™',
        'improvement_strategy': 'åˆ¶ç´„ç†è§£ãƒ»è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨'
    },
    
    'integration_capability': {
        'phase1_6_baseline': 'å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå®Ÿè£…',
        'target_improvement': 'ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰',
        'measurement': 'ã‚·ã‚¹ãƒ†ãƒ çµ±åˆæˆåŠŸç‡ãƒ»å“è³ª',
        'improvement_strategy': 'ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆãƒ»é‹ç”¨è€ƒæ…®'
    },
    
    'business_value_creation': {
        'phase1_6_baseline': 'æŠ€è¡“å®Ÿè£…ä¸­å¿ƒ',
        'target_improvement': 'ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤æœ€å¤§åŒ–ä¸­å¿ƒ',
        'measurement': 'å®Ÿç”¨æ€§ãƒ»ROIãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦',
        'improvement_strategy': 'ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆæ€è€ƒãƒ»ä¾¡å€¤é‡è¦–è¨­è¨ˆ'
    }
}
```

#### **ã‚­ãƒ£ãƒªã‚¢ç›®æ¨™ã¨ã®é€£æº**
```python
career_alignment_strategy = {
    'immediate_goal': {
        'target': 'å¹´å700ä¸‡å††ä»¥ä¸Šãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢è·',
        'required_evidence': [
            'ã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å®Œæˆãƒ»é‹ç”¨',
            'BigQueryãƒ»GCPãƒ»Pythoné«˜åº¦æ´»ç”¨å®Ÿç¸¾',
            'MLå®Ÿè£…ãƒ»é‹ç”¨ãƒ»æ”¹å–„ã®å®Ÿè¨¼ä¾‹',
            'ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆãƒ»å•é¡Œè§£æ±ºèƒ½åŠ›ã®å®Ÿä¾‹'
        ],
        'portfolio_preparation': [
            'GitHubå…¬é–‹ãƒ»æŠ€è¡“ãƒ–ãƒ­ã‚°åŸ·ç­†',
            'æŠ€è¡“é¸æŠãƒ»è¨­è¨ˆåˆ¤æ–­ã®æ ¹æ‹ èª¬æ˜',
            'å®Ÿè£…ãƒ»é‹ç”¨ãƒ»æ”¹å–„ã®å…¨ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¨¼',
            'ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤å‰µå‡ºã®å…·ä½“ä¾‹'
        ]
    },
    
    'skill_market_alignment': {
        'high_demand_skills': [
            'BigQueryãƒ»GCPãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£',
            'Pythonãƒ»MLãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³',
            'Dockerãƒ»Kubernetesãƒ»MLOps',
            'ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆãƒ»å•é¡Œè§£æ±ºãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³'
        ],
        'differentiation_strategy': [
            'å®Ÿç”¨ã‚·ã‚¹ãƒ†ãƒ å®Œæˆå®Ÿç¸¾',
            'åˆ¶ç´„ç†è§£ãƒ»å›é¿æŠ€è¡“ã®æ·±å±¤çŸ¥è­˜',
            'å®Ÿè¨¼çš„åˆ¤æ–­ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³æ€è€ƒ',
            'é‹ç”¨ãƒ»ä¿å®ˆã‚’è€ƒæ…®ã—ãŸå®Ÿç”¨è¨­è¨ˆ'
        ]
    }
}
```

---

## ğŸ† ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæˆæœç·æ‹¬ãƒ»å­¦ç¿’ä¾¡å€¤

### **Phase 1-6å®Œäº†æ™‚ç‚¹ã§ã®å®Ÿç¸¾**

#### **æ§‹ç¯‰ã‚·ã‚¹ãƒ†ãƒ ãƒ»æŠ€è¡“åŸºç›¤**
- **æ±äº¬é›»åŠ›ãƒ‡ãƒ¼ã‚¿è‡ªå‹•åé›†**: 30ãƒ¶æœˆåˆ†ï¼ˆ21,888ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰å®Œå…¨è‡ªå‹•åŒ–
- **Google Cloudçµ±åˆåŸºç›¤**: GCS + BigQueryç‰©ç†ãƒ†ãƒ¼ãƒ–ãƒ«æœ€é©è¨­è¨ˆ
- **æ°—è±¡ãƒ‡ãƒ¼ã‚¿çµ±åˆ**: 9éƒ½çœŒÃ—2.5å¹´åˆ†ï¼ˆ22ä¸‡ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰çµ±åˆå‡¦ç†å®Œäº†
- **çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒˆ**: é›»åŠ›Ã—æ°—è±¡Ã—ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼Ã—ç¥æ—¥çµ±åˆãƒ“ãƒ¥ãƒ¼ï¼ˆ197,856ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰
- **ç‰¹å¾´é‡åˆ†æåŸºç›¤**: Ridge Plotãƒ»çµ±è¨ˆåˆ†æã«ã‚ˆã‚‹åƒè‘‰æ°—æ¸©æ¡ç”¨æ±ºå®š
- **MLå®Ÿè£…æº–å‚™**: XGBoostç‰¹å¾´é‡æˆ¦ç•¥ç¢ºå®šãƒ»å®Ÿè£…è¨­è¨ˆå®Œäº†

#### **æŠ€è¡“ç¿’å¾—ã®è³ªã¨æ·±åº¦**
- **Pythoné«˜åº¦é–‹ç™º**: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»CLIè¨­è¨ˆã®å®Ÿè·µç¿’å¾—
- **BigQueryå°‚é–€çŸ¥è­˜**: åˆ¶ç´„ç†è§£ãƒ»å›é¿æŠ€è¡“ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ä½“ç³»çš„ç¿’å¾—
- **ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: ETLè¨­è¨ˆãƒ»å¤§è¦æ¨¡å‡¦ç†ãƒ»å“è³ªç®¡ç†ã®å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ç¿’å¾—
- **çµ±è¨ˆãƒ»åˆ†ææ€è€ƒ**: æ¢ç´¢çš„åˆ†æãƒ»å®Ÿè¨¼çš„åˆ¤æ–­ãƒ»å®Ÿç”¨æ€§é‡è¦–ã®æ€è€ƒæ³•ç¢ºç«‹
- **ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆèƒ½åŠ›**: åˆ¶ç´„è€ƒæ…®ãƒ»é‹ç”¨é‡è¦–ãƒ»æ‹¡å¼µæ€§è¨­è¨ˆã®å®Ÿè·µçš„èƒ½åŠ›

#### **å•é¡Œè§£æ±ºãƒ»å­¦ç¿’èƒ½åŠ›ã®ç™ºé”**
- **ã‚¨ãƒ©ãƒ¼è§£æ±ºãƒ‘ã‚¿ãƒ¼ãƒ³**: ä½“ç³»çš„èª¿æŸ»â†’æ ¹æœ¬åŸå› åˆ†æâ†’äºˆé˜²çš„è¨­è¨ˆã®ç¢ºç«‹
- **åˆ¶ç´„å›é¿æŠ€è¡“**: BigQueryãƒ»APIãƒ»ãƒ‡ãƒ¼ã‚¿å‡¦ç†åˆ¶ç´„ã®ç™ºè¦‹ãƒ»å›é¿æŠ€è¡“ä½“ç³»
- **å®Ÿè¨¼çš„æ„æ€æ±ºå®š**: ç†è«–ã‚ˆã‚Šå®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿é‡è¦–ã®åˆ¤æ–­ãƒ—ãƒ­ã‚»ã‚¹ç¢ºç«‹
- **æ®µéšçš„å®Ÿè£…**: ç¢ºå®Ÿãªç†è§£ã«åŸºã¥ãå …å®Ÿãªé–‹ç™ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒç¿’å¾—
- **ãƒ¡ã‚¿å­¦ç¿’èƒ½åŠ›**: å­¦ç¿’æ–¹æ³•ã®å­¦ç¿’ãƒ»çŸ¥è­˜ä½“ç³»åŒ–ãƒ»å†åˆ©ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºç«‹

### **Phase 7å®Ÿè£…æˆåŠŸã¸ã®åŸºç›¤**

#### **XGBoostå®Ÿè£…ã«å‘ã‘ãŸæº–å‚™å®Œäº†åº¦**
- âœ… **ãƒ‡ãƒ¼ã‚¿åŸºç›¤**: çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒˆãƒ»ç‰¹å¾´é‡ãƒ†ãƒ¼ãƒ–ãƒ«å®Œæˆ
- âœ… **ç‰¹å¾´é‡æˆ¦ç•¥**: ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼>æ™‚ç³»åˆ—>æ°—è±¡ã®å„ªå…ˆé †ä½ç¢ºå®š
- âœ… **æŠ€è¡“çŸ¥è­˜**: Python MLãƒ»BigQueryçµ±åˆãƒ»ç‰¹å¾´é‡è¨­è¨ˆã®å®Ÿç”¨ç¿’å¾—
- âœ… **è©•ä¾¡è¨­è¨ˆ**: æ¡ä»¶åˆ¥è©•ä¾¡ãƒ»MAPEç›®æ¨™è¨­å®šãƒ»æ¤œè¨¼æ‰‹æ³•ç¢ºç«‹
- âœ… **å®Ÿè£…è¨ˆç”»**: æ®µéšçš„å®Ÿè£…ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãƒ»é‹ç”¨è€ƒæ…®è¨­è¨ˆ

#### **å®Ÿç”¨ã‚·ã‚¹ãƒ†ãƒ å®Œæˆã¸ã®é“ç­‹**
- **Week 1-2**: åŸºæœ¬XGBoostå®Ÿè£…ãƒ»ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´é‡çµ±åˆ
- **Week 3-4**: ãƒ©ã‚°ç‰¹å¾´é‡ãƒ»ç§»å‹•å¹³å‡ãƒ»æ°—è±¡ç‰¹å¾´é‡è¿½åŠ 
- **Week 5-6**: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãƒ»ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- **Week 7-8**: æ¡ä»¶åˆ¥è©•ä¾¡ãƒ»Feature Importanceåˆ†æãƒ»æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ç¢ºå®š
- **Week 9-12**: äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ»è‡ªå‹•åŒ–ãƒ»é‹ç”¨ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰

### **ã‚­ãƒ£ãƒªã‚¢ä¾¡å€¤ãƒ»å¸‚å ´ä¾¡å€¤**

#### **ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢è·ã¸ã®ç«¶äº‰åŠ›**
- **å®Ÿç”¨ã‚·ã‚¹ãƒ†ãƒ å®Ÿç¸¾**: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å®Œæˆäºˆå®š
- **GCPå°‚é–€æ€§**: BigQueryãƒ»GCSãƒ»MLçµ±åˆã®å®Ÿè·µçš„é«˜åº¦çŸ¥è­˜
- **å•é¡Œè§£æ±ºå®Ÿç¸¾**: åˆ¶ç´„ç™ºè¦‹ãƒ»å›é¿ãƒ»æœ€é©åŒ–ã®å…·ä½“çš„äº‹ä¾‹
- **è¨­è¨ˆæ€æƒ³**: å®Ÿç”¨æ€§ãƒ»é‹ç”¨æ€§ãƒ»æ‹¡å¼µæ€§ã‚’è€ƒæ…®ã—ãŸå®Ÿå‹™çš„è¨­è¨ˆèƒ½åŠ›
- **å­¦ç¿’èƒ½åŠ›**: æ–°æŠ€è¡“ç¿’å¾—ãƒ»å•é¡Œè§£æ±ºãƒ»ç¶™ç¶šæ”¹å–„ã®å®Ÿè¨¼ã•ã‚ŒãŸèƒ½åŠ›

#### **å¹´å700ä¸‡å††é”æˆã¸ã®æ ¹æ‹ **
- **æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯**: å¸‚å ´éœ€è¦ã®é«˜ã„GCPãƒ»Pythonãƒ»MLãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- **å®Ÿè£…å®Ÿç¸¾**: ç†è«–çŸ¥è­˜ã§ãªãå®Ÿéš›ã«å‹•ä½œã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰çµŒé¨“
- **å•é¡Œè§£æ±ºåŠ›**: åˆ¶ç´„ãƒ»ã‚¨ãƒ©ãƒ¼ãƒ»æœ€é©åŒ–èª²é¡Œã®ä½“ç³»çš„è§£æ±ºèƒ½åŠ›
- **ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤**: æŠ€è¡“å®Ÿè£…ã¨ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤å‰µå‡ºã®ä¸¡ç«‹å®Ÿç¸¾
- **æˆé•·ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«**: ç¶™ç¶šçš„å­¦ç¿’ãƒ»æ”¹å–„ãƒ»æ‹¡å¼µã¸ã®å®Ÿè¨¼ã•ã‚ŒãŸèƒ½åŠ›

---

**ğŸ‰ Phase 1-6ã«ã‚ˆã‚Šã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ã—ã¦ã®åŸºç¤ä½“åŠ›ï¼ˆPythonãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ»ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼‰ã‹ã‚‰å¿œç”¨åŠ›ï¼ˆçµ±è¨ˆåˆ†æãƒ»ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆãƒ»æˆ¦ç•¥æ€è€ƒï¼‰ã¾ã§ã€å¤±æ•—ã¨æˆåŠŸã‚’é‡ã­ãªãŒã‚‰ä½“ç³»çš„ãªæŠ€è¡“ç¿’å¾—ãŒå®Œäº†ã—ã¾ã—ãŸï¼**

**ğŸš€ Phase 7ä»¥é™ã§ã¯ã€ã“ã®ç¢ºå›ºãŸã‚‹å­¦ç¿’åŸºç›¤ã‚’æ´»ç”¨ã—ã¦ã€XGBoostå®Ÿè£…â†’MLOpsâ†’æœ¬æ ¼é‹ç”¨ã‚·ã‚¹ãƒ†ãƒ ã¸ã¨ç™ºå±•ã•ã›ã€ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ã—ã¦ã®å¸‚å ´ä¾¡å€¤ã‚’ç¢ºç«‹ã—ã¦ã„ãã¾ã™ï¼**