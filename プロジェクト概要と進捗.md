# ğŸ¯ ã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ - Phase 1-6 è©³ç´°è¨˜éŒ²

## ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåŸºæœ¬æƒ…å ±

**ç›®çš„**: Google Cloudãƒ™ãƒ¼ã‚¹ã®é›»åŠ›ä½¿ç”¨é‡äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰  
**ç›®æ¨™**: å¹´å700ä¸‡å††ä»¥ä¸Šã®ãƒ•ãƒ«ãƒªãƒ¢ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢è·ã¸ã®å°±è·  
**æœŸé–“**: 2025å¹´4æœˆã€œ2025å¹´7æœˆï¼ˆPhase 1-6å®Œäº†ï¼‰  
**ç’°å¢ƒ**: Windows + VS Code + Python3.12 + GCP  

---

## ğŸ”¥ Phaseåˆ¥å®Ÿè£…è©³ç´°è¨˜éŒ²

### **Phase 1: PowerDataDownloaderã‚¯ãƒ©ã‚¹åŸºç›¤å®Ÿè£…**

#### **1-1: ã‚¯ãƒ©ã‚¹è¨­è¨ˆãƒ»åŸºæœ¬æ§‹é€ **
```python
# ãƒ•ã‚¡ã‚¤ãƒ«: src/data_processing/data_downloader.py
class PowerDataDownloader:
    BASE_URL = "https://www.tepco.co.jp/forecast/html/images"
    
    def __init__(self, base_dir="data/raw"):
        self.base_dir = Path(base_dir)
        logger.info(f"PowerDataDownloader initialized with base_dir: {self.base_dir}")
```

#### **1-2: æ—¥ä»˜å‡¦ç†ãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…**
```python
def get_months_from_date(self, date_str):
    try:
        date = datetime.strptime(date_str, '%Y%m%d')
    except ValueError as e:
        logger.error(f"Invalid date format {date_str}: {e}")
        raise ValueError(f"æ—¥ä»˜ã¯YYYYMMDDå½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„: {date_str}")
    
    # æœªæ¥æ—¥ä»˜ãƒã‚§ãƒƒã‚¯
    if date.date() > datetime.today().date():
        today_str = datetime.today().strftime('%Y%m%d')
        raise ValueError(f"æœªæ¥ã®æ—¥ä»˜ã¯æŒ‡å®šã§ãã¾ã›ã‚“: {date_str} (ä»Šæ—¥: {today_str})")
    
    month = date.strftime('%Y%m')
    return {month}
```

#### **1-3: HTTPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ZIPè§£å‡å®Ÿè£…**
```python
def download_month_data(self, yyyymm):
    url = f"{self.BASE_URL}/{yyyymm}_power_usage.zip"
    month_dir = self.base_dir / yyyymm
    zip_path = month_dir / f"{yyyymm}.zip"
    
    try:
        month_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Downloading: {url}")
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        with open(zip_path, 'wb') as f:
            f.write(response.content)
        
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(month_dir)
        
        return True
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            logger.warning(f"Data for {yyyymm} not yet available (404)")
            return False
        else:
            logger.error(f"HTTP error downloading {yyyymm}: {e}")
            raise
```

#### **1-4: è¤‡æ•°å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰å®Ÿè£…**
```python
# ãƒ¡ã‚¤ãƒ³é–¢æ•°ã§ã®å¼•æ•°å‡¦ç†
parser.add_argument('--days', type=int, default=5)
parser.add_argument('--month', type=str)
parser.add_argument('--date', type=str)

# æ’ä»–ãƒã‚§ãƒƒã‚¯å®Ÿè£…
specified_args = [bool(args.month), bool(args.date), args.days != 5]
if sum(specified_args) > 1:
    print("âŒ ã‚¨ãƒ©ãƒ¼: --days, --month, --date ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
    return

# å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ†å²
if args.month:
    results = downloader.download_for_month(args.month)
elif args.date:
    results = downloader.download_for_date(args.date)
else:
    results = downloader.download_for_days(args.days)
```

### **Phase 2: GCSUploaderã‚¯ãƒ©ã‚¹çµ±åˆå®Ÿè£…**

#### **2-1: Google Cloud Storage APIçµ±åˆ**
```python
# ãƒ•ã‚¡ã‚¤ãƒ«: src/data_processing/gcs_uploader.py
from google.cloud import storage

class GCSUploader:
    def __init__(self, bucket_name, project_id=None):
        self.bucket_name = bucket_name
        self.project_id = project_id
        self.client = storage.Client(project=project_id)
        self.bucket = self.client.bucket(bucket_name)
        logger.info(f"GCSUploader initialized with bucket: {bucket_name}")
```

#### **2-2: CSVè‡ªå‹•åŠ å·¥æ©Ÿèƒ½å®Ÿè£…**
```python
def _process_raw_csv_to_hourly(self, input_csv_path):
    # åŠ å·¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ç”Ÿæˆ
    base_name = os.path.basename(input_csv_path)
    dir_name = os.path.dirname(input_csv_path)
    processed_filename = base_name.replace('_power_usage.csv', '_hourly_temp.csv')
    output_csv_path = os.path.join(dir_name, processed_filename)
    
    try:
        # Shift-JISã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§CSVèª­ã¿è¾¼ã¿
        with open(input_csv_path, 'r', encoding='shift-jis') as f:
            content = f.read()
        
        lines = content.split('\n')
        output_lines = ['date,hour,actual_power,supply_capacity']
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆDATE,TIME...ï¼‰ã‚’è¦‹ã¤ã‘ã‚‹
        header_line_index = -1
        for i, line in enumerate(lines):
            if 'DATE,TIME,å½“æ—¥å®Ÿç¸¾(ä¸‡kW)' in line:
                header_line_index = i
                break
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã®æ¬¡è¡Œã‹ã‚‰24è¡Œå‡¦ç†
        for i in range(header_line_index + 1, min(header_line_index + 25, len(lines))):
            line = lines[i].strip()
            if not line:
                continue
                
            parts = line.split(',')
            if len(parts) >= 6:
                # æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµ±ä¸€: "2025/4/1" â†’ "2025-04-01"
                date_str = parts[0].strip()
                date_parts = date_str.split('/')
                if len(date_parts) == 3:
                    year = date_parts[0]
                    month = date_parts[1].zfill(2)
                    day = date_parts[2].zfill(2)
                    formatted_date = f"{year}-{month}-{day}"
                
                # æ™‚åˆ»æŠ½å‡º: "13:00" â†’ 13 â†’ "13"
                time_str = parts[1].strip()
                hour = int(time_str.split(':')[0])
                hour_str = str(hour).zfill(2)
                
                actual_power = float(parts[2])
                supply_capacity = float(parts[5])
                
                output_line = f"{formatted_date},{hour_str},{actual_power},{supply_capacity}"
                output_lines.append(output_line)
        
        # UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å‡ºåŠ›
        with open(output_csv_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(output_lines))
        
        return output_csv_path
    except Exception as e:
        logger.error(f"Error processing CSV to hourly data: {e}")
        raise
```

#### **2-3: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Ÿè£…**
```python
def upload_directory(self, local_dir_path, destination_prefix="", file_extension=None):
    uploaded_uris = []
    
    for root, _, files in os.walk(local_dir_path):
        for file in files:
            if file_extension and not file.endswith(file_extension):
                continue
            
            local_file_path = os.path.join(root, file)
            rel_path = os.path.relpath(local_file_path, local_dir_path)
            gcs_path = os.path.join(destination_prefix, rel_path).replace("\\", "/")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯åŠ å·¥å‡¦ç†
            if file.endswith('.csv'):
                try:
                    processed_file_path = self._process_raw_csv_to_hourly(local_file_path)
                    gcs_filename = file.replace('_power_usage.csv', '_hourly.csv')
                    processed_gcs_path = gcs_path.replace(file, gcs_filename)
                    
                    uri = self.upload_file(processed_file_path, processed_gcs_path)
                    uploaded_uris.append(uri)
                    
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»åŸå½¢ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                    os.remove(processed_file_path)
                    os.remove(local_file_path)
                    
                except Exception as e:
                    logger.error(f"Failed to process CSV file {file}: {e}")
                    uri = self.upload_file(local_file_path, gcs_path)
                    uploaded_uris.append(uri)
            else:
                uri = self.upload_file(local_file_path, gcs_path)
                uploaded_uris.append(uri)
    
    return uploaded_uris
```

### **Phase 3: MainETLPipelineã‚¯ãƒ©ã‚¹çµ±åˆå®Ÿè£…**

#### **3-1: ETLçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ**
```python
# ãƒ•ã‚¡ã‚¤ãƒ«: src/pipelines/main_etl.py
class MainETLPipeline:
    def __init__(self, base_dir="data/raw", bucket_name="energy-env-data"):
        self.base_dir = Path(base_dir)
        self.bucket_name = bucket_name
        self.downloader = PowerDataDownloader(str(self.base_dir))
        self.uploader = GCSUploader(bucket_name)
        logger.info(f"MainETLPipeline initialized: {base_dir} â†’ gs://{bucket_name}")
```

#### **3-2: è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½å®Ÿè£…**
```python
def _cleanup_old_zip_versions(self):
    try:
        execution_date = datetime.today()
        cutoff_date = execution_date - timedelta(days=14)
        
        current_month = execution_date.strftime('%Y%m')
        previous_month = (execution_date.replace(day=1) - timedelta(days=1)).strftime('%Y%m')
        months_to_check = {current_month, previous_month}
        
        total_deleted = 0
        for month in months_to_check:
            archive_prefix = f"archives/{month}/"
            blobs = list(self.uploader.client.list_blobs(
                self.uploader.bucket, prefix=archive_prefix
            ))
            
            for blob in blobs:
                if not blob.name.endswith('.zip'):
                    continue
                    
                path_parts = blob.name.split('/')
                if len(path_parts) < 3:
                    continue
                    
                date_str = path_parts[2]
                try:
                    file_date = datetime.strptime(date_str, '%Y-%m-%d')
                except ValueError:
                    continue
                
                # 2é€±é–“ã‚ˆã‚Šå¤ãã€æœˆæœ«æ—¥ã§ãªã„å ´åˆã¯å‰Šé™¤
                last_day_of_month = calendar.monthrange(file_date.year, file_date.month)[1]
                if (file_date < cutoff_date and file_date.day != last_day_of_month):
                    blob.delete()
                    logger.info(f"Deleted old ZIP: {blob.name}")
                    total_deleted += 1
        
        if total_deleted > 0:
            logger.info(f"ZIP cleanup completed: deleted {total_deleted} files")
    except Exception as e:
        logger.warning(f"ZIP cleanup failed: {e}")
```

#### **3-3: è©³ç´°çµæœè¡¨ç¤ºæ©Ÿèƒ½å®Ÿè£…**
```python
def print_etl_results(results):
    print(f"\n{'='*60}")
    print("ğŸ“Š ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œçµæœ")
    print('='*60)
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
    print("\nğŸ”½ Extract (ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)")
    dl_results = results['download_results']
    if dl_results['success']:
        print(f"âœ… æˆåŠŸ: {', '.join(dl_results['success'])}")
    if dl_results['failed']:
        print(f"âŒ å¤±æ•—: {', '.join(dl_results['failed'])}")
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
    print("\nğŸ”¼ Load (ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰)")
    up_results = results['upload_results']
    if up_results['success']:
        print(f"âœ… æˆåŠŸ: {', '.join(up_results['success'])}")
    if up_results['failed']:
        print(f"âŒ å¤±æ•—: {', '.join(up_results['failed'])}")
    
    # ç·åˆçµæœ
    status_emoji = {'success': 'ğŸ‰', 'partial': 'âš ï¸', 'failed': 'ğŸ’¥'}
    print(f"\nğŸ“‹ ç·åˆçµæœ")
    print(f"{status_emoji[results['overall_status']]} {results['message']}")
```

### **Phase 4: ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ãƒ»ãƒ†ã‚¹ãƒˆå®Ÿè£…**

#### **4-1: éšå±¤ãƒ­ã‚¬ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…**
```python
# ãƒ•ã‚¡ã‚¤ãƒ«: src/utils/logging_config.py
from logging import getLogger, StreamHandler, Formatter, INFO, WARNING

def setup_logging(level=INFO):
    app_logger = getLogger('energy_env')
    
    # äºŒé‡è¨­å®šã‚’é¿ã‘ã‚‹
    if len(app_logger.handlers) > 0:
        return
    
    app_logger.setLevel(level)
    
    # çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    formatter = Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')
    
    console_handler = StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    app_logger.addHandler(console_handler)
    
    # ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«èª¿æ•´
    getLogger('google').setLevel(WARNING)
    getLogger('googleapiclient').setLevel(WARNING)
```

#### **4-2: åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè£…**
```python
# ãƒ•ã‚¡ã‚¤ãƒ«: tests/test_data_downloader.py
def test_initialization():
    print_test_header("PowerDataDownloaderåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ")
    try:
        downloader1 = PowerDataDownloader()
        print_result(True, f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆæœŸåŒ–: base_dir={downloader1.base_dir}")
        
        downloader2 = PowerDataDownloader("custom/data")
        print_result(True, f"ã‚«ã‚¹ã‚¿ãƒ åˆæœŸåŒ–: base_dir={downloader2.base_dir}")
        
        return downloader1
    except Exception as e:
        print_result(False, f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def test_future_date_handling():
    downloader = PowerDataDownloader()
    future_date = datetime.today() + timedelta(days=180)
    future_date_str = future_date.strftime('%Y%m%d')
    
    try:
        months = downloader.get_months_from_date(future_date_str)
        print_result(False, f"æœªæ¥æ—¥ä»˜ãŒæ‹’å¦ã•ã‚Œãªã‹ã£ãŸ: {months}")
    except ValueError as e:
        if "æœªæ¥ã®æ—¥ä»˜ã¯æŒ‡å®šã§ãã¾ã›ã‚“" in str(e):
            print_result(True, f"æœªæ¥æ—¥ä»˜ã‚’æ­£ã—ãæ‹’å¦: {e}")
        else:
            print_result(False, f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
```

### **Phase 5: BigQueryç‰©ç†ãƒ†ãƒ¼ãƒ–ãƒ«è¨­è¨ˆãƒ»å®Ÿè£…**

#### **5-1: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–**
```sql
-- energy_data_hourlyãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
CREATE TABLE `energy-env.dev_energy_data.energy_data_hourly` (
    date DATE,
    hour INTEGER,
    actual_power FLOAT64,
    supply_capacity FLOAT64,
    created_at TIMESTAMP
)
PARTITION BY date
CLUSTER BY hour;
```

#### **5-2: LOAD DATA FROM FILESé«˜é€ŸæŠ•å…¥**
```sql
-- 30ãƒ¶æœˆåˆ†ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
LOAD DATA INTO `energy-env.dev_energy_data.energy_data_hourly`
(date, hour, actual_power, supply_capacity, created_at)
FROM FILES (
    format = 'CSV',
    uris = ['gs://energy-env-data/raw_data/*/*.csv'],
    skip_leading_rows = 1
);
-- çµæœ: 21,888ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ•å…¥å®Œäº†
```

#### **5-3: å‹çµ±ä¸€å•é¡Œè§£æ±ºå®Ÿè£…**
```sql
-- å•é¡Œ: weather_data.hour (STRING) vs energy_data.hour (INTEGER)
-- è§£æ±º: JOINæ™‚å‹å¤‰æ›
SELECT 
    energy.date,
    energy.hour,
    energy.actual_power,
    weather.temperature_2m
FROM `energy-env.dev_energy_data.energy_data_hourly` energy
LEFT JOIN `energy-env.dev_energy_data.weather_data` weather 
    ON energy.date = weather.date 
    AND energy.hour = CAST(weather.hour AS INTEGER)  -- å‹å¤‰æ›
```

### **Phase 6: æ°—è±¡ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…**

#### **6-1: WeatherProcessorã‚¯ãƒ©ã‚¹å®Ÿè£…**
```python
# ãƒ•ã‚¡ã‚¤ãƒ«: src/data_processing/weather_processor.py
class WeatherProcessor:
    VALID_PREFECTURES = [
        'tokyo', 'kanagawa', 'saitama', 'chiba', 'ibaraki', 
        'tochigi', 'gunma', 'yamanashi', 'shizuoka'
    ]
    
    def convert_json_to_csv(self, json_file_path, date_filter=None):
        json_path = Path(json_file_path)
        file_stem = json_path.stem  # tokyo_2023 or tokyo_2025_0715
        parts = file_stem.split('_')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if len(parts) < 2:
            raise ValueError(f"Invalid filename format: {json_path.name}")
        
        prefecture = parts[0]
        if prefecture not in self.VALID_PREFECTURES:
            raise ValueError(f"Invalid prefecture '{prefecture}'")
        
        year_str = parts[1]
        try:
            year = int(year_str)
            if not (2010 <= year <= 2100):
                raise ValueError(f"Year must be between 2010-2100: {year}")
        except ValueError:
            raise ValueError(f"Invalid year format '{year_str}': {json_path.name}")
        
        # æ—¥ä»˜éƒ¨åˆ†ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if len(parts) == 3:
            date_part = parts[2]
            if len(date_part) != 4 or not date_part.isdigit():
                raise ValueError(f"Date must be MMDD format: {date_part}")
            
            try:
                month = int(date_part[:2])
                day = int(date_part[2:])
                datetime(year, month, day)  # å®Ÿåœ¨æ—¥ä»˜ãƒã‚§ãƒƒã‚¯
            except ValueError:
                raise ValueError(f"Invalid date {year}-{date_part}")
```

#### **6-2: WeatherBigQueryLoaderã‚¯ãƒ©ã‚¹å®Ÿè£…**
```python
# ãƒ•ã‚¡ã‚¤ãƒ«: src/data_processing/weather_bigquery_loader.py
class WeatherBigQueryLoader:
    def create_external_table(self, file_uris):
        if not file_uris:
            return
        
        uris_str = "', '".join(file_uris)
        create_sql = f"""
        CREATE OR REPLACE EXTERNAL TABLE `{self.project_id}.{self.dataset_id}.temp_weather_external`
        (
            prefecture STRING,
            date STRING,
            hour STRING,
            temperature_2m FLOAT64,
            relative_humidity_2m FLOAT64,
            precipitation FLOAT64,
            weather_code INT64
        )
        OPTIONS (
            format = 'CSV',
            uris = ['{uris_str}'],
            skip_leading_rows = 1
        );
        """
        
        job = self.bq_client.query(create_sql)
        job.result()
        
    def delete_duplicate_data(self):
        # è¤‡æ•°ã‚«ãƒ©ãƒ INå¥åˆ¶ç´„å›é¿: CONCATæ–¹å¼
        delete_query = f"""
        DELETE FROM `{self.project_id}.{self.dataset_id}.{self.table_id}` 
        WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
          AND CONCAT(prefecture, '|', CAST(date AS STRING), '|', hour) IN (
              SELECT CONCAT(prefecture, '|', CAST(PARSE_DATE('%Y-%m-%d', date) AS STRING), '|', hour)
              FROM `{self.project_id}.{self.dataset_id}.temp_weather_external`
              WHERE PARSE_DATE('%Y-%m-%d', date) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
          )
        """
        
        job = self.bq_client.query(delete_query)
        result = job.result()
        logger.info(f"Deleted duplicate data: {job.num_dml_affected_rows} rows")
```

### **Phase 7: çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒˆãƒ»ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼çµ±åˆå®Ÿè£…**

#### **7-1: ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰**
```sql
-- calendar_dataãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
CREATE TABLE `energy-env.dev_energy_data.calendar_data` (
    date DATE,
    day_of_week STRING,
    is_weekend BOOLEAN,
    is_holiday BOOLEAN,
    holiday_name STRING
);

-- å†…é–£åºœç¥æ—¥ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆ1,050ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰
INSERT INTO `energy-env.dev_energy_data.calendar_data` 
VALUES 
    ('2025-01-01', 'Wednesday', false, true, 'å…ƒæ—¥'),
    ('2025-01-13', 'Monday', false, true, 'æˆäººã®æ—¥'),
    ('2025-02-11', 'Tuesday', false, true, 'å»ºå›½è¨˜å¿µã®æ—¥'),
    -- ... 1,050ãƒ¬ã‚³ãƒ¼ãƒ‰
```

#### **7-2: çµ±åˆãƒ“ãƒ¥ãƒ¼æ§‹ç¯‰å®Ÿè£…**
```sql
-- power_weather_integratedãƒ“ãƒ¥ãƒ¼ä½œæˆ
CREATE OR REPLACE VIEW `energy-env.dev_energy_data.power_weather_integrated` AS
SELECT 
    -- é›»åŠ›ãƒ‡ãƒ¼ã‚¿
    energy.date,
    energy.hour,
    energy.actual_power,
    energy.supply_capacity,
    
    -- æ°—è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆåƒè‘‰ã®ã¿ï¼‰
    weather.temperature_2m,
    weather.relative_humidity_2m,
    weather.precipitation,
    weather.weather_code,
    
    -- ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿
    calendar.day_of_week,
    calendar.is_weekend,
    calendar.is_holiday,
    
    -- æ´¾ç”Ÿé …ç›®
    EXTRACT(MONTH FROM energy.date) as month
    
FROM `energy-env.dev_energy_data.energy_data_hourly` energy
LEFT JOIN `energy-env.dev_energy_data.weather_data` weather 
    ON energy.date = weather.date 
    AND energy.hour = CAST(weather.hour AS INTEGER)
    AND weather.prefecture = 'chiba'  -- åƒè‘‰ã®ã¿ã«çµã‚‹
LEFT JOIN `energy-env.dev_energy_data.calendar_data` calendar
    ON energy.date = calendar.date;

-- çµæœ: 197,856ãƒ¬ã‚³ãƒ¼ãƒ‰çµ±åˆãƒ“ãƒ¥ãƒ¼å®Œæˆ
```

### **Phase 8: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè£…**

#### **8-1: å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹æ™‚ç³»åˆ—ç‰¹å¾´é‡å®Ÿè£…**
```sql
-- business_days_lagãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
CREATE OR REPLACE TABLE `energy-env.dev_energy_data.business_days_lag` AS
SELECT 
  date,
  hour,
  actual_power,
  supply_capacity,
  
  -- å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚° 1ï½30æ—¥åˆ†ï¼ˆãƒªãƒ†ãƒ©ãƒ«å€¤æŒ‡å®šï¼‰
  LAG(actual_power, 24) OVER (ORDER BY date, hour) as lag_1_business_day,
  LAG(actual_power, 48) OVER (ORDER BY date, hour) as lag_2_business_day,
  LAG(actual_power, 72) OVER (ORDER BY date, hour) as lag_3_business_day,
  -- ... 30æ—¥åˆ†
  LAG(actual_power, 720) OVER (ORDER BY date, hour) as lag_30_business_day,
  
  -- å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ç§»å‹•å¹³å‡
  AVG(actual_power) OVER (ORDER BY date, hour ROWS BETWEEN 119 PRECEDING AND 0 PRECEDING) as avg_5_business_days,
  AVG(actual_power) OVER (ORDER BY date, hour ROWS BETWEEN 239 PRECEDING AND 0 PRECEDING) as avg_10_business_days,
  -- ... 30æ—¥åˆ†
  
  -- å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹å¤‰åŒ–ç‡ï¼ˆNULLå®‰å…¨è¨ˆç®—ï¼‰
  (actual_power - LAG(actual_power, 24) OVER (ORDER BY date, hour)) / NULLIF(LAG(actual_power, 24) OVER (ORDER BY date, hour), 0) * 100 as change_rate_1_business_day,
  -- ... 30æ—¥åˆ†

FROM `energy-env.dev_energy_data.power_weather_integrated`
WHERE is_holiday = false AND is_weekend = false  -- å–¶æ¥­æ—¥ã®ã¿æŠ½å‡º
ORDER BY date, hour;
```

#### **8-2: å…¨æ—¥ãƒ™ãƒ¼ã‚¹æ™‚ç³»åˆ—ç‰¹å¾´é‡å®Ÿè£…**
```sql
-- all_days_lagãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆåŒæ§‹æˆã€WHEREæ¡ä»¶ã®ã¿å·®ç•°ï¼‰
CREATE OR REPLACE TABLE `energy-env.dev_energy_data.all_days_lag` AS
SELECT 
  date,
  hour,
  actual_power,
  supply_capacity,
  is_weekend,
  is_holiday,
  
  -- å®Ÿæ™‚é–“ãƒ™ãƒ¼ã‚¹ãƒ©ã‚°ãƒ»ç§»å‹•å¹³å‡ãƒ»å¤‰åŒ–ç‡ï¼ˆå–¶æ¥­æ—¥ç‰ˆã¨åŒæ§‹æˆï¼‰
  LAG(actual_power, 24) OVER (ORDER BY date, hour) as lag_1_day,
  -- ... 30æ—¥åˆ†å…¨ç‰¹å¾´é‡

FROM `energy-env.dev_energy_data.power_weather_integrated`
ORDER BY date, hour;  -- å…¨æ—¥å¯¾è±¡
```

#### **8-3: çµ±åˆæ©Ÿæ¢°å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿè£…**
```sql
-- ml_featuresãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆ190ç‰¹å¾´é‡çµ±åˆï¼‰
CREATE OR REPLACE TABLE `energy-env.dev_energy_data.ml_features` AS
SELECT 
  -- åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ï¼ˆ12ç‰¹å¾´é‡ï¼‰
  pwi.date, pwi.hour, pwi.actual_power, pwi.supply_capacity,
  pwi.temperature_2m, pwi.relative_humidity_2m, pwi.precipitation, pwi.weather_code,
  pwi.day_of_week, pwi.is_weekend, pwi.is_holiday, pwi.month,
  
  -- å¾ªç’°ç‰¹å¾´é‡ï¼ˆ2ç‰¹å¾´é‡ã€PIåˆ¶ç´„å›é¿ï¼‰
  SIN(2 * 3.141592653589793 * pwi.hour / 24) as hour_sin,
  COS(2 * 3.141592653589793 * pwi.hour / 24) as hour_cos,
  
  -- å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆ90ç‰¹å¾´é‡ï¼‰
  bd.lag_1_business_day, bd.lag_2_business_day, /* ... 30å€‹ */
  bd.avg_5_business_days, bd.avg_10_business_days, /* ... 6å€‹ */
  bd.change_rate_1_business_day, /* ... 30å€‹ */
  
  -- å…¨æ—¥ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆ90ç‰¹å¾´é‡ï¼‰
  ad.lag_1_day, ad.lag_2_day, /* ... 30å€‹ */
  ad.avg_5_days, ad.avg_10_days, /* ... 6å€‹ */
  ad.change_rate_1_day, /* ... 30å€‹ */

FROM `energy-env.dev_energy_data.power_weather_integrated` pwi
LEFT JOIN `energy-env.dev_energy_data.business_days_lag` bd
  ON pwi.date = bd.date AND pwi.hour = bd.hour
LEFT JOIN `energy-env.dev_energy_data.all_days_lag` ad
  ON pwi.date = ad.date AND pwi.hour = ad.hour
ORDER BY pwi.date, pwi.hour;

-- çµæœ: ç´„190ç‰¹å¾´é‡ã®çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Œæˆ
```

---

## ğŸ› ï¸ é‡è¦ãªæŠ€è¡“å®Ÿè£…è©³ç´°

### **BigQueryåˆ¶ç´„å›é¿æŠ€è¡“å®Ÿè£…**

#### **MERGEæ–‡ã«ã‚ˆã‚‹åŠ¹ç‡çš„æ›´æ–°**
```sql
-- business_day_numberè¿½åŠ æ™‚ã®MERGEå®Ÿè£…
MERGE `energy-env.dev_energy_data.calendar_data` AS target
USING (
  SELECT 
    date,
    ROW_NUMBER() OVER (ORDER BY date) as business_day_number
  FROM `energy-env.dev_energy_data.calendar_data`
  WHERE is_holiday = false AND is_weekend = false
) AS source
ON target.date = source.date
WHEN MATCHED THEN
  UPDATE SET business_day_number = source.business_day_number;
```

#### **è¤‡æ•°ã‚«ãƒ©ãƒ INå¥åˆ¶ç´„å›é¿**
```sql
-- åˆ¶ç´„: WHERE (col1, col2) IN (SELECT ...) ä¸å¯
-- å›é¿: CONCATæ–¹å¼
WHERE CONCAT(prefecture, '|', CAST(date AS STRING), '|', hour) IN (
    SELECT CONCAT(prefecture, '|', CAST(date AS STRING), '|', hour)
    FROM external_table
)
```

#### **DELETE JOINåˆ¶ç´„å›é¿**
```sql
-- åˆ¶ç´„: DELETE FROM table1 JOIN table2 ä¸å¯
-- å›é¿: EXISTSæ–¹å¼
DELETE FROM `energy-env.dev_energy_data.weather_data` 
WHERE EXISTS (
    SELECT 1 
    FROM `energy-env.dev_energy_data.temp_weather_external` ext
    WHERE weather_data.prefecture = ext.prefecture
      AND weather_data.date = ext.date
      AND weather_data.hour = ext.hour
);
```

#### **Python APIåˆ¶ç´„å›é¿ï¼ˆWITHå¥+UPDATEåŒæ™‚å®Ÿè¡Œä¸å¯ï¼‰**
```python
# åˆ¶ç´„: BigQueryã§WITHå¥ã¨UPDATEæ–‡ã®åŒæ™‚å®Ÿè¡Œä¸å¯
# å›é¿: ç”ŸSQLæ–‡å­—åˆ—ã«ã‚ˆã‚‹ç›´æ¥å®Ÿè¡Œ
sql_string = """
WITH business_days AS (
  SELECT date, ROW_NUMBER() OVER (ORDER BY date) as number
  FROM calendar_data WHERE is_holiday = false
)
UPDATE calendar_data SET business_day_number = bd.number
FROM business_days bd WHERE calendar_data.date = bd.date;
"""

job = client.query(sql_string)  # ç”ŸSQLå®Ÿè¡Œ
job.result()
```

### **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒ‡ãƒ¼ã‚¿å¤‰æ›å®Ÿè£…è©³ç´°**

#### **Shift-JISè‡ªå‹•æ¤œå‡ºãƒ»UTF-8å¤‰æ›**
```python
def _process_raw_csv_to_hourly(self, input_csv_path):
    try:
        # Shift-JISã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
        with open(input_csv_path, 'r', encoding='shift-jis') as f:
            content = f.read()
        
        lines = content.split('\n')
        output_lines = []
        output_lines.append('date,hour,actual_power,supply_capacity')
        
        # æ±é›»CSVç‰¹æœ‰ã®ãƒ˜ãƒƒãƒ€ãƒ¼æ¤œç´¢
        header_patterns = [
            'DATE,TIME,å½“æ—¥å®Ÿç¸¾(ä¸‡kW)',
            'DATE,TIME,å½“æ—¥å®Ÿç¸¾ï¼ˆä¸‡kWï¼‰',  # å…¨è§’æ‹¬å¼§ãƒ‘ã‚¿ãƒ¼ãƒ³
            'DATE,TIME,å®Ÿç¸¾(ä¸‡kW)'        # çŸ­ç¸®ãƒ‘ã‚¿ãƒ¼ãƒ³
        ]
        
        header_line_index = -1
        for i, line in enumerate(lines):
            for pattern in header_patterns:
                if pattern in line:
                    header_line_index = i
                    break
            if header_line_index != -1:
                break
        
        if header_line_index == -1:
            raise ValueError("Header line not found in CSV")
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œå‡¦ç†ï¼ˆ24æ™‚é–“åˆ†ï¼‰
        processed_count = 0
        for i in range(header_line_index + 1, min(header_line_index + 25, len(lines))):
            line = lines[i].strip()
            if not line:
                continue
                
            parts = line.split(',')
            if len(parts) >= 6:
                try:
                    # è¤‡æ•°æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œ
                    date_str = parts[0].strip()
                    if '/' in date_str:
                        # "2025/4/1" â†’ "2025-04-01"
                        date_parts = date_str.split('/')
                        year = date_parts[0]
                        month = date_parts[1].zfill(2)
                        day = date_parts[2].zfill(2)
                        formatted_date = f"{year}-{month}-{day}"
                    elif '-' in date_str:
                        # "2025-04-01" â†’ ãã®ã¾ã¾
                        formatted_date = date_str
                    else:
                        # "20250401" â†’ "2025-04-01"
                        if len(date_str) == 8:
                            formatted_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
                        else:
                            formatted_date = date_str
                    
                    # æ™‚åˆ»æ­£è¦åŒ–: "13:00" â†’ 13 â†’ "13"
                    time_str = parts[1].strip()
                    if ':' in time_str:
                        hour = int(time_str.split(':')[0])
                    else:
                        hour = int(time_str)
                    hour_str = str(hour).zfill(2)
                    
                    # æ•°å€¤ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»æ¤œè¨¼
                    actual_power = float(parts[2])
                    supply_capacity = float(parts[5])
                    
                    # ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
                    if actual_power < 0 or supply_capacity < 0:
                        logger.warning(f"Negative power values: {actual_power}, {supply_capacity}")
                        continue
                    
                    if actual_power > supply_capacity * 1.1:  # 10%ãƒãƒ¼ã‚¸ãƒ³
                        logger.warning(f"Actual power exceeds supply: {actual_power} > {supply_capacity}")
                    
                    output_line = f"{formatted_date},{hour_str},{actual_power},{supply_capacity}"
                    output_lines.append(output_line)
                    processed_count += 1
                    
                except (ValueError, IndexError) as e:
                    logger.warning(f"Skipping invalid row {i}: {line} - {e}")
                    continue
        
        # UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å‡ºåŠ›
        with open(output_csv_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(output_lines))
        
        logger.info(f"Successfully processed {processed_count} hourly records to {output_csv_path}")
        return output_csv_path
        
    except UnicodeDecodeError:
        # Shift-JISå¤±æ•—æ™‚ã®CP932ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        try:
            with open(input_csv_path, 'r', encoding='cp932') as f:
                content = f.read()
            # åŒã˜å‡¦ç†ã‚’ç¹°ã‚Šè¿”ã—
        except UnicodeDecodeError:
            # UTF-8ã§ã®èª­ã¿è¾¼ã¿è©¦è¡Œ
            with open(input_csv_path, 'r', encoding='utf-8') as f:
                content = f.read()
```

### **æ°—è±¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…è©³ç´°**

#### **ãƒ•ã‚¡ã‚¤ãƒ«åãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–å®Ÿè£…**
```python
def convert_json_to_csv(self, json_file_path, date_filter=None):
    json_path = Path(json_file_path)
    file_stem = json_path.stem
    parts = file_stem.split('_')
    
    # æ®µéšçš„ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if len(parts) < 2:
        raise ValueError(f"Invalid filename format (need prefecture_year): {json_path.name}")
    elif len(parts) > 3:
        raise ValueError(f"Too many parts in filename: {json_path.name}")
    
    # éƒ½çœŒåãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    prefecture = parts[0].lower()
    if prefecture not in self.VALID_PREFECTURES:
        raise ValueError(f"Invalid prefecture '{prefecture}'. Valid: {self.VALID_PREFECTURES}")
    
    # å¹´ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    year_str = parts[1]
    try:
        year = int(year_str)
        if not (2010 <= year <= 2100):
            raise ValueError(f"Year must be between 2010-2100: {year}")
    except ValueError:
        raise ValueError(f"Invalid year format '{year_str}': {json_path.name}")
    
    # æ—¥ä»˜ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    date_part = None
    if len(parts) == 3:
        date_part = parts[2]
        # MMDDå½¢å¼ãƒã‚§ãƒƒã‚¯
        if len(date_part) != 4 or not date_part.isdigit():
            raise ValueError(f"Date must be MMDD format: {date_part}")
        
        # å®Ÿåœ¨æ—¥ä»˜ãƒã‚§ãƒƒã‚¯
        try:
            month = int(date_part[:2])
            day = int(date_part[2:])
            if not (1 <= month <= 12):
                raise ValueError(f"Invalid month: {month}")
            if not (1 <= day <= 31):
                raise ValueError(f"Invalid day: {day}")
            
            # æœˆæ—¥çµ„ã¿åˆã‚ã›ã®å®Ÿåœ¨æ€§ãƒã‚§ãƒƒã‚¯
            test_date = datetime(year, month, day)
        except ValueError as e:
            raise ValueError(f"Invalid date {year}-{date_part}: {e}")
    
    # JSONãƒ‡ãƒ¼ã‚¿æ§‹é€ æ¤œè¨¼
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    required_fields = ['hourly']
    for field in required_fields:
        if field not in data:
            raise ValueError(f"Missing required field '{field}' in {json_path.name}")
    
    hourly_data = data['hourly']
    required_hourly_fields = ['time', 'temperature_2m', 'relative_humidity_2m', 'precipitation', 'weather_code']
    for field in required_hourly_fields:
        if field not in hourly_data:
            raise ValueError(f"Missing required hourly field '{field}' in {json_path.name}")
    
    # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    times = hourly_data['time']
    temperatures = hourly_data['temperature_2m']
    humidity = hourly_data['relative_humidity_2m']
    precipitation = hourly_data['precipitation']
    weather_codes = hourly_data['weather_code']
    
    # é…åˆ—é•·ä¸€è‡´ãƒã‚§ãƒƒã‚¯
    field_lengths = [len(times), len(temperatures), len(humidity), len(precipitation), len(weather_codes)]
    if len(set(field_lengths)) > 1:
        raise ValueError(f"Inconsistent array lengths in {json_path.name}: {field_lengths}")
```

#### **ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»ç•°å¸¸å€¤æ¤œå‡º**
```python
# CSVå¤‰æ›æ™‚ã®å“è³ªãƒã‚§ãƒƒã‚¯
converted_count = 0
anomaly_count = 0

for i in range(len(times)):
    time_str = times[i]
    dt = datetime.fromisoformat(time_str)
    date_str = dt.strftime('%Y-%m-%d')
    hour_str = dt.strftime('%H')
    
    # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
    if date_filter and date_str != date_filter:
        continue
    
    # æ°—è±¡ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    temp = temperatures[i]
    humidity = humidity[i]
    precip = precipitation[i]
    weather_code = weather_codes[i]
    
    # ç•°å¸¸å€¤æ¤œå‡º
    anomalies = []
    if temp is None or temp < -50 or temp > 60:
        anomalies.append(f"temperature: {temp}")
    if humidity is None or humidity < 0 or humidity > 100:
        anomalies.append(f"humidity: {humidity}")
    if precip is None or precip < 0 or precip > 1000:
        anomalies.append(f"precipitation: {precip}")
    if weather_code is None or weather_code < 0 or weather_code > 99:
        anomalies.append(f"weather_code: {weather_code}")
    
    if anomalies:
        logger.warning(f"Anomalies in {json_path.name} at {time_str}: {', '.join(anomalies)}")
        anomaly_count += 1
        # ç•°å¸¸å€¤ã¯è£œé–“ã¾ãŸã¯é™¤å¤–
        if len(anomalies) > 2:  # åŠæ•°ä»¥ä¸Šç•°å¸¸å€¤ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            continue
        
        # ç°¡å˜ãªè£œé–“ï¼ˆå‰å¾Œã®å€¤ã®å¹³å‡ï¼‰
        if i > 0 and i < len(times) - 1:
            if temp is None or temp < -50 or temp > 60:
                temp = (temperatures[i-1] + temperatures[i+1]) / 2
            # ä»–ã®é …ç›®ã‚‚åŒæ§˜ã«è£œé–“
    
    # CSVè¡Œå‡ºåŠ›
    writer.writerow([prefecture, date_str, hour_str, temp, humidity, precip, weather_code])
    converted_count += 1

logger.info(f"Converted {converted_count} records, detected {anomaly_count} anomalies")
```

### **Jupyter Notebookåˆ†æç’°å¢ƒå®Ÿè£…è©³ç´°**

#### **BigQueryç›´æ¥é€£æºãƒ»é«˜é€Ÿã‚¯ã‚¨ãƒªå®Ÿè£…**
```python
# energy-envä»®æƒ³ç’°å¢ƒå†…ã§ã®Jupyterè¨­å®š
import os
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'keys/energy-env-key.json'

from google.cloud import bigquery
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
client = bigquery.Client(project='energy-env')

# å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„å–å¾—
def get_power_data(start_date='2025-01-01', end_date='2025-12-31', limit=None):
    """é›»åŠ›ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæ—¥ä»˜ç¯„å›²æŒ‡å®šã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ´»ç”¨ï¼‰"""
    limit_clause = f"LIMIT {limit}" if limit else ""
    
    query = f"""
    SELECT 
        date,
        hour,
        actual_power,
        supply_capacity,
        temperature_2m,
        is_weekend,
        is_holiday
    FROM `energy-env.dev_energy_data.power_weather_integrated`
    WHERE date BETWEEN '{start_date}' AND '{end_date}'
    ORDER BY date, hour
    {limit_clause}
    """
    
    # ã‚¯ã‚¨ãƒªå®Ÿè¡Œâ†’DataFrameå¤‰æ›
    df = client.query(query).to_dataframe()
    return df

# æœˆåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æç”¨é–¢æ•°
def analyze_monthly_patterns(year=2025):
    """æœˆåˆ¥é›»åŠ›éœ€è¦ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
    query = f"""
    SELECT 
        EXTRACT(MONTH FROM date) as month,
        hour,
        AVG(actual_power) as avg_power,
        STDDEV(actual_power) as std_power,
        COUNT(*) as sample_count
    FROM `energy-env.dev_energy_data.power_weather_integrated`
    WHERE EXTRACT(YEAR FROM date) = {year}
    GROUP BY month, hour
    ORDER BY month, hour
    """
    
    df = client.query(query).to_dataframe()
    
    # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆæœˆÃ—æ™‚é–“ï¼‰
    pivot_avg = df.pivot(index='hour', columns='month', values='avg_power')
    pivot_std = df.pivot(index='hour', columns='month', values='std_power')
    
    return pivot_avg, pivot_std

# Ridge Plotåˆ†æå®Ÿè£…
def create_ridge_plot_analysis():
    """éƒ½çœŒåˆ¥æ°—æ¸©ãƒ‡ãƒ¼ã‚¿Ridge Plotåˆ†æ"""
    # å…¨éƒ½çœŒãƒ‡ãƒ¼ã‚¿å–å¾—
    query = """
    SELECT 
        prefecture,
        temperature_2m,
        actual_power,
        is_weekend,
        is_holiday
    FROM `energy-env.dev_energy_data.weather_data` w
    JOIN `energy-env.dev_energy_data.energy_data_hourly` e
        ON w.date = e.date AND CAST(w.hour AS INTEGER) = e.hour
    JOIN `energy-env.dev_energy_data.calendar_data` c
        ON w.date = c.date
    WHERE w.date >= '2025-01-01'
    """
    
    df = client.query(query).to_dataframe()
    
    # å¹³æ—¥ãƒ»ä¼‘æ—¥åˆ¥åˆ†æ
    weekday_df = df[~df['is_weekend'] & ~df['is_holiday']]
    weekend_df = df[df['is_weekend'] | df['is_holiday']]
    
    # å„éƒ½çœŒã®é›»åŠ›ãƒ¬ãƒ³ã‚¸è¨ˆç®—
    prefecture_stats = {}
    for prefecture in df['prefecture'].unique():
        weekday_data = weekday_df[weekday_df['prefecture'] == prefecture]['actual_power']
        weekend_data = weekend_df[weekend_df['prefecture'] == prefecture]['actual_power']
        
        prefecture_stats[prefecture] = {
            'weekday_range': weekday_data.quantile(0.75) - weekday_data.quantile(0.25),
            'weekend_range': weekend_data.quantile(0.75) - weekend_data.quantile(0.25),
            'weekday_std': weekday_data.std(),
            'weekend_std': weekend_data.std()
        }
    
    return prefecture_stats

# å¯è¦–åŒ–é–¢æ•°
def plot_seasonal_patterns(pivot_data, title="é›»åŠ›éœ€è¦ãƒ‘ã‚¿ãƒ¼ãƒ³"):
    """å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³å¯è¦–åŒ–"""
    fig, ax = plt.subplots(figsize=(15, 10))
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
    sns.heatmap(pivot_data, annot=True, fmt='.0f', cmap='YlOrRd', ax=ax)
    ax.set_title(title, fontsize=16)
    ax.set_xlabel('æœˆ', fontsize=12)
    ax.set_ylabel('æ™‚é–“', fontsize=12)
    
    # 8å­£ç¯€ã‚°ãƒ«ãƒ¼ãƒ—ã®å¢ƒç•Œç·šè¿½åŠ 
    seasonal_boundaries = [2.5, 4.5, 6.5, 9.5, 11.5]  # 3æœˆ, 5æœˆ, 7æœˆ, 10æœˆ, 12æœˆ
    for boundary in seasonal_boundaries:
        ax.axvline(x=boundary, color='blue', linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    plt.show()
    
    return fig
```

### **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè£…è©³ç´°**

#### **å–¶æ¥­æ—¥åˆ¤å®šãƒ»ãƒ©ã‚°è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…**
```python
# å–¶æ¥­æ—¥åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
def is_business_day(date_obj, holiday_list, custom_holidays=None):
    """å–¶æ¥­æ—¥åˆ¤å®šï¼ˆåœŸæ—¥ãƒ»ç¥æ—¥ãƒ»ã‚«ã‚¹ã‚¿ãƒ ä¼‘æ—¥é™¤å¤–ï¼‰"""
    # åœŸæ—¥ãƒã‚§ãƒƒã‚¯
    if date_obj.weekday() >= 5:  # åœŸæ›œ=5, æ—¥æ›œ=6
        return False
    
    # ç¥æ—¥ãƒã‚§ãƒƒã‚¯
    date_str = date_obj.strftime('%Y-%m-%d')
    if date_str in holiday_list:
        return False
    
    # ã‚«ã‚¹ã‚¿ãƒ ä¼‘æ—¥ãƒã‚§ãƒƒã‚¯ï¼ˆå¹´æœ«å¹´å§‹ç­‰ï¼‰
    if custom_holidays:
        if date_str in custom_holidays:
            return False
    
    return True

# SQL ã§ã®å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚°è¨ˆç®—æ¤œè¨¼
def validate_business_lag_calculation():
    """å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚°è¨ˆç®—ã®æ­£ç¢ºæ€§æ¤œè¨¼"""
    query = """
    WITH business_data AS (
        SELECT 
            date,
            hour,
            actual_power,
            ROW_NUMBER() OVER (ORDER BY date, hour) as row_num
        FROM `energy-env.dev_energy_data.power_weather_integrated`
        WHERE is_holiday = false AND is_weekend = false
    ),
    lag_validation AS (
        SELECT 
            date,
            hour,
            actual_power,
            row_num,
            LAG(actual_power, 24) OVER (ORDER BY date, hour) as lag_1_business_day_manual,
            LAG(date, 24) OVER (ORDER BY date, hour) as lag_1_business_date
        FROM business_data
    )
    SELECT 
        date,
        hour,
        actual_power,
        lag_1_business_day_manual,
        lag_1_business_date,
        DATE_DIFF(date, lag_1_business_date, DAY) as day_diff,
        -- å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ã§ã¯å¿…ãš1å–¶æ¥­æ—¥ï¼ˆ1-3æ—¥ã®é–“ï¼‰ã®å·®
        CASE 
            WHEN DATE_DIFF(date, lag_1_business_date, DAY) BETWEEN 1 AND 3 THEN 'OK'
            ELSE 'ERROR'
        END as validation_status
    FROM lag_validation
    WHERE row_num > 24  -- æœ€åˆã®24è¡Œã¯é™¤å¤–
    ORDER BY date, hour
    LIMIT 100
    """
    
    df = client.query(query).to_dataframe()
    return df
```

#### **å¾ªç’°ç‰¹å¾´é‡ãƒ»å¤‰åŒ–ç‡è¨ˆç®—å®Ÿè£…**
```python
# å¾ªç’°ç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼
def validate_circular_features():
    """å¾ªç’°ç‰¹å¾´é‡ã®å‘¨æœŸæ€§æ¤œè¨¼"""
    import numpy as np
    
    hours = range(24)
    hour_sin = [np.sin(2 * np.pi * h / 24) for h in hours]
    hour_cos = [np.cos(2 * np.pi * h / 24) for h in hours]
    
    # å‘¨æœŸæ€§ç¢ºèª
    print("Hour | Sin    | Cos    | SinÂ²+CosÂ² ")
    print("-" * 35)
    for h in range(24):
        sin_val = hour_sin[h]
        cos_val = hour_cos[h]
        sum_squares = sin_val**2 + cos_val**2
        print(f"{h:4d} | {sin_val:6.3f} | {cos_val:6.3f} | {sum_squares:9.6f}")
    
    # 24æ™‚é–“å¾Œã®å€¤ã¨åŒã˜ã«ãªã‚‹ã“ã¨ã‚’ç¢ºèª
    assert abs(hour_sin[0] - hour_sin[0]) < 1e-10
    assert abs(hour_cos[0] - hour_cos[0]) < 1e-10
    
    return hour_sin, hour_cos

# å¤‰åŒ–ç‡è¨ˆç®—ã®å®‰å…¨æ€§æ¤œè¨¼
def safe_change_rate_calculation():
    """å¤‰åŒ–ç‡è¨ˆç®—ã®NULLå®‰å…¨æ€§ãƒ»ç•°å¸¸å€¤å¯¾å¿œ"""
    query = """
    SELECT 
        date,
        hour,
        actual_power,
        LAG(actual_power, 24) OVER (ORDER BY date, hour) as prev_power,
        
        -- å®‰å…¨ãªå¤‰åŒ–ç‡è¨ˆç®—
        CASE 
            WHEN LAG(actual_power, 24) OVER (ORDER BY date, hour) IS NULL THEN NULL
            WHEN LAG(actual_power, 24) OVER (ORDER BY date, hour) = 0 THEN NULL
            ELSE (actual_power - LAG(actual_power, 24) OVER (ORDER BY date, hour)) 
                 / LAG(actual_power, 24) OVER (ORDER BY date, hour) * 100
        END as change_rate_safe,
        
        -- NULLIFä½¿ç”¨ç‰ˆ
        (actual_power - LAG(actual_power, 24) OVER (ORDER BY date, hour)) 
        / NULLIF(LAG(actual_power, 24) OVER (ORDER BY date, hour), 0) * 100 as change_rate_nullif,
        
        -- ç•°å¸¸å€¤ãƒ•ãƒ©ã‚°
        CASE 
            WHEN ABS((actual_power - LAG(actual_power, 24) OVER (ORDER BY date, hour)) 
                     / NULLIF(LAG(actual_power, 24) OVER (ORDER BY date, hour), 0) * 100) > 50 
            THEN 'OUTLIER'
            ELSE 'NORMAL'
        END as outlier_flag
        
    FROM `energy-env.dev_energy_data.power_weather_integrated`
    ORDER BY date, hour
    LIMIT 1000
    """
    
    df = client.query(query).to_dataframe()
    
    # ç•°å¸¸å€¤çµ±è¨ˆ
    outlier_count = len(df[df['outlier_flag'] == 'OUTLIER'])
    total_count = len(df)
    print(f"Outlier rate: {outlier_count}/{total_count} ({outlier_count/total_count*100:.2f}%)")
    
    return df
```

### **æœ€çµ‚çµ±åˆãƒ»å“è³ªä¿è¨¼å®Ÿè£…**

#### **ml_featuresãƒ†ãƒ¼ãƒ–ãƒ«å“è³ªæ¤œè¨¼**
```python
def validate_ml_features_quality():
    """æ©Ÿæ¢°å­¦ç¿’ç”¨ç‰¹å¾´é‡ãƒ†ãƒ¼ãƒ–ãƒ«ã®å“è³ªæ¤œè¨¼"""
    
    # åŸºæœ¬çµ±è¨ˆ
    basic_stats_query = """
    SELECT 
        COUNT(*) as total_rows,
        COUNT(DISTINCT date) as unique_dates,
        COUNT(DISTINCT hour) as unique_hours,
        MIN(date) as min_date,
        MAX(date) as max_date,
        AVG(actual_power) as avg_power,
        STDDEV(actual_power) as std_power
    FROM `energy-env.dev_energy_data.ml_features`
    """
    
    # NULLå€¤ãƒã‚§ãƒƒã‚¯
    null_check_query = """
    SELECT 
        'actual_power' as column_name, 
        COUNT(*) - COUNT(actual_power) as null_count,
        COUNT(*) as total_count
    FROM `energy-env.dev_energy_data.ml_features`
    
    UNION ALL
    
    SELECT 
        'lag_1_business_day',
        COUNT(*) - COUNT(lag_1_business_day),
        COUNT(*)
    FROM `energy-env.dev_energy_data.ml_features`
    
    UNION ALL
    
    SELECT 
        'temperature_2m',
        COUNT(*) - COUNT(temperature_2m),
        COUNT(*)
    FROM `energy-env.dev_energy_data.ml_features`
    """
    
    # ç‰¹å¾´é‡ç›¸é–¢ãƒã‚§ãƒƒã‚¯
    correlation_query = """
    SELECT 
        CORR(actual_power, temperature_2m) as power_temp_corr,
        CORR(actual_power, lag_1_day) as power_lag1_corr,
        CORR(actual_power, lag_7_day) as power_lag7_corr,
        CORR(hour_sin, hour_cos) as circular_independence
    FROM `energy-env.dev_energy_data.ml_features`
    WHERE temperature_2m IS NOT NULL 
      AND lag_1_day IS NOT NULL
      AND lag_7_day IS NOT NULL
    """
    
    basic_stats = client.query(basic_stats_query).to_dataframe()
    null_stats = client.query(null_check_query).to_dataframe()
    correlation_stats = client.query(correlation_query).to_dataframe()
    
    print("=== ML Features Quality Report ===")
    print("\nåŸºæœ¬çµ±è¨ˆ:")
    print(basic_stats.to_string(index=False))
    print("\nNULLå€¤çµ±è¨ˆ:")
    print(null_stats.to_string(index=False))
    print("\nç›¸é–¢çµ±è¨ˆ:")
    print(correlation_stats.to_string(index=False))
    
    return basic_stats, null_stats, correlation_stats

# ç‰¹å¾´é‡é‡è¦åº¦äº‹å‰è©•ä¾¡
def preliminary_feature_importance():
    """XGBoostå®Ÿè£…å‰ã®ç‰¹å¾´é‡é‡è¦åº¦äº‹å‰è©•ä¾¡"""
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import train_test_split
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆè¨ˆç®—åŠ¹ç‡ã®ãŸã‚ï¼‰
    query = """
    SELECT 
        actual_power,
        temperature_2m,
        hour,
        CAST(is_weekend AS INT64) as is_weekend_int,
        CAST(is_holiday AS INT64) as is_holiday_int,
        hour_sin,
        hour_cos,
        lag_1_day,
        lag_7_day,
        avg_5_days,
        change_rate_1_day
    FROM `energy-env.dev_energy_data.ml_features`
    WHERE temperature_2m IS NOT NULL 
      AND lag_1_day IS NOT NULL 
      AND lag_7_day IS NOT NULL
      AND RAND() < 0.1  -- 10%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    """
    
    df = client.query(query).to_dataframe()
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†é›¢
    feature_cols = ['temperature_2m', 'hour', 'is_weekend_int', 'is_holiday_int', 
                    'hour_sin', 'hour_cos', 'lag_1_day', 'lag_7_day', 'avg_5_days', 'change_rate_1_day']
    X = df[feature_cols].fillna(0)  # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
    y = df['actual_power']
    
    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Random Forest ã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦è©•ä¾¡
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    
    # ç‰¹å¾´é‡é‡è¦åº¦è¡¨ç¤º
    importance_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("=== Preliminary Feature Importance (Random Forest) ===")
    print(importance_df.to_string(index=False))
    
    # äºˆæ¸¬ç²¾åº¦è©•ä¾¡
    train_score = rf.score(X_train, y_train)
    test_score = rf.score(X_test, y_test)
    print(f"\nRÂ² Score - Train: {train_score:.4f}, Test: {test_score:.4f}")
    
    return importance_df, rf
```

---

## ğŸ¯ Phase 7å®Ÿè£…æº–å‚™å®Œäº†

### **190ç‰¹å¾´é‡çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°æ§‹æˆ**

#### **åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ12å€‹ï¼‰**
```sql
-- é›»åŠ›ãƒ»ä¾›çµ¦ãƒ‡ãƒ¼ã‚¿
actual_power FLOAT64,      -- äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
supply_capacity FLOAT64,   -- ä¾›çµ¦åŠ›ï¼ˆå®¹é‡åˆ¶ç´„æƒ…å ±ï¼‰

-- æ°—è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆåƒè‘‰åŸºæº–ï¼‰
temperature_2m FLOAT64,         -- æ°—æ¸©ï¼ˆå†·æš–æˆ¿éœ€è¦ï¼‰
relative_humidity_2m FLOAT64,   -- æ¹¿åº¦ï¼ˆä½“æ„Ÿæ¸©åº¦ï¼‰
precipitation FLOAT64,          -- é™æ°´é‡ï¼ˆå¤–å‡ºãƒ»æ´»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
weather_code INT64,             -- å¤©å€™ã‚³ãƒ¼ãƒ‰ï¼ˆåŒ…æ‹¬çš„å¤©å€™çŠ¶æ…‹ï¼‰

-- ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼æƒ…å ±
day_of_week STRING,       -- æ›œæ—¥ï¼ˆé€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
is_weekend BOOLEAN,       -- é€±æœ«ãƒ•ãƒ©ã‚°ï¼ˆå¹³æ—¥/ä¼‘æ—¥åŒºåˆ¥ï¼‰
is_holiday BOOLEAN,       -- ç¥æ—¥ãƒ•ãƒ©ã‚°ï¼ˆç‰¹åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
month INTEGER,            -- æœˆï¼ˆå­£ç¯€æ€§ï¼‰

-- æ™‚é–“æƒ…å ±
date DATE,               -- æ—¥ä»˜ï¼ˆæ™‚ç³»åˆ—ã‚­ãƒ¼ï¼‰
hour INTEGER             -- æ™‚é–“ï¼ˆæ—¥å†…ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
```

#### **å¾ªç’°ç‰¹å¾´é‡ï¼ˆ2å€‹ï¼‰**
```sql
-- 24æ™‚é–“å‘¨æœŸæ€§
hour_sin FLOAT64,        -- SIN(2Ï€ * hour / 24) 
hour_cos FLOAT64         -- COS(2Ï€ * hour / 24)
```

#### **å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆ90å€‹ï¼‰**
```sql
-- ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆ30å€‹ï¼‰
lag_1_business_day FLOAT64,   -- 1å–¶æ¥­æ—¥å‰
lag_2_business_day FLOAT64,   -- 2å–¶æ¥­æ—¥å‰
-- ... 30å–¶æ¥­æ—¥å‰ã¾ã§
lag_30_business_day FLOAT64,

-- ç§»å‹•å¹³å‡ï¼ˆ6å€‹ï¼‰
avg_5_business_days FLOAT64,   -- 5å–¶æ¥­æ—¥ç§»å‹•å¹³å‡
avg_10_business_days FLOAT64,  -- 10å–¶æ¥­æ—¥ç§»å‹•å¹³å‡
avg_15_business_days FLOAT64,  -- 15å–¶æ¥­æ—¥ç§»å‹•å¹³å‡
avg_20_business_days FLOAT64,  -- 20å–¶æ¥­æ—¥ç§»å‹•å¹³å‡
avg_25_business_days FLOAT64,  -- 25å–¶æ¥­æ—¥ç§»å‹•å¹³å‡
avg_30_business_days FLOAT64,  -- 30å–¶æ¥­æ—¥ç§»å‹•å¹³å‡

-- å¤‰åŒ–ç‡ï¼ˆ30å€‹ï¼‰
change_rate_1_business_day FLOAT64,   -- 1å–¶æ¥­æ—¥å‰æ¯”ç‡ï¼ˆ%ï¼‰
change_rate_2_business_day FLOAT64,   -- 2å–¶æ¥­æ—¥å‰æ¯”ç‡ï¼ˆ%ï¼‰
-- ... 30å–¶æ¥­æ—¥å‰æ¯”ç‡ã¾ã§
change_rate_30_business_day FLOAT64,

-- æ®‹ã‚Š24å€‹ã¯é‡è¤‡ã‚«ã‚¦ãƒ³ãƒˆï¼ˆlagé‡è¤‡ç­‰ï¼‰ã«ã‚ˆã‚‹å†…éƒ¨èª¿æ•´
```

#### **å…¨æ—¥ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆ90å€‹ï¼‰**
```sql
-- ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆ30å€‹ï¼‰
lag_1_day FLOAT64,       -- 1æ—¥å‰ï¼ˆå®Ÿæ™‚é–“ï¼‰
lag_2_day FLOAT64,       -- 2æ—¥å‰
-- ... 30æ—¥å‰ã¾ã§
lag_30_day FLOAT64,

-- ç§»å‹•å¹³å‡ï¼ˆ6å€‹ï¼‰
avg_5_days FLOAT64,      -- 5æ—¥ç§»å‹•å¹³å‡
avg_10_days FLOAT64,     -- 10æ—¥ç§»å‹•å¹³å‡
avg_15_days FLOAT64,     -- 15æ—¥ç§»å‹•å¹³å‡
avg_20_days FLOAT64,     -- 20æ—¥ç§»å‹•å¹³å‡
avg_25_days FLOAT64,     -- 25æ—¥ç§»å‹•å¹³å‡
avg_30_days FLOAT64,     -- 30æ—¥ç§»å‹•å¹³å‡

-- å¤‰åŒ–ç‡ï¼ˆ30å€‹ï¼‰
change_rate_1_day FLOAT64,    -- 1æ—¥å‰æ¯”ç‡ï¼ˆ%ï¼‰
change_rate_2_day FLOAT64,    -- 2æ—¥å‰æ¯”ç‡ï¼ˆ%ï¼‰
-- ... 30æ—¥å‰æ¯”ç‡ã¾ã§
change_rate_30_day FLOAT64,

-- æ®‹ã‚Š24å€‹ã¯é‡è¤‡ã‚«ã‚¦ãƒ³ãƒˆèª¿æ•´
```

### **XGBoostå®Ÿè£…æº–å‚™ãƒ»è¨­è¨ˆæ–¹é‡**

#### **ç‰¹å¾´é‡é¸æŠæˆ¦ç•¥**
```python
# Phase 7å®Ÿè£…äºˆå®šã®XGBoostç‰¹å¾´é‡é¸æŠã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
feature_groups = {
    'calendar': ['is_weekend', 'is_holiday', 'month', 'day_of_week', 'hour'],
    'circular': ['hour_sin', 'hour_cos'],
    'weather': ['temperature_2m', 'relative_humidity_2m', 'precipitation', 'weather_code'],
    'business_lag': ['lag_1_business_day', 'lag_5_business_day', 'lag_10_business_day'],
    'realtime_lag': ['lag_1_day', 'lag_7_day', 'lag_14_day'],
    'moving_avg': ['avg_5_business_days', 'avg_5_days', 'avg_10_business_days'],
    'change_rate': ['change_rate_1_business_day', 'change_rate_1_day', 'change_rate_7_day']
}

# æ®µéšçš„ç‰¹å¾´é‡è¿½åŠ å®Ÿé¨“è¨­è¨ˆ
experiment_sets = [
    'calendar',                                    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
    'calendar + circular',                         # æ™‚é–“å‘¨æœŸæ€§è¿½åŠ 
    'calendar + circular + weather',               # æ°—è±¡æƒ…å ±è¿½åŠ 
    'calendar + circular + weather + business_lag', # å–¶æ¥­æ—¥ãƒ©ã‚°è¿½åŠ 
    'all_features'                                 # å…¨ç‰¹å¾´é‡
]
```

#### **ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™è¨­å®š**
```python
# Phase 7ã§å®Ÿè£…äºˆå®šã®è©•ä¾¡æŒ‡æ¨™
evaluation_metrics = {
    'primary': 'MAPE',      # Mean Absolute Percentage Error (ç›®æ¨™: 8-12%)
    'secondary': [
        'MAE',              # Mean Absolute Error
        'RMSE',             # Root Mean Square Error
        'RÂ²',               # æ±ºå®šä¿‚æ•°
        'directional_accuracy'  # æ–¹å‘æ€§ç²¾åº¦ï¼ˆä¸Šæ˜‡/ä¸‹é™äºˆæ¸¬ï¼‰
    ]
}

# æ™‚é–“åˆ¥ãƒ»æ¡ä»¶åˆ¥è©•ä¾¡è¨­è¨ˆ
evaluation_conditions = {
    'overall': 'all_data',
    'peak_hours': 'hour IN (10, 11, 14, 15)',
    'weekday': 'is_weekend = false AND is_holiday = false',
    'weekend': 'is_weekend = true OR is_holiday = true',
    'summer': 'month IN (7, 8, 9)',
    'winter': 'month IN (12, 1, 2)',
    'mild_season': 'month IN (4, 5, 6, 10, 11)'
}
```

#### **ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­è¨ˆ**
```python
# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­è¨ˆ
cv_strategy = {
    'method': 'TimeSeriesSplit',
    'n_splits': 5,
    'test_size': '30_days',
    'gap': '7_days',  # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®é–“éš”
    'validation_approach': 'walk_forward'
}

# å­¦ç¿’ãƒ»è©•ä¾¡æœŸé–“è¨­è¨ˆ
train_test_split = {
    'train_period': '2023-01-01 to 2025-05-31',  # 2å¹´5ãƒ¶æœˆ
    'validation_period': '2025-06-01 to 2025-06-30',  # 1ãƒ¶æœˆ
    'test_period': '2025-07-01 to 2025-07-26',   # æœ€æ–°ãƒ‡ãƒ¼ã‚¿
    'feature_lag_consideration': 'max_30_days'
}
```

### **å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ æº–å‚™**

#### **Phase 7å®Ÿè£…äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«**
```python
# src/models/xgboost_predictor.py
class XGBoostPredictor:
    def __init__(self, feature_config, model_params):
        self.feature_config = feature_config
        self.model_params = model_params
        self.model = None
        self.feature_importance_ = None
    
    def prepare_features(self, df, feature_groups):
        """ç‰¹å¾´é‡é¸æŠãƒ»å‰å‡¦ç†"""
        pass
    
    def train(self, X_train, y_train, X_val, y_val):
        """XGBoostãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""
        pass
    
    def predict(self, X):
        """äºˆæ¸¬å®Ÿè¡Œ"""
        pass
    
    def evaluate(self, X_test, y_test, conditions):
        """å¤šè§’çš„è©•ä¾¡"""
        pass

# src/models/feature_selector.py  
class FeatureSelector:
    def __init__(self, ml_features_table):
        self.table = ml_features_table
    
    def get_feature_groups(self):
        """ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—å®šç¾©"""
        pass
    
    def select_by_importance(self, importance_threshold):
        """é‡è¦åº¦ãƒ™ãƒ¼ã‚¹é¸æŠ"""
        pass
    
    def handle_missing_values(self, strategy):
        """æ¬ æå€¤å‡¦ç†æˆ¦ç•¥"""
        pass

# src/evaluation/model_evaluator.py
class ModelEvaluator:
    def __init__(self, metrics_config):
        self.metrics = metrics_config
    
    def calculate_mape(self, y_true, y_pred):
        """MAPEè¨ˆç®—"""
        pass
    
    def evaluate_by_conditions(self, model, test_data, conditions):
        """æ¡ä»¶åˆ¥è©•ä¾¡"""
        pass
    
    def create_evaluation_report(self, results):
        """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        pass
```

#### **äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ çµ±åˆè¨­è¨ˆ**
```python
# src/pipelines/prediction_pipeline.py
class PredictionPipeline:
    def __init__(self, model_path, feature_config):
        self.model = self.load_model(model_path)
        self.feature_config = feature_config
    
    def predict_next_24_hours(self, current_datetime):
        """ç¿Œæ—¥24æ™‚é–“äºˆæ¸¬"""
        # 1. æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
        # 2. ç‰¹å¾´é‡ç”Ÿæˆï¼ˆãƒ©ã‚°ãƒ»ç§»å‹•å¹³å‡ãƒ»å¤‰åŒ–ç‡ï¼‰
        # 3. æ°—è±¡äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        # 4. ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬å®Ÿè¡Œ
        # 5. çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ»ä¿å­˜
        pass
    
    def predict_next_week(self, current_datetime):
        """1é€±é–“å…ˆäºˆæ¸¬"""
        pass
    
    def update_model_with_new_data(self):
        """æ–°ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ›´æ–°"""
        pass

# tests/test_xgboost_predictor.py
class TestXGBoostPredictor:
    def test_feature_preparation(self):
        """ç‰¹å¾´é‡æº–å‚™ãƒ†ã‚¹ãƒˆ"""
        pass
    
    def test_model_training(self):
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆ"""
        pass
    
    def test_prediction_accuracy(self):
        """äºˆæ¸¬ç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        pass
    
    def test_feature_importance(self):
        """ç‰¹å¾´é‡é‡è¦åº¦ãƒ†ã‚¹ãƒˆ"""
        pass
```

### **Phase 7æˆåŠŸæŒ‡æ¨™ãƒ»æ¤œè¨¼é …ç›®**

#### **æŠ€è¡“çš„æˆåŠŸæŒ‡æ¨™**
```python
success_criteria = {
    'model_performance': {
        'mape_overall': '< 12%',
        'mape_peak_hours': '< 15%',
        'mape_weekday': '< 10%',
        'mape_weekend': '< 8%',
        'r2_score': '> 0.85'
    },
    'feature_engineering': {
        'calendar_importance': '> 30%',    # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´é‡ã®åˆè¨ˆé‡è¦åº¦
        'lag_importance': '> 25%',         # ãƒ©ã‚°ç‰¹å¾´é‡ã®åˆè¨ˆé‡è¦åº¦
        'weather_importance': '10-20%',   # æ°—è±¡ç‰¹å¾´é‡ã®åˆè¨ˆé‡è¦åº¦
        'top_10_features_coverage': '> 70%'  # ä¸Šä½10ç‰¹å¾´é‡ã®ã‚«ãƒãƒ¼ç‡
    },
    'system_robustness': {
        'training_time': '< 30min',
        'prediction_time': '< 10sec',
        'memory_usage': '< 8GB',
        'cross_validation_stability': 'CV_std < 2%'
    }
}
```

#### **ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤æ¤œè¨¼é …ç›®**
```python
business_validation = {
    'practical_usability': {
        'peak_hour_accuracy': 'ãƒ”ãƒ¼ã‚¯æ™‚é–“ï¼ˆ10-11æ™‚ã€14-15æ™‚ï¼‰ã§ã®é«˜ç²¾åº¦',
        'weekend_prediction': 'ä¼‘æ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ­£ç¢ºãªäºˆæ¸¬',
        'seasonal_adaptation': 'å­£ç¯€å¤‰åŒ–ã¸ã®é©å¿œæ€§',
        'holiday_effect': 'ç¥æ—¥åŠ¹æœï¼ˆ2000ä¸‡kWãƒ¬ãƒ³ã‚¸ç¸®å°ï¼‰ã®é©åˆ‡ãªå­¦ç¿’'
    },
    'operational_value': {
        'daily_prediction': 'ç¿Œæ—¥24æ™‚é–“äºˆæ¸¬ã®å®Ÿç”¨ç²¾åº¦',
        'weekly_forecast': '1é€±é–“å…ˆäºˆæ¸¬ã®åŸºæœ¬ç²¾åº¦',
        'model_interpretability': 'äºˆæ¸¬æ ¹æ‹ ã®èª¬æ˜å¯èƒ½æ€§',
        'update_efficiency': 'æ–°ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ¢ãƒ‡ãƒ«æ›´æ–°åŠ¹ç‡'
    },
    'scalability': {
        'feature_extensibility': 'æ–°ç‰¹å¾´é‡è¿½åŠ ã¸ã®å¯¾å¿œ',
        'data_volume_scaling': 'ãƒ‡ãƒ¼ã‚¿é‡å¢—åŠ ã¸ã®å¯¾å¿œ',
        'real_time_prediction': 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã¸ã®æ‹¡å¼µæ€§'
    }
}
```

### **Phase 7å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—è©³ç´°**

#### **Week 1: XGBooståŸºæœ¬å®Ÿè£…**
- XGBoostPredictor ã‚¯ãƒ©ã‚¹å®Ÿè£…
- åŸºæœ¬ç‰¹å¾´é‡ï¼ˆcalendar + circular + weatherï¼‰ã§ã®å­¦ç¿’
- MAPEè©•ä¾¡ãƒ»Feature Importanceåˆ†æ
- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç²¾åº¦ç¢ºç«‹

#### **Week 2: æ™‚ç³»åˆ—ç‰¹å¾´é‡çµ±åˆ**
- å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ãƒ»å…¨æ—¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚°ç‰¹å¾´é‡è¿½åŠ 
- ç§»å‹•å¹³å‡ãƒ»å¤‰åŒ–ç‡ç‰¹å¾´é‡çµ±åˆ
- æ®µéšçš„ç‰¹å¾´é‡è¿½åŠ å®Ÿé¨“
- æœ€é©ç‰¹å¾´é‡ã‚»ãƒƒãƒˆæ±ºå®š

#### **Week 3: ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ãƒ»è©•ä¾¡**
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…
- æ¡ä»¶åˆ¥è©•ä¾¡ï¼ˆå¹³æ—¥/ä¼‘æ—¥/å­£ç¯€åˆ¥ï¼‰
- æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ç¢ºå®š

#### **Week 4: äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ**
- PredictionPipelineå®Ÿè£…
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬æ©Ÿèƒ½
- ãƒ¢ãƒ‡ãƒ«æ›´æ–°ãƒ»é‹ç”¨æ©Ÿèƒ½
- åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

---

## ğŸš€ Phase 1-6å®Œäº†ã‚µãƒãƒªãƒ¼

### **æ§‹ç¯‰å®Œäº†ã‚·ã‚¹ãƒ†ãƒ **
- âœ… **ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**: 30ãƒ¶æœˆåˆ†ãƒ‡ãƒ¼ã‚¿å®Œå…¨è‡ªå‹•åŒ–
- âœ… **BigQueryçµ±åˆåŸºç›¤**: 197,856ãƒ¬ã‚³ãƒ¼ãƒ‰çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒˆ
- âœ… **æ°—è±¡ãƒ‡ãƒ¼ã‚¿çµ±åˆ**: 22ä¸‡ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ»åƒè‘‰æ°—æ¸©ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–
- âœ… **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: 190ç‰¹å¾´é‡çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- âœ… **åˆ†æç’°å¢ƒ**: Jupyter + BigQueryé€£æºãƒ»æ¢ç´¢çš„åˆ†æåŸºç›¤

### **æŠ€è¡“çš„æˆç†Ÿåº¦**
- âœ… **Pythoné«˜åº¦é–‹ç™º**: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ãƒ»CLIçµ±åˆãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- âœ… **Google Cloudçµ±åˆ**: BigQueryåˆ¶ç´„å›é¿ãƒ»å‹çµ±ä¸€ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- âœ… **ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: ETLè¨­è¨ˆãƒ»å¤§è¦æ¨¡å‡¦ç†ãƒ»å“è³ªä¿è¨¼
- âœ… **çµ±è¨ˆãƒ»åˆ†ææ€è€ƒ**: Ridge Plotåˆ†æãƒ»å®Ÿè¨¼çš„æ„æ€æ±ºå®šãƒ»å¯è¦–åŒ–è¨­è¨ˆ
- âœ… **é‹ç”¨è¨­è¨ˆ**: ãƒ­ã‚°ç›£è¦–ãƒ»è‡ªå‹•åŒ–ãƒ»ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†

### **Phase 7å®Ÿè£…æº–å‚™**
- âœ… **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: ml_features ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ190ç‰¹å¾´é‡å®Œæˆï¼‰
- âœ… **äºˆæ¸¬æˆ¦ç•¥**: XGBoostå›å¸°ãƒ»ç‰¹å¾´é‡é‡è¦–æˆ¦ç•¥ç¢ºå®š
- âœ… **æŠ€è¡“åŸºç›¤**: BigQueryæœ€é©åŒ–ãƒ»GCSçµ±åˆãƒ»åˆ†æç’°å¢ƒ
- âœ… **å®Ÿè£…è¨­è¨ˆ**: ã‚¯ãƒ©ã‚¹è¨­è¨ˆãƒ»è©•ä¾¡æŒ‡æ¨™ãƒ»æˆåŠŸåŸºæº–æ˜ç¢ºåŒ–

**ğŸ‰ Phase 1-6ã«ã‚ˆã‚Šã€ã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®åŸºç›¤æ§‹ç¯‰ãƒ»ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãŒå®Œæˆã—ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ï¼ˆPhase 7ï¼‰ã¸ã®æº–å‚™ãŒå®Œå…¨ã«æ•´ã„ã¾ã—ãŸï¼æ¬¡ã®ãƒ•ã‚§ãƒ¼ã‚ºã§ã¯190ç‰¹å¾´é‡ã‚’æ´»ç”¨ã—ãŸXGBoostäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ã«ã‚ˆã‚Šã€MAPE 8-12%ç›®æ¨™ã§ã®å®Ÿç”¨çš„ãªé›»åŠ›éœ€è¦äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Œæˆã•ã›ã¾ã™ã€‚**