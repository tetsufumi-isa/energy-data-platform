# Phase 11: BQログ修正・天気データ再投入完了

**日時**: 2025年10月11日
**Phase**: 11（基盤整備→日次運用→予測精度分析）

---

## セッション概要

BigQueryログ書き込みの2つの重大なバグを発見・修正し、天気データの重複問題を解決。天気データテーブルを再作成してクリーンな状態で再投入完了。

---

## 主要成果

### 1. BigQueryログ書き込みバグ修正（2件）

#### バグ1: `insert_rows_json()`戻り値未チェック
**問題**:
- `insert_rows_json()`はエラー時に例外を投げず、エラーリストを返却
- 全モジュールで戻り値をチェックしていなかった
- → BigQuery書き込み失敗が**完全にサイレント化**

**修正内容**（4ファイル、16箇所）:
```python
# 修正前（サイレント失敗）
self.bq_client.insert_rows_json(table_id, [log_data])

# 修正後（エラー検出）
errors = self.bq_client.insert_rows_json(table_id, [log_data])
if errors:
    raise Exception(f"BigQuery insert errors: {errors}")
```

**修正ファイル**:
- `src/data_processing/weather_bigquery_loader.py:187-189`
- `src/data_processing/data_downloader.py:71-73`
- `src/data_processing/power_bigquery_loader.py:188-190`
- `src/data_processing/weather_downloader.py:104-106`

#### バグ2: `additional_info` JSON型対応不足
**問題**:
- テーブル定義: `additional_info JSON`
- Pythonコード: Python辞書をそのまま渡していた
- → BigQueryが型不一致エラー: "This field: additional_info is not a record."

**修正内容**（4ファイル、16箇所）:
```python
# 修正前（型不一致）
"additional_info": {
    "data_type": "historical",
    "files_processed": 1
}

# 修正後（JSON文字列化）
"additional_info": json.dumps({
    "data_type": "historical",
    "files_processed": 1
})
```

**技術的理解の向上**:
- BigQueryのJSON型はJSON文字列を期待
- RECORD型（構造化型）との違い
- JSON型のメリット: スキーマレス、柔軟、プロセスごとに異なる情報を記録可能

### 2. 天気データ重複問題の解決

**問題発見**:
```sql
-- 重複確認クエリ結果
count_hour    count_count_hour
24            92        -- 正常
216           912       -- 9倍重複
240           10        -- 10倍重複
```

**原因**: ストリーミングバッファによるDELETE制約
- `insert_rows_json()`で投入したデータは数時間DELETE不可
- 重複削除処理が機能しなかった

**解決策**: テーブル削除→再作成
- `sql/create_weather_data_table.sql` 作成
- パーティション設定: `PARTITION BY date`
- 保持期間: 3年（1095日）

**再投入結果**:
```
期間: 2023/1/1～2025/10/10
投入レコード数: 24336行（1014日分）
重複: 0件

検証結果:
count_hour    count_count_hour
24            1014       -- 完璧！
```

### 3. weather_bigquery_loader機能追加

**追加機能**: `--file-name` オプション
```python
parser.add_argument('--file-name', type=str, default=None,
                   help='ファイル名を直接指定（例: chiba_2025_07_historical_range.json）')
```

**背景**:
- 既存: 日付ベース命名規則のみ対応（`chiba_2025_1011_historical.json`）
- 問題: 期間指定ダウンロード時のファイル名が異なる（`chiba_2023_01_historical_range.json`）
- ユーザー要望: 「ファイル指定して、なければそこでエラーがいい」

### 4. Windows環境対応（emoji削除）

**問題**: `UnicodeEncodeError: 'cp932' codec can't encode character`

**修正内容**:
- `weather_downloader.py`: 7箇所
- `data_downloader.py`: 複数箇所
- `main_etl.py`: 7箇所

コーディング方針に従い、全てのemojiを削除。

---

## 技術的理解の向上

### BigQuery JSON型 vs RECORD型の比較

| 項目 | JSON型（採用） | RECORD型 |
|------|---------------|----------|
| スキーマ | 柔軟（スキーマレス） | 固定 |
| クエリ | `JSON_EXTRACT(additional_info, '$.data_type')` | `additional_info.data_type` |
| Pythonから | `json.dumps({...})` | 辞書そのまま |
| 用途 | プロセスごとに異なる情報 | 固定構造データ |
| メリット | 後からフィールド追加が自由 | 型チェック、パフォーマンス良 |

**選択理由**: 各プロセス（TEPCO_API、WEATHER_API、POWER_BQ_LOAD等）で記録する情報が全く異なるため、JSON型が最適。

### ストリーミングバッファの制約

- `insert_rows_json()`で投入したデータは即座にクエリ可能
- ただし、数時間はDELETE/UPDATE不可
- 重複削除が必要な場合、別の投入方法を検討する必要あり

---

## 次回セッション予定

### 次回TODO
1. **電力データの再投入**（2023年～最新月）
2. 日次処理実装（電気・天気の自動実行）
3. 異常検知システム実装
4. 過去5回分の予測実行（例：10/4、10/3、10/2、10/1、9/30）
5. 予測精度検証モジュール実装（1日目～16日目の精度を5回分平均で算出）

### 優先順位
1. 電力データ再投入（基盤データ整備完了のため）
2. 日次処理実装（自動運用開始）
3. 予測精度検証（Phase 11の主目的）

---

## ファイル変更サマリー

### 新規作成
- `sql/create_weather_data_table.sql` - 天気データテーブル定義（パーティション付き）

### 修正ファイル
- `src/data_processing/weather_bigquery_loader.py` - 2つのバグ修正 + --file-nameオプション追加
- `src/data_processing/power_bigquery_loader.py` - 2つのバグ修正
- `src/data_processing/data_downloader.py` - 2つのバグ修正 + emoji削除
- `src/data_processing/weather_downloader.py` - 2つのバグ修正 + emoji削除
- `src/pipelines/main_etl.py` - emoji削除

### 実行コマンド履歴
```bash
# 天気データダウンロード
python -m src.data_processing.weather_downloader --start-date 2023-01-01 --end-date 2025-10-10

# 天気データBQ投入
python -m src.data_processing.weather_bigquery_loader --data-type historical --file-name chiba_2023_01_historical_range.json
```

---

## TODOリスト（最新）

### 完了
- [x] 最新月までのデータ取得実行（電気・天気）
- [x] BQログ書き込みバグ修正（4ファイル：insert_rows_json戻り値チェック追加）
- [x] additional_info JSON型対応修正（4ファイル：json.dumps()追加）
- [x] 天気データテーブル再作成・再投入（2023/1/1～2025/10/10）

### 次回
- [ ] 電力データの再投入（2023年～最新月）
- [ ] 日次処理実装（電気・天気の自動実行）
- [ ] 異常検知システム実装
- [ ] 過去5回分の予測実行
- [ ] 予測精度検証モジュール実装
- [ ] BQ修正・作成（精度検証結果反映）
- [ ] 日次実行セット（予測+精度検証の自動運用開始）
- [ ] Looker Studio予測結果表示ダッシュボード作成
- [ ] Looker Studio監視ページ作成（プロセス実行ログ・エラー監視）
- [ ] gcs_uploaderをdata_type対応にリファクタリング（power/weather/prediction）
- [ ] power_bigquery_loaderからgcs_uploader呼び出し実装
- [ ] weather_bigquery_loaderからgcs_uploader呼び出し実装
- [ ] prediction系モジュールからgcs_uploader呼び出し実装
- [ ] Airflow環境構築・DAG実装（Cloud Composer使用）
