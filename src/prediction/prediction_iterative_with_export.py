# %%
# ================================================================
# æ®µéšçš„äºˆæ¸¬ãƒ†ã‚¹ãƒˆ + äºˆæ¸¬çµæœä¿å­˜æ©Ÿèƒ½ï¼ˆPhase 11ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ï¼‰
# ç›®çš„: å®Ÿé‹ç”¨ã§ã®äºˆæ¸¬å€¤ä¾å­˜ã«ã‚ˆã‚‹ç²¾åº¦åŠ£åŒ–æ¸¬å®š + CSVä¿å­˜
# æœŸé–“: 2025-06-01 ï½ 2025-06-16 (16æ—¥é–“Ã—24æ™‚é–“=384å›äºˆæ¸¬)
# æ–°æ©Ÿèƒ½: äºˆæ¸¬çµæœã‚’CSVå½¢å¼ã§ä¿å­˜ï¼ˆGCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»Looker Studioç”¨ï¼‰
# ãƒ­ã‚°å¯¾å¿œ: é‡è¦ãªãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒ­ã‚°å‡ºåŠ›
# ================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score
import xgboost as xgb
from datetime import datetime, timedelta
import warnings
import os
from pathlib import Path
from logging import getLogger, FileHandler, StreamHandler, Formatter, INFO
import sys
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
def setup_prediction_logging():
    """äºˆæ¸¬å®Ÿé¨“ç”¨ã®ãƒ­ã‚°è¨­å®š"""
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    energy_env_path = os.getenv('ENERGY_ENV_PATH', '.')
    log_dir = Path(energy_env_path) / 'logs' / 'predictions'
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = log_dir / f'prediction_iterative_{timestamp}.log'
    
    # ãƒ­ã‚¬ãƒ¼è¨­å®š
    logger = getLogger('energy_env.predictions')
    logger.setLevel(INFO)
    
    # æ—¢å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
    logger.handlers.clear()
    
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
    formatter = Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    file_handler = FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆé‡è¦ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ï¼‰
    console_handler = StreamHandler(sys.stdout)
    console_handler.setFormatter(Formatter('%(message)s'))
    logger.addHandler(console_handler)
    
    return logger, log_file

# ãƒ­ã‚°è¨­å®šå®Ÿè¡Œ
logger, log_file_path = setup_prediction_logging()

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'Meiryo'
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

logger.info("ğŸ”„ æ®µéšçš„äºˆæ¸¬å®Ÿé¨“é–‹å§‹ï¼ˆä¿å­˜æ©Ÿèƒ½ä»˜ããƒ»dropna()ãªã—ï¼‰")
logger.info("=" * 60)
logger.info(f"ğŸ“‹ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file_path}")
print("ğŸ”„ æ®µéšçš„äºˆæ¸¬ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆä¿å­˜æ©Ÿèƒ½ä»˜ããƒ»dropna()ãªã—ï¼‰")
print("=" * 60)

# %%
# ================================================================
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»åŸºæœ¬ç¢ºèª
# ================================================================

# ml_features.csvã®èª­ã¿è¾¼ã¿
ml_features = pd.read_csv('../../../data/ml/ml_features.csv')
print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {ml_features.shape}")

# dateã¨houråˆ—ã‹ã‚‰datetimeåˆ—ã‚’ä½œæˆ
ml_features['datetime'] = pd.to_datetime(ml_features['date'].astype(str) + ' ' + ml_features['hour'].astype(str).str.zfill(2) + ':00:00')

# calendar_data_with_prev_business.csvèª­ã¿è¾¼ã¿
calendar_data = pd.read_csv('../../../data/ml/calendar_data_with_prev_business.csv')
print(f"ğŸ“… å–¶æ¥­æ—¥ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")

# datetimeåˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®šï¼ˆé«˜é€Ÿæ¤œç´¢ã®ãŸã‚ï¼‰
ml_features = ml_features.set_index('datetime')
calendar_data['date'] = pd.to_datetime(calendar_data['date'])
calendar_data = calendar_data.set_index('date')

print(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")

# %%
# ================================================================
# 2. Phase 9ç‰¹å¾´é‡å®šç¾©ï¼ˆ12ç‰¹å¾´é‡ï¼‰
# ================================================================

# Phase 9ã§ä½¿ç”¨ã—ãŸ12ç‰¹å¾´é‡ã‚’å®šç¾©
features = [
    'hour',                    # æ™‚é–“ï¼ˆ0-23ï¼‰
    'is_weekend',             # é€±æœ«ãƒ•ãƒ©ã‚°
    'is_holiday',             # ç¥æ—¥ãƒ•ãƒ©ã‚°  
    'month',                  # æœˆï¼ˆ1-12ï¼‰
    'hour_sin',               # æ™‚é–“å‘¨æœŸæ€§ï¼ˆsinï¼‰
    'hour_cos',               # æ™‚é–“å‘¨æœŸæ€§ï¼ˆcosï¼‰
    'lag_1_day',              # 1æ—¥å‰åŒæ™‚åˆ»ï¼ˆé‡è¦ï¼ï¼‰
    'lag_7_day',              # 7æ—¥å‰åŒæ™‚åˆ»
    'lag_1_business_day',     # 1å–¶æ¥­æ—¥å‰åŒæ™‚åˆ»ï¼ˆé‡è¦åº¦84.3%ï¼ï¼‰
    'temperature_2m',         # æ°—æ¸©
    'relative_humidity_2m',   # æ¹¿åº¦
    'precipitation'           # é™æ°´é‡
]

print(f"\nğŸ”§ ä½¿ç”¨ç‰¹å¾´é‡: {len(features)}å€‹")
for i, feature in enumerate(features, 1):
    print(f"  {i:2d}. {feature}")

# %%
# ================================================================
# 3. XGBoostãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆåœŸæ—¥ç¥æ—¥å¯¾å¿œãƒ»dropna()ãªã—ï¼‰
# ================================================================

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆï½2025/5/31ã¾ã§ï¼‰
train_end_date = '2025-05-31'
train_data = ml_features[ml_features.index <= train_end_date].copy()

# ç‰¹å¾´é‡ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™ï¼ˆæ¬ æå€¤è¾¼ã¿ï¼‰
X_train = train_data[features]
y_train = train_data['actual_power']

print(f"\nğŸ¤– XGBoostãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")
print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train):,}ä»¶ï¼ˆæ¬ æå€¤è¾¼ã¿ï¼‰")
print(f"å­¦ç¿’æœŸé–“: {train_data.index.min()} ï½ {train_data.index.max()}")

# XGBoostãƒ¢ãƒ‡ãƒ«ï¼ˆPhase 9è¨­å®šï¼‰
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    max_depth=8,
    learning_rate=0.05,
    random_state=42,
    verbosity=0
)

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
logger.info("ğŸ¤– XGBoostãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")
xgb_model.fit(X_train, y_train)
logger.info("âœ… XGBoostãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")
print("ğŸ¤– XGBoostãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")

# %%
# ================================================================
# 4. ç‰¹å¾´é‡æº–å‚™é–¢æ•°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰
# ================================================================

def prepare_features_no_fallback(target_datetime, predictions):
    """
    äºˆæ¸¬å¯¾è±¡æ™‚åˆ»ã®ç‰¹å¾´é‡ã‚’æº–å‚™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰
    
    Args:
        target_datetime: äºˆæ¸¬å¯¾è±¡æ—¥æ™‚
        predictions: éå»ã®äºˆæ¸¬å€¤è¾æ›¸
        
    Returns:
        list: 12ç‰¹å¾´é‡ã®å€¤ãƒªã‚¹ãƒˆ
    """
    # ml_features.csvã‹ã‚‰è©²å½“æ™‚åˆ»ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    if target_datetime in ml_features.index:
        # åŸºæœ¬çš„ã«ml_features.csvã®å€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨
        feature_values = ml_features.loc[target_datetime, features].values.copy()
        
        # lagç‰¹å¾´é‡ã®ã¿äºˆæ¸¬å€¤ã§ä¸Šæ›¸ãï¼ˆè©²å½“ã™ã‚‹å ´åˆã®ã¿ï¼‰
        for i, feature in enumerate(features):
            if feature == 'lag_1_day':
                lag_datetime = target_datetime - timedelta(days=1)
                if lag_datetime in predictions:
                    feature_values[i] = predictions[lag_datetime]
            
            elif feature == 'lag_7_day':
                lag_datetime = target_datetime - timedelta(days=7)
                if lag_datetime in predictions:
                    feature_values[i] = predictions[lag_datetime]
                    
            elif feature == 'lag_1_business_day':
                # 1å–¶æ¥­æ—¥å‰ã®äºˆæ¸¬å€¤ãŒã‚ã‚Œã°ä½¿ç”¨ï¼ˆãªã‘ã‚Œã°nanã®ã¾ã¾ï¼‰
                for days_back in range(1, 8):  # æœ€å¤§7æ—¥å‰ã¾ã§æ¢ç´¢
                    lag_datetime = target_datetime - timedelta(days=days_back)
                    if lag_datetime in predictions:
                        # å–¶æ¥­æ—¥åˆ¤å®š
                        lag_date = lag_datetime.date()
                        if lag_date in calendar_data.index and not calendar_data.loc[lag_date, 'is_holiday'] and lag_datetime.weekday() < 5:
                            feature_values[i] = predictions[lag_datetime]
                            break
        
        return feature_values
    
    else:
        # ml_features.csvã«ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        raise ValueError(f"Target datetime {target_datetime} not found in ml_features")

# %%
# ================================================================
# 5. æ®µéšçš„äºˆæ¸¬å®Ÿè¡Œï¼ˆ2025-06-01 ï½ 2025-06-16ï¼‰
# ================================================================

# äºˆæ¸¬æœŸé–“è¨­å®š
start_date = pd.to_datetime('2025-06-01')
end_date = pd.to_datetime('2025-06-16')

# äºˆæ¸¬çµæœæ ¼ç´è¾æ›¸
predictions = {}
daily_results = []

logger.info("ğŸ”„ æ®µéšçš„äºˆæ¸¬å®Ÿè¡Œé–‹å§‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰")
logger.info(f"äºˆæ¸¬æœŸé–“: {start_date.date()} ï½ {end_date.date()}")
logger.info(f"äºˆæ¸¬å›æ•°: {16 * 24}å›ï¼ˆ16æ—¥Ã—24æ™‚é–“ï¼‰")
logger.info("âœ… dropna()ãªã— - XGBoostæ¬ æå€¤è‡ªå‹•å‡¦ç†ä½¿ç”¨")
logger.info("âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Œå…¨å‰Šé™¤")

print(f"\nğŸ”„ æ®µéšçš„äºˆæ¸¬å®Ÿè¡Œé–‹å§‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰")
print(f"äºˆæ¸¬æœŸé–“: {start_date.date()} ï½ {end_date.date()}")
print(f"äºˆæ¸¬å›æ•°: {16 * 24}å›ï¼ˆ16æ—¥Ã—24æ™‚é–“ï¼‰")

# 16æ—¥é–“ã®æ®µéšçš„äºˆæ¸¬
current_date = start_date

for day in range(16):
    daily_predictions = []
    daily_actuals = []
    
    print(f"\nğŸ“… Day {day+1}: {current_date.strftime('%Y-%m-%d')}")
    
    # 1æ—¥24æ™‚é–“ã®äºˆæ¸¬
    for hour in range(24):
        target_datetime = current_date + timedelta(hours=hour)
        
        # ç‰¹å¾´é‡æº–å‚™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰
        feature_values = prepare_features_no_fallback(target_datetime, predictions)
        
        # DataFrameã«å¤‰æ›ï¼ˆXGBoostã«å…¥åŠ›ï¼‰
        X_pred = pd.DataFrame([feature_values], columns=features)
        
        # äºˆæ¸¬å®Ÿè¡Œ
        pred_value = xgb_model.predict(X_pred)[0]
        predictions[target_datetime] = pred_value
        daily_predictions.append(pred_value)
        
        # å®Ÿç¸¾å€¤å–å¾—ï¼ˆæ¤œè¨¼ç”¨ï¼‰
        if target_datetime in ml_features.index:
            actual_value = ml_features.loc[target_datetime, 'actual_power']
            daily_actuals.append(actual_value)
    
    # æ—¥åˆ¥ç²¾åº¦è¨ˆç®—
    if len(daily_actuals) == 24:  # å®Œå…¨ãª1æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
        daily_mape = mean_absolute_percentage_error(daily_actuals, daily_predictions) * 100
        daily_mae = mean_absolute_error(daily_actuals, daily_predictions)
        daily_r2 = r2_score(daily_actuals, daily_predictions)
        
        daily_results.append({
            'day': day + 1,
            'date': current_date.strftime('%Y-%m-%d'),
            'mape': daily_mape,
            'mae': daily_mae,
            'r2': daily_r2,
            'predictions_mean': np.mean(daily_predictions),
            'actuals_mean': np.mean(daily_actuals)
        })
        
        print(f"  MAPE: {daily_mape:.2f}%, MAE: {daily_mae:.1f}ä¸‡kW, RÂ²: {daily_r2:.4f}")
    
    current_date += timedelta(days=1)

logger.info("âœ… æ®µéšçš„äºˆæ¸¬å®Œäº†")
print(f"\nâœ… æ®µéšçš„äºˆæ¸¬å®Œäº†")

# %%
# ================================================================
# 6. å…¨æœŸé–“ç²¾åº¦è¨ˆç®—ãƒ»çµæœåˆ†æ
# ================================================================

# å…¨æœŸé–“ã®äºˆæ¸¬å€¤ãƒ»å®Ÿç¸¾å€¤åé›†
all_predictions = []
all_actuals = []
prediction_datetimes = []

for dt, pred in predictions.items():
    if dt in ml_features.index:
        all_predictions.append(pred)
        all_actuals.append(ml_features.loc[dt, 'actual_power'])
        prediction_datetimes.append(dt)

# å…¨æœŸé–“ç²¾åº¦è¨ˆç®—
overall_mape = mean_absolute_percentage_error(all_actuals, all_predictions) * 100
overall_mae = mean_absolute_error(all_actuals, all_predictions)
overall_r2 = r2_score(all_actuals, all_predictions)

logger.info("ğŸ“Š æ®µéšçš„äºˆæ¸¬ å…¨æœŸé–“çµæœè¨ˆç®—å®Œäº†")
logger.info(f"äºˆæ¸¬ä»¶æ•°: {len(all_predictions)}ä»¶, MAPE: {overall_mape:.2f}%")

print(f"\nğŸ“Š æ®µéšçš„äºˆæ¸¬ å…¨æœŸé–“çµæœ")
print(f"=" * 40)
print(f"äºˆæ¸¬ä»¶æ•°: {len(all_predictions)}ä»¶")
print(f"MAPE: {overall_mape:.2f}%")
print(f"MAE:  {overall_mae:.2f}ä¸‡kW")
print(f"RÂ²:   {overall_r2:.4f}")

# %%
# ================================================================
# 7. ã€æ–°æ©Ÿèƒ½ã€‘äºˆæ¸¬çµæœCSVä¿å­˜æ©Ÿèƒ½
# ================================================================

def save_prediction_results_to_csv(predictions, daily_results, base_output_dir="data/predictions"):
    """
    äºˆæ¸¬çµæœã‚’CSVå½¢å¼ã§ä¿å­˜ï¼ˆLooker Studioç”¨ãƒ»æˆ¦ç•¥çš„ãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
    
    Args:
        predictions (dict): äºˆæ¸¬çµæœè¾æ›¸ {datetime: predicted_value}
        daily_results (list): æ—¥åˆ¥çµæœãƒªã‚¹ãƒˆ
        base_output_dir (str): ãƒ™ãƒ¼ã‚¹å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    
    Returns:
        dict: ä¿å­˜çµæœæƒ…å ±
    """
    # å®Ÿè¡Œã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç”Ÿæˆ
    now = datetime.now()
    timestamp = now.strftime('%Y%m%d_%H%M%S')
    run_date = now.strftime('%Y-%m-%d')
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ
    base_path = Path(base_output_dir)
    detailed_dir = base_path / "detailed"
    summary_dir = base_path / "daily_summary"
    metadata_dir = base_path / "metadata"
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    detailed_dir.mkdir(parents=True, exist_ok=True)
    summary_dir.mkdir(parents=True, exist_ok=True)
    metadata_dir.mkdir(parents=True, exist_ok=True)
    
    # äºˆæ¸¬çµæœã‚’DataFrameã«å¤‰æ›
    prediction_data = []
    
    for target_datetime, predicted_value in predictions.items():
        # å®Ÿç¸¾å€¤ã‚’å–å¾—ï¼ˆã‚ã‚‹å ´åˆã®ã¿ï¼‰
        actual_value = None
        error_abs = None
        error_rate = None
        
        if target_datetime in ml_features.index:
            actual_value = ml_features.loc[target_datetime, 'actual_power']
            error_abs = abs(predicted_value - actual_value)
            error_rate = error_abs / actual_value * 100
        
        prediction_data.append({
            'prediction_run_timestamp': timestamp,
            'prediction_run_date': run_date,
            'target_datetime': target_datetime.strftime('%Y-%m-%d %H:%M:%S'),
            'target_date': target_datetime.strftime('%Y-%m-%d'),
            'target_hour': target_datetime.hour,
            'target_weekday': target_datetime.weekday(),  # 0=æœˆæ›œ
            'target_is_weekend': 1 if target_datetime.weekday() >= 5 else 0,
            'predicted_power': round(predicted_value, 2),
            'actual_power': round(actual_value, 2) if actual_value is not None else None,
            'error_absolute': round(error_abs, 2) if error_abs is not None else None,
            'error_percentage': round(error_rate, 2) if error_rate is not None else None,
            'prediction_type': '16day_iterative',
            'model_version': 'phase9_xgboost_v1',
            'created_at': now.strftime('%Y-%m-%d %H:%M:%S')
        })
    
    # 1. æ™‚é–“åˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    predictions_df = pd.DataFrame(prediction_data)
    detailed_filename = f"predictions_detailed_{timestamp}.csv"
    detailed_filepath = detailed_dir / detailed_filename
    predictions_df.to_csv(detailed_filepath, index=False, encoding='utf-8')
    
    # 2. æ—¥åˆ¥ã‚µãƒãƒªãƒ¼ä¿å­˜
    summary_filepath = None
    if daily_results:
        daily_df = pd.DataFrame(daily_results)
        daily_df['prediction_run_timestamp'] = timestamp
        daily_df['prediction_run_date'] = run_date
        daily_df['created_at'] = now.strftime('%Y-%m-%d %H:%M:%S')
        
        summary_filename = f"predictions_summary_{timestamp}.csv"
        summary_filepath = summary_dir / summary_filename
        daily_df.to_csv(summary_filepath, index=False, encoding='utf-8')
    
    # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata = {
        'execution_info': {
            'timestamp': timestamp,
            'run_date': run_date,
            'prediction_count': len(predictions),
            'date_range_start': min(predictions.keys()).strftime('%Y-%m-%d'),
            'date_range_end': max(predictions.keys()).strftime('%Y-%m-%d'),
            'overall_mape': round(overall_mape, 2)
        },
        'file_paths': {
            'detailed_csv': str(detailed_filepath),
            'summary_csv': str(summary_filepath) if summary_filepath else None
        },
        'model_info': {
            'model_version': 'phase9_xgboost_v1',
            'prediction_type': '16day_iterative',
            'features_used': len(features)
        }
    }
    
    metadata_filename = f"metadata_{timestamp}.json"
    metadata_filepath = metadata_dir / metadata_filename
    
    import json
    with open(metadata_filepath, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜çµæœè¿”å´
    result = {
        'success': True,
        'timestamp': timestamp,
        'run_date': run_date,
        'files': {
            'detailed_csv': str(detailed_filepath),
            'summary_csv': str(summary_filepath) if summary_filepath else None,
            'metadata_json': str(metadata_filepath)
        },
        'prediction_count': len(predictions),
        'date_range': f"{min(predictions.keys()).date()} to {max(predictions.keys()).date()}",
        'overall_mape': round(overall_mape, 2)
    }
    
    return result

# äºˆæ¸¬çµæœä¿å­˜å®Ÿè¡Œ
logger.info("ğŸ’¾ äºˆæ¸¬çµæœCSVä¿å­˜é–‹å§‹ï¼ˆæˆ¦ç•¥çš„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ»æ§‹é€ åŒ–ï¼‰")
print(f"\nğŸ’¾ äºˆæ¸¬çµæœCSVä¿å­˜é–‹å§‹ï¼ˆæˆ¦ç•¥çš„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ»æ§‹é€ åŒ–ï¼‰")
print("=" * 50)

save_result = save_prediction_results_to_csv(predictions, daily_results)

if save_result['success']:
    logger.info("âœ… äºˆæ¸¬çµæœä¿å­˜å®Œäº†")
    logger.info(f"å®Ÿè¡Œã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {save_result['timestamp']}")
    logger.info(f"äºˆæ¸¬ä»¶æ•°: {save_result['prediction_count']}ä»¶, ç²¾åº¦: MAPE {save_result['overall_mape']}%")
    logger.info(f"è©³ç´°ãƒ‡ãƒ¼ã‚¿: {save_result['files']['detailed_csv']}")
    
    print(f"âœ… äºˆæ¸¬çµæœä¿å­˜å®Œäº†")
    print(f"â° å®Ÿè¡Œã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {save_result['timestamp']}")
    print(f"")
    print(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"  ğŸ“Š è©³ç´°ãƒ‡ãƒ¼ã‚¿: {save_result['files']['detailed_csv']}")
    print(f"  ğŸ“ˆ æ—¥åˆ¥ã‚µãƒãƒªãƒ¼: {save_result['files']['summary_csv']}")
    print(f"  ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {save_result['files']['metadata_json']}")
    print(f"")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
    print(f"  äºˆæ¸¬ä»¶æ•°: {save_result['prediction_count']}ä»¶")
    print(f"  å¯¾è±¡æœŸé–“: {save_result['date_range']}")
    print(f"  ç²¾åº¦: MAPE {save_result['overall_mape']}%")
else:
    logger.error("âŒ äºˆæ¸¬çµæœä¿å­˜å¤±æ•—")
    print(f"âŒ ä¿å­˜å¤±æ•—")

# %%
# ================================================================
# 8. å¤–ã‚Œå€¤æ¤œå‡ºãƒ»åˆ†æï¼ˆIQRæ³•ï¼‰
# ================================================================

print(f"\nğŸ“Š å¤–ã‚Œå€¤æ¤œå‡ºãƒ»åˆ†æ")
print(f"=" * 40)

# æ®‹å·®è¨ˆç®—
residuals = np.array(all_actuals) - np.array(all_predictions)
abs_residuals = np.abs(residuals)

# IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
Q1 = np.percentile(abs_residuals, 25)
Q3 = np.percentile(abs_residuals, 75)
IQR = Q3 - Q1
outlier_threshold = Q3 + 1.5 * IQR

# å¤–ã‚Œå€¤ç‰¹å®š
outliers = abs_residuals > outlier_threshold
outliers_count = np.sum(outliers)

print(f"æ®‹å·®çµ±è¨ˆ:")
print(f"  Q1: {Q1:.2f}ä¸‡kW")
print(f"  Q3: {Q3:.2f}ä¸‡kW") 
print(f"  IQR: {IQR:.2f}ä¸‡kW")
print(f"  å¤–ã‚Œå€¤é–¾å€¤: {outlier_threshold:.2f}ä¸‡kW")
print(f"  å¤–ã‚Œå€¤: {outliers_count}ä»¶ ({outliers_count/len(abs_residuals)*100:.1f}%)")

# %%
# ================================================================
# 9. å®Ÿé¨“ç·æ‹¬ãƒ»Phase 11æº–å‚™å®Œäº†ç¢ºèª
# ================================================================

print(f"\nğŸ‰ æ®µéšçš„äºˆæ¸¬å®Ÿé¨“ãƒ»Phase 11æº–å‚™å®Œäº†")
print("="*60)

print(f"ã€å®Ÿé¨“æˆæœã€‘")
print(f"âœ… æ®µéšçš„äºˆæ¸¬ç²¾åº¦: MAPE {overall_mape:.2f}%")
print(f"âœ… å‰å›å›ºå®šäºˆæ¸¬ã‹ã‚‰ã®åŠ£åŒ–: +{overall_mape - 2.54:.2f}%")
print(f"âœ… å¤–ã‚Œå€¤: {outliers_count}ä»¶ ({outliers_count/len(abs_residuals)*100:.1f}%)")
print(f"âœ… åœŸæ—¥ç¥æ—¥å¯¾å¿œ: dropna()ãªã—ã§ã‚‚å®‰å®šé‹ç”¨")

print(f"\nã€Phase 11ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æº–å‚™å®Œäº†ã€‘")
print(f"ğŸ“Š è©³ç´°ãƒ‡ãƒ¼ã‚¿CSV: {save_result['files']['detailed_csv']}")
print(f"ğŸ“ˆ æ—¥åˆ¥ã‚µãƒãƒªãƒ¼CSV: {save_result['files']['summary_csv']}")
print(f"ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSON: {save_result['files']['metadata_json']}")
print(f"ğŸ”— æ¬¡ã‚¹ãƒ†ãƒƒãƒ—: GCSUploader â†’ BigQuery â†’ Looker Studio")

print(f"\nã€Phase 10å®Œäº†åˆ¤æ–­ã€‘")
if overall_mape <= 3.5:
    print(f"âœ… MAPE {overall_mape:.2f}% - å®Ÿç”¨ãƒ¬ãƒ™ãƒ«é”æˆ")
    print(f"âœ… Phase 10æ—¥æ¬¡è‡ªå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰OK")
    print(f"âœ… Phase 11 Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç§»è¡ŒOK")
else:
    print(f"âš ï¸ MAPE {overall_mape:.2f}% - ç²¾åº¦å‘ä¸Šç­–è¦æ¤œè¨")
    print(f"âš ï¸ Phase 10ã‚·ã‚¹ãƒ†ãƒ åŒ–å‰ã«è¿½åŠ æ”¹è‰¯æ¨å¥¨")

logger.info("ğŸ‰ Phase 10æ®µéšçš„äºˆæ¸¬å®Ÿé¨“ãƒ»ä¿å­˜æ©Ÿèƒ½ä»˜ããƒãƒ¼ã‚¸ãƒ§ãƒ³å®Œäº†ï¼")
logger.info("âœ¨ Phase 11 Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰æº–å‚™å®Œäº†ï¼")

print(f"\nğŸ‰ Phase 10æ®µéšçš„äºˆæ¸¬å®Ÿé¨“ãƒ»ä¿å­˜æ©Ÿèƒ½ä»˜ããƒãƒ¼ã‚¸ãƒ§ãƒ³å®Œäº†ï¼")
print(f"âœ¨ Phase 11 Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰æº–å‚™å®Œäº†ï¼")

# %%
# ================================================================
# 10. ä¿å­˜ã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
# ================================================================

print(f"\nğŸ“‹ ä¿å­˜ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª")
print("=" * 50)

# predictions CSVã®å†…å®¹ç¢ºèª
if save_result['files']['detailed_csv'] and os.path.exists(save_result['files']['detailed_csv']):
    saved_df = pd.read_csv(save_result['files']['detailed_csv'])
    print(f"âœ… è©³ç´°äºˆæ¸¬çµæœCSVèª­ã¿è¾¼ã¿å®Œäº†")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {saved_df.shape}")
    print(f"ğŸ“… æœŸé–“: {saved_df['target_date'].min()} ï½ {saved_df['target_date'].max()}")
    print(f"ğŸ• æ™‚é–“ç¯„å›²: {saved_df['target_hour'].min()}æ™‚ ï½ {saved_df['target_hour'].max()}æ™‚")
    
    print(f"\næœ€åˆã®3è¡Œ:")
    print(saved_df.head(3).to_string(index=False))
    
    print(f"\nåˆ—æƒ…å ±:")
    for i, col in enumerate(saved_df.columns):
        print(f"  {i+1:2d}. {col}")

# daily summary CSVã®å†…å®¹ç¢ºèª  
if save_result['files']['summary_csv'] and os.path.exists(save_result['files']['summary_csv']):
    daily_saved_df = pd.read_csv(save_result['files']['summary_csv'])
    print(f"\nâœ… æ—¥åˆ¥ã‚µãƒãƒªãƒ¼CSVèª­ã¿è¾¼ã¿å®Œäº†")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {daily_saved_df.shape}")
    print(f"ğŸ“ˆ MAPEç¯„å›²: {daily_saved_df['mape'].min():.2f}% ï½ {daily_saved_df['mape'].max():.2f}%")

print(f"\nğŸ”— æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
print(f"1. GCSUploaderã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
print(f"2. BigQueryã«EXTERNAL TABLEä½œæˆã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿æŠ•å…¥")
print(f"3. Looker Studioã§BigQueryãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¥ç¶š")
print(f"4. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰ãƒ»å…¬é–‹è¨­å®š")