"""
ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æŠ•å…¥çµ±åˆå‡¦ç†

æ—¥æ¬¡è‡ªå‹•å®Ÿè¡Œç”¨ï¼šé›»åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆéå»5æ—¥åˆ†ï¼‰+ æ°—è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆéå»10æ—¥+äºˆæ¸¬16æ—¥ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€
GCSãƒ»BigQueryã«æŠ•å…¥ã™ã‚‹ã€‚

å®Ÿè¡Œæ–¹æ³•:
    python -m src.pipelines.main_etl

Note:
    å€‹åˆ¥ã®æŸ”è»Ÿãªå®Ÿè¡Œï¼ˆç‰¹å®šæœˆãƒ»ç‰¹å®šæ—¥ãƒ»ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã¯å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç›´æ¥å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
    - é›»åŠ›ãƒ‡ãƒ¼ã‚¿: src.data_processing.data_downloader
    - æ°—è±¡ãƒ‡ãƒ¼ã‚¿: src.data_processing.weather_downloader
    - BQæŠ•å…¥: src.data_processing.power_bigquery_loader, weather_bigquery_loader
"""

import os
from pathlib import Path
from logging import getLogger

from src.data_processing.data_downloader import PowerDataDownloader
from src.data_processing.gcs_uploader import GCSUploader
from src.data_processing.weather_downloader import WeatherDownloader
from src.data_processing.weather_bigquery_loader import WeatherBigQueryLoader
from src.data_processing.power_bigquery_loader import PowerBigQueryLoader
from src.utils.logging_config import setup_logging

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å°‚ç”¨ã®ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—
logger = getLogger('energy_env.main_etl')

class MainETLPipeline:
    """ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - Extract + Loadçµ±åˆå‡¦ç†"""
    
    def __init__(self, base_dir=None, bucket_name="energy-env-data", project_id="energy-env"):
        """
        åˆæœŸåŒ–

        Args:
            base_dir (str): ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆï¼ˆé›»åŠ›ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
                          Noneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ENERGY_ENV_PATHã‹ã‚‰å–å¾—
            bucket_name (str): GCSãƒã‚±ãƒƒãƒˆå
            project_id (str): GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        """
        if base_dir is None:
            energy_env_path = os.getenv('ENERGY_ENV_PATH')
            if energy_env_path is None:
                raise ValueError("ENERGY_ENV_PATH environment variable is not set")
            base_dir = os.path.join(energy_env_path, 'data', 'raw')

        self.base_dir = Path(base_dir)
        self.bucket_name = bucket_name
        self.project_id = project_id

        # é›»åŠ›ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.power_downloader = PowerDataDownloader(str(self.base_dir))
        self.uploader = GCSUploader(bucket_name)
        self.power_bq_loader = PowerBigQueryLoader(project_id, str(self.base_dir))

        # æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.weather_downloader = WeatherDownloader()
        self.weather_bq_loader = WeatherBigQueryLoader(project_id)

        logger.info(f"MainETLPipeline initialized: {base_dir} â†’ gs://{bucket_name}")
    
    def run_etl_for_days(self, days=5):
        """
        æ—¥æ•°æŒ‡å®šã§ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œï¼ˆé›»åŠ›+æ°—è±¡ãƒ‡ãƒ¼ã‚¿çµ±åˆç‰ˆï¼‰

        Args:
            days (int): ä»Šæ—¥ã‹ã‚‰é¡ã‚‹æ—¥æ•°

        Returns:
            dict: å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼
        """
        logger.info(f"Starting ETL pipeline for {days} days")

        # Phase 1: Extract (ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)
        logger.info("Phase 1: Extracting data (Power + Weather)")

        # 1-1. é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        logger.info("Phase 1-1: Downloading power data from TEPCO website")
        power_download_results = self.power_downloader.download_for_days(days)

        # 1-2. æ°—è±¡ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        logger.info("Phase 1-2: Downloading weather data from Open-Meteo API")
        try:
            weather_download_results = self.weather_downloader.download_daily_weather_data()
            weather_download_success = True
        except Exception as e:
            logger.error(f"Weather data download failed: {e}")
            weather_download_results = None
            weather_download_success = False

        # Phase 2: Load to GCS (é›»åŠ›ãƒ‡ãƒ¼ã‚¿ã®ã¿)
        logger.info("Phase 2: Loading power data to Google Cloud Storage")
        if power_download_results['success']:
            upload_results = self._upload_downloaded_data(power_download_results['success'])
        else:
            logger.warning("No power data downloaded. Skipping GCS upload.")
            upload_results = {'success': [], 'failed': []}

        # Phase 3: Load to BigQuery
        logger.info("Phase 3: Loading data to BigQuery (Power + Weather)")

        # 3-1. é›»åŠ›ãƒ‡ãƒ¼ã‚¿BQæŠ•å…¥
        logger.info("Phase 3-1: Loading power data to BigQuery")
        try:
            power_bq_results = self.power_bq_loader.load_power_data(days)
            power_bq_success = power_bq_results['status'] == 'success'
        except Exception as e:
            logger.error(f"Power data BQ load failed: {e}")
            power_bq_results = {'status': 'failed', 'message': str(e), 'files_processed': 0, 'rows_inserted': 0}
            power_bq_success = False

        # 3-2. æ°—è±¡ãƒ‡ãƒ¼ã‚¿BQæŠ•å…¥ï¼ˆhistorical + forecastï¼‰
        weather_bq_results = {'historical': None, 'forecast': None}
        weather_bq_success = False

        if weather_download_success:
            logger.info("Phase 3-2: Loading weather data to BigQuery")

            # Historical ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
            try:
                weather_bq_results['historical'] = self.weather_bq_loader.load_weather_data('historical')
                historical_success = weather_bq_results['historical']['status'] == 'success'
            except Exception as e:
                logger.error(f"Weather historical data BQ load failed: {e}")
                weather_bq_results['historical'] = {'status': 'failed', 'message': str(e), 'files_processed': 0, 'rows_inserted': 0}
                historical_success = False

            # Forecast ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
            try:
                weather_bq_results['forecast'] = self.weather_bq_loader.load_weather_data('forecast')
                forecast_success = weather_bq_results['forecast']['status'] == 'success'
            except Exception as e:
                logger.error(f"Weather forecast data BQ load failed: {e}")
                weather_bq_results['forecast'] = {'status': 'failed', 'message': str(e), 'files_processed': 0, 'rows_inserted': 0}
                forecast_success = False

            weather_bq_success = historical_success and forecast_success
        else:
            logger.warning("Weather data download failed. Skipping weather BQ load.")

        # çµæœã‚µãƒãƒªãƒ¼ä½œæˆ
        summary = self._create_summary_integrated(
            power_download_results,
            upload_results,
            power_bq_results,
            weather_download_success,
            weather_bq_success
        )
        logger.info(f"ETL pipeline completed: {summary}")

        return {
            'power_download_results': power_download_results,
            'weather_download_results': weather_download_results,
            'upload_results': upload_results,
            'power_bq_results': power_bq_results,
            'weather_bq_results': weather_bq_results,
            'summary': summary
        }
    
    def _upload_downloaded_data(self, successful_months):
        """
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        
        Args:
            successful_months (list): ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸã—ãŸæœˆã®ãƒªã‚¹ãƒˆ
            
        Returns:
            dict: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
        """
        upload_results = {'success': [], 'failed': []}
        
        for month in successful_months:
            try:
                month_dir = self.base_dir / month
                
                # CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
                csv_files = list(month_dir.glob("*.csv"))
                
                if not csv_files:
                    logger.warning(f"No CSV files found in {month_dir}")
                    upload_results['failed'].append({
                        'month': month,
                        'error': 'No CSV files found'
                    })
                    continue
                
                # å„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                month_success = True
                for csv_file in csv_files:
                    try:
                        gcs_file_name = f"raw_data/{month}/{csv_file.name}"
                        uri = self.uploader.upload_file(str(csv_file), gcs_file_name)
                        logger.info(f"Successfully uploaded {csv_file.name} to {uri}")
                    except Exception as e:
                        logger.error(f"Failed to upload {csv_file}: {e}")
                        month_success = False
                
                if month_success:
                    upload_results['success'].append(month)
                else:
                    upload_results['failed'].append({
                        'month': month,
                        'error': 'One or more CSV files failed to upload'
                    })
                    
            except Exception as e:
                logger.error(f"Failed to process month {month}: {e}")
                upload_results['failed'].append({
                    'month': month,
                    'error': str(e)
                })
        
        return upload_results
    
    def _create_summary_integrated(self, power_download_results, upload_results,
                                    power_bq_results, weather_download_success, weather_bq_success):
        """
        å®Ÿè¡Œçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆï¼ˆé›»åŠ›+æ°—è±¡ãƒ‡ãƒ¼ã‚¿çµ±åˆç‰ˆï¼‰

        Args:
            power_download_results (dict): é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
            upload_results (dict): GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
            power_bq_results (dict): é›»åŠ›ãƒ‡ãƒ¼ã‚¿BQæŠ•å…¥çµæœ
            weather_download_success (bool): æ°—è±¡ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸãƒ•ãƒ©ã‚°
            weather_bq_success (bool): æ°—è±¡ãƒ‡ãƒ¼ã‚¿BQæŠ•å…¥æˆåŠŸãƒ•ãƒ©ã‚°

        Returns:
            str: ã‚µãƒãƒªãƒ¼æ–‡å­—åˆ—
        """
        power_download_success = len(power_download_results['success']) > 0
        power_bq_success = power_bq_results['status'] == 'success'
        upload_success = len(upload_results['success']) > 0

        success_components = []
        failure_components = []

        # é›»åŠ›ãƒ‡ãƒ¼ã‚¿
        if power_download_success and upload_success and power_bq_success:
            success_components.append("é›»åŠ›ãƒ‡ãƒ¼ã‚¿å®Œäº†")
        else:
            if not power_download_success:
                failure_components.append("é›»åŠ›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
            if not upload_success:
                failure_components.append("GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
            if not power_bq_success:
                failure_components.append("é›»åŠ›BQæŠ•å…¥å¤±æ•—")

        # æ°—è±¡ãƒ‡ãƒ¼ã‚¿
        if weather_download_success and weather_bq_success:
            success_components.append("æ°—è±¡ãƒ‡ãƒ¼ã‚¿å®Œäº†")
        else:
            if not weather_download_success:
                failure_components.append("æ°—è±¡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
            if weather_download_success and not weather_bq_success:
                failure_components.append("æ°—è±¡BQæŠ•å…¥å¤±æ•—")

        # ã‚µãƒãƒªãƒ¼æ–‡å­—åˆ—ç”Ÿæˆ
        if not failure_components:
            return f"å®Œå…¨æˆåŠŸ: {', '.join(success_components)}"
        elif success_components:
            return f"éƒ¨åˆ†æˆåŠŸ: {', '.join(success_components)} / {', '.join(failure_components)}"
        else:
            return f"å¤±æ•—: {', '.join(failure_components)}"


def print_results(results):
    """å®Ÿè¡Œçµæœã‚’è¡¨ç¤ºï¼ˆé›»åŠ›+æ°—è±¡ãƒ‡ãƒ¼ã‚¿çµ±åˆç‰ˆï¼‰"""
    print(f"\n{'='*60}")
    print("ğŸ“Š ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œçµæœï¼ˆé›»åŠ›+æ°—è±¡ãƒ‡ãƒ¼ã‚¿ï¼‰")
    print('='*60)

    # é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
    if 'power_download_results' in results:
        power_download_results = results['power_download_results']
        print("\nğŸ“¥ é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ:")
        if power_download_results['success']:
            print(f"  âœ… æˆåŠŸ: {', '.join(power_download_results['success'])}")
        if power_download_results['failed']:
            print(f"  âŒ å¤±æ•—: {', '.join(power_download_results['failed'])}")

    # æ°—è±¡ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
    if 'weather_download_results' in results and results['weather_download_results']:
        print("\nğŸŒ¤ï¸ æ°—è±¡ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ:")
        weather_results = results['weather_download_results']
        if 'historical' in weather_results and weather_results['historical']:
            print(f"  âœ… éå»ãƒ‡ãƒ¼ã‚¿: {len(weather_results['historical'])}ä»¶")
        if 'forecast' in weather_results and weather_results['forecast']:
            print(f"  âœ… äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿: {len(weather_results['forecast'])}ä»¶")

    # GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
    if 'upload_results' in results:
        upload_results = results['upload_results']
        print("\nğŸ“¤ GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœï¼ˆé›»åŠ›ãƒ‡ãƒ¼ã‚¿ï¼‰:")
        if upload_results['success']:
            print(f"  âœ… æˆåŠŸ: {', '.join(upload_results['success'])}")
        if upload_results['failed']:
            print("  âŒ å¤±æ•—:")
            for item in upload_results['failed']:
                if isinstance(item, dict):
                    print(f"    {item['month']}: {item['error']}")
                else:
                    print(f"    {item}")

    # BigQueryæŠ•å…¥çµæœï¼ˆé›»åŠ›ï¼‰
    if 'power_bq_results' in results and results['power_bq_results']:
        power_bq = results['power_bq_results']
        print("\nğŸ’¾ BigQueryæŠ•å…¥çµæœï¼ˆé›»åŠ›ãƒ‡ãƒ¼ã‚¿ï¼‰:")
        status_mark = 'âœ…' if power_bq['status'] == 'success' else 'âŒ'
        print(f"  {status_mark} ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {power_bq['status']}")
        print(f"  ğŸ“ å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {power_bq['files_processed']}")
        print(f"  ğŸ“Š æŠ•å…¥ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {power_bq['rows_inserted']}")

    # BigQueryæŠ•å…¥çµæœï¼ˆæ°—è±¡ï¼‰
    if 'weather_bq_results' in results and results['weather_bq_results']:
        weather_bq = results['weather_bq_results']
        print("\nğŸ’¾ BigQueryæŠ•å…¥çµæœï¼ˆæ°—è±¡ãƒ‡ãƒ¼ã‚¿ï¼‰:")

        if weather_bq.get('historical'):
            hist = weather_bq['historical']
            status_mark = 'âœ…' if hist['status'] == 'success' else 'âŒ'
            print(f"  {status_mark} éå»ãƒ‡ãƒ¼ã‚¿: {hist['rows_inserted']}è¡ŒæŠ•å…¥")

        if weather_bq.get('forecast'):
            fore = weather_bq['forecast']
            status_mark = 'âœ…' if fore['status'] == 'success' else 'âŒ'
            print(f"  {status_mark} äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿: {fore['rows_inserted']}è¡ŒæŠ•å…¥")

    # ã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“ˆ ç·åˆçµæœ: {results['summary']}")
    print('='*60)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - æ—¥æ¬¡ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    # ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–
    setup_logging()

    print("ğŸš€ ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹ï¼ˆé›»åŠ›+æ°—è±¡ãƒ‡ãƒ¼ã‚¿çµ±åˆç‰ˆï¼‰")
    print("ğŸ“Š å‡¦ç†å†…å®¹: é›»åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆéå»5æ—¥åˆ†ï¼‰+ æ°—è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆéå»10æ—¥+äºˆæ¸¬16æ—¥ï¼‰")

    # ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    try:
        pipeline = MainETLPipeline()
    except ValueError as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print("   ENERGY_ENV_PATHç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return

    print(f"ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆ: {pipeline.base_dir}")
    print(f"â˜ï¸  GCSãƒã‚±ãƒƒãƒˆ: gs://{pipeline.bucket_name}")
    print(f"ğŸ”§ GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {pipeline.project_id}")

    # æ—¥æ¬¡ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    try:
        results = pipeline.run_etl_for_days(days=5)

        # çµæœè¡¨ç¤º
        print_results(results)

    except Exception as e:
        logger.error(f"ETL pipeline failed: {e}")
        print(f"ğŸ’¥ ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return

    print("ğŸ ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")


if __name__ == "__main__":
    main()