"""
ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰â†’GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµ±åˆå‡¦ç†

å®Ÿè¡Œæ–¹æ³•:
    python -m src.main_etl                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: éå»5æ—¥åˆ†
    python -m src.main_etl --days 7          # éå»7æ—¥åˆ†
    python -m src.main_etl --month 202505    # æŒ‡å®šæœˆ
    python -m src.main_etl --date 20250501   # ç‰¹å®šæ—¥
    python -m src.main_etl --bucket my-bucket # ã‚«ã‚¹ã‚¿ãƒ ãƒã‚±ãƒƒãƒˆ
"""

import argparse
import calendar
from datetime import datetime, timedelta
from pathlib import Path
from logging import getLogger

from src.data_processing.data_downloader import PowerDataDownloader
from src.data_processing.gcs_uploader import GCSUploader
from src.utils.logging_config import setup_logging

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å°‚ç”¨ã®ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—
logger = getLogger('energy_env.main_etl')

class MainETLPipeline:
    """ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - Extract + Loadçµ±åˆå‡¦ç†"""
    
    def __init__(self, base_dir="data/raw", bucket_name="energy-env-data"):
        """
        åˆæœŸåŒ–
        
        Args:
            base_dir (str): ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ
            bucket_name (str): GCSãƒã‚±ãƒƒãƒˆå
        """
        self.base_dir = Path(base_dir)
        self.bucket_name = bucket_name
        
        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self.downloader = PowerDataDownloader(str(self.base_dir))
        self.uploader = GCSUploader(bucket_name)
        
        logger.info(f"MainETLPipeline initialized: {base_dir} â†’ gs://{bucket_name}")
    
    def run_etl_for_days(self, days=5):
        """
        æ—¥æ•°æŒ‡å®šã§ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        
        Args:
            days (int): ä»Šæ—¥ã‹ã‚‰é¡ã‚‹æ—¥æ•°
            
        Returns:
            dict: å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼
        """
        logger.info(f"Starting ETL pipeline for {days} days")
        
        # Phase 1: Extract (ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)
        logger.info("Phase 1: Extracting data from TEPCO website")
        download_results = self.downloader.download_for_days(days)
        
        if not download_results['success']:
            logger.warning("No data downloaded successfully. Skipping upload phase.")
            return {
                'download_results': download_results,
                'upload_results': {'success': [], 'failed': []},
                'overall_status': 'failed',
                'message': 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãŸã‚ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ'
            }
        
        # Phase 2: Load (GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰)
        logger.info("Phase 2: Loading data to Google Cloud Storage")
        upload_results = self._upload_downloaded_data(download_results['success'])
        
        # çµæœã‚µãƒãƒªãƒ¼ä½œæˆ
        overall_status = 'success' if download_results['success'] and upload_results['success'] else 'partial'
        
        return {
            'download_results': download_results,
            'upload_results': upload_results,
            'overall_status': overall_status,
            'message': self._create_summary_message(download_results, upload_results)
        }
    
    def run_etl_for_month(self, yyyymm):
        """
        æœˆæŒ‡å®šã§ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        
        Args:
            yyyymm (str): å¹´æœˆ (YYYYMMå½¢å¼)
            
        Returns:
            dict: å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼
        """
        logger.info(f"Starting ETL pipeline for month {yyyymm}")
        
        # Phase 1: Extract
        logger.info("Phase 1: Extracting data from TEPCO website")
        download_results = self.downloader.download_for_month(yyyymm)
        
        if not download_results['success']:
            logger.warning(f"No data downloaded for month {yyyymm}. Skipping upload phase.")
            return {
                'download_results': download_results,
                'upload_results': {'success': [], 'failed': []},
                'overall_status': 'failed',
                'message': f'æœˆ{yyyymm}ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãŸã‚ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ'
            }
        
        # Phase 2: Load
        logger.info("Phase 2: Loading data to Google Cloud Storage")
        upload_results = self._upload_downloaded_data(download_results['success'])
        
        # çµæœã‚µãƒãƒªãƒ¼ä½œæˆ
        overall_status = 'success' if download_results['success'] and upload_results['success'] else 'partial'
        
        return {
            'download_results': download_results,
            'upload_results': upload_results,
            'overall_status': overall_status,
            'message': self._create_summary_message(download_results, upload_results)
        }
    
    def run_etl_for_date(self, date_str):
        """
        æ—¥ä»˜æŒ‡å®šã§ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        
        Args:
            date_str (str): æ—¥ä»˜æ–‡å­—åˆ— (YYYYMMDDå½¢å¼)
            
        Returns:
            dict: å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼
        """
        logger.info(f"Starting ETL pipeline for date {date_str}")
        
        # Phase 1: Extract
        logger.info("Phase 1: Extracting data from TEPCO website")
        download_results = self.downloader.download_for_date(date_str)
        
        if not download_results['success']:
            logger.warning(f"No data downloaded for date {date_str}. Skipping upload phase.")
            return {
                'download_results': download_results,
                'upload_results': {'success': [], 'failed': []},
                'overall_status': 'failed',
                'message': f'æ—¥ä»˜{date_str}ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãŸã‚ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ'
            }
        
        # Phase 2: Load
        logger.info("Phase 2: Loading data to Google Cloud Storage")
        upload_results = self._upload_downloaded_data(download_results['success'])
        
        # çµæœã‚µãƒãƒªãƒ¼ä½œæˆ
        overall_status = 'success' if download_results['success'] and upload_results['success'] else 'partial'
        
        return {
            'download_results': download_results,
            'upload_results': upload_results,
            'overall_status': overall_status,
            'message': self._create_summary_message(download_results, upload_results)
        }
    
    def _upload_downloaded_data(self, successful_months):
        """
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        
        Args:
            successful_months (list): ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸã—ãŸæœˆã®ãƒªã‚¹ãƒˆ
            
        Returns:
            dict: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
        """
        upload_results = {'success': [], 'failed': []}
        
        for month in successful_months:
            month_dir = self.base_dir / month
            
            if not month_dir.exists():
                logger.warning(f"Directory not found: {month_dir}")
                upload_results['failed'].append(month)
                continue
            
            try:
                # æœˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®CSVã¨ZIPã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                destination_prefix = f"raw_data/{month}"
                
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                csv_uris = self.uploader.upload_directory(
                    str(month_dir), 
                    destination_prefix,
                    file_extension=".csv"
                )
                
                # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ—¥ä»˜ä»˜ãã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                today_str = datetime.today().strftime('%Y-%m-%d')
                zip_uris = self.uploader.upload_directory(
                    str(month_dir),
                    f"archives/{month}/{today_str}",  # æ—¥ä»˜ä»˜ããƒ‘ã‚¹
                    file_extension=".zip"
                )
                
                uploaded_uris = csv_uris + zip_uris
                
                # å¤ã„ZIPãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                self._cleanup_old_zip_versions()
                
                logger.info(f"Uploaded {len(uploaded_uris)} files for month {month}")
                upload_results['success'].append(month)
                
            except Exception as e:
                logger.error(f"Failed to upload data for month {month}: {e}")
                upload_results['failed'].append(month)
        
        return upload_results
    
    def _cleanup_old_zip_versions(self):
        """
        å¤ã„ZIPãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        ä»Šæœˆã¨å…ˆæœˆã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰2é€±é–“ã‚ˆã‚Šå¤ã„ï¼ˆæœˆæœ«é™¤ãï¼‰ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        """
        try:
            execution_date = datetime.today()
            cutoff_date = execution_date - timedelta(days=14)
            
            # ä»Šæœˆã¨å…ˆæœˆã‚’ãƒã‚§ãƒƒã‚¯
            current_month = execution_date.strftime('%Y%m')
            previous_month = (execution_date.replace(day=1) - timedelta(days=1)).strftime('%Y%m')
            
            months_to_check = {current_month, previous_month}
            
            logger.info(f"Starting ZIP cleanup for months: {sorted(months_to_check)}")
            
            total_deleted = 0
            total_kept = 0
            
            for month in months_to_check:
                # GCSä¸Šã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
                archive_prefix = f"archives/{month}/"
                blobs = list(self.uploader.client.list_blobs(
                    self.uploader.bucket, 
                    prefix=archive_prefix
                ))
                
                if not blobs:
                    continue
                    
                deleted_count = 0
                kept_count = 0
                
                for blob in blobs:
                    if not blob.name.endswith('.zip'):
                        continue
                        
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º: archives/202506/2025-06-01/202506.zip
                    path_parts = blob.name.split('/')
                    if len(path_parts) < 3:
                        continue
                        
                    date_str = path_parts[2]  # "2025-06-01"
                    
                    try:
                        file_date = datetime.strptime(date_str, '%Y-%m-%d')
                    except ValueError:
                        continue  # æ—¥ä»˜å½¢å¼ã§ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    
                    # å‰Šé™¤åˆ¤å®šã¨å®Ÿè¡Œ
                    # åŸºæº–æ—¥ã‚ˆã‚Šå¤ãã€ã‹ã¤æœˆæœ«æ—¥ã§ãªã„å ´åˆã¯å‰Šé™¤
                    last_day_of_month = calendar.monthrange(file_date.year, file_date.month)[1]
                    if (file_date < cutoff_date and file_date.day != last_day_of_month):
                        blob.delete()
                        logger.info(f"Deleted old ZIP: {blob.name}")
                        deleted_count += 1
                    else:
                        kept_count += 1
                
                if deleted_count > 0:
                    logger.info(f"Month {month}: deleted {deleted_count} old ZIPs, kept {kept_count} ZIPs")
                
                total_deleted += deleted_count
                total_kept += kept_count
            
            if total_deleted > 0:
                logger.info(f"ZIP cleanup completed: deleted {total_deleted} files, kept {total_kept} files")
                
        except Exception as e:
            logger.warning(f"ZIP cleanup failed: {e}")
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—ã¯å…¨ä½“ã®å‡¦ç†ã‚’æ­¢ã‚ãªã„
    
    def _create_summary_message(self, download_results, upload_results):
        """
        å®Ÿè¡Œçµæœã®ã‚µãƒãƒªãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
        
        Args:
            download_results (dict): ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
            upload_results (dict): ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
            
        Returns:
            str: ã‚µãƒãƒªãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        dl_success = len(download_results['success'])
        dl_failed = len(download_results['failed'])
        up_success = len(upload_results['success'])
        up_failed = len(upload_results['failed'])
        
        if dl_success == 0:
            return "ã™ã¹ã¦ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ"
        elif up_success == dl_success:
            return f"ETLå®Œå…¨æˆåŠŸ: {dl_success}æœˆåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¾ã—ãŸ"
        elif up_success > 0:
            return f"ETLéƒ¨åˆ†æˆåŠŸ: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰{dl_success}æœˆ, ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰{up_success}æœˆ"
        else:
            return f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ({dl_success}æœˆ)ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—"


def print_etl_results(results):
    """ETLå®Ÿè¡Œçµæœã‚’è¡¨ç¤º"""
    print(f"\n{'='*60}")
    print("ğŸ“Š ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œçµæœ")
    print('='*60)
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
    print("\nğŸ”½ Extract (ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)")
    dl_results = results['download_results']
    if dl_results['success']:
        print(f"âœ… æˆåŠŸ: {', '.join(dl_results['success'])}")
    if dl_results['failed']:
        print(f"âŒ å¤±æ•—: {', '.join(dl_results['failed'])}")
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
    print("\nğŸ”¼ Load (ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰)")
    up_results = results['upload_results']
    if up_results['success']:
        print(f"âœ… æˆåŠŸ: {', '.join(up_results['success'])}")
    if up_results['failed']:
        print(f"âŒ å¤±æ•—: {', '.join(up_results['failed'])}")
    
    # ç·åˆçµæœ
    status_emoji = {
        'success': 'ğŸ‰',
        'partial': 'âš ï¸',
        'failed': 'ğŸ’¥'
    }
    
    print(f"\nğŸ“‹ ç·åˆçµæœ")
    print(f"{status_emoji[results['overall_status']]} {results['message']}")
    print('='*60)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    # ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–
    setup_logging()
    
    parser = argparse.ArgumentParser(description='ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰â†’GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰')
    parser.add_argument('--days', type=int, default=5,
                       help='ä»Šæ—¥ã‹ã‚‰é¡ã‚‹æ—¥æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)')
    parser.add_argument('--month', type=str,
                       help='æŒ‡å®šæœˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (YYYYMMå½¢å¼)')
    parser.add_argument('--date', type=str,
                       help='ç‰¹å®šæ—¥ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (YYYYMMDDå½¢å¼)')
    parser.add_argument('--base-dir', type=str, default='data/raw',
                       help='ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/raw)')
    parser.add_argument('--bucket', type=str, default='energy-env-data',
                       help='GCSãƒã‚±ãƒƒãƒˆå (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: energy-env-data)')
    
    args = parser.parse_args()
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ¤å®šã¨æ’ä»–ãƒã‚§ãƒƒã‚¯ã‚’ä¸€æ‹¬å‡¦ç†
    if args.month and args.date:
        print("âŒ ã‚¨ãƒ©ãƒ¼: --month ã¨ --date ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
        return
    elif args.month and (args.days != 5):
        print("âŒ ã‚¨ãƒ©ãƒ¼: --month ã¨ --days ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
        return
    elif args.date and (args.days != 5):
        print("âŒ ã‚¨ãƒ©ãƒ¼: --date ã¨ --days ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
        return
    
    # ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    pipeline = MainETLPipeline(args.base_dir, args.bucket)
    
    print("ğŸš€ ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
    print(f"ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆ: {args.base_dir}")
    print(f"â˜ï¸  GCSãƒã‚±ãƒƒãƒˆ: gs://{args.bucket}")
    
    try:
        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ¤å®šã¨ETLå®Ÿè¡Œ
        if args.month:
            print(f"ğŸ“… æŒ‡å®šæœˆãƒ¢ãƒ¼ãƒ‰: {args.month}")
            results = pipeline.run_etl_for_month(args.month)
        elif args.date:
            print(f"ğŸ“… ç‰¹å®šæ—¥ãƒ¢ãƒ¼ãƒ‰: {args.date}")
            results = pipeline.run_etl_for_date(args.date)
        else:
            print(f"ğŸ“… æ—¥æ•°æŒ‡å®šãƒ¢ãƒ¼ãƒ‰: éå»{args.days}æ—¥åˆ†")
            results = pipeline.run_etl_for_days(args.days)
        
        # çµæœè¡¨ç¤º
        print_etl_results(results)
        
    except Exception as e:
        logger.error(f"ETL pipeline failed: {e}")
        print(f"ğŸ’¥ ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    print("ğŸ ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")


if __name__ == "__main__":
    main()