"""
ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰â†’GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµ±åˆå‡¦ç†

å®Ÿè¡Œæ–¹æ³•:
    python -m src.pipelines.main_etl                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: éå»5æ—¥åˆ†
    python -m src.pipelines.main_etl --days 7          # éå»7æ—¥åˆ†
    python -m src.pipelines.main_etl --month 202505    # æŒ‡å®šæœˆ
    python -m src.pipelines.main_etl --date 20250501   # ç‰¹å®šæ—¥
    python -m src.pipelines.main_etl --bucket my-bucket # ã‚«ã‚¹ã‚¿ãƒ ãƒã‚±ãƒƒãƒˆ
"""

import os
import argparse
import calendar
from datetime import datetime, timedelta
from pathlib import Path
from logging import getLogger

from src.data_processing.data_downloader import PowerDataDownloader
from src.data_processing.gcs_uploader import GCSUploader
from src.utils.logging_config import setup_logging

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å°‚ç”¨ã®ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—
logger = getLogger('energy_env.main_etl')

class MainETLPipeline:
    """ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - Extract + Loadçµ±åˆå‡¦ç†"""
    
    def __init__(self, base_dir=None, bucket_name="energy-env-data"):
        """
        åˆæœŸåŒ–
        
        Args:
            base_dir (str): ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ
                          Noneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ENERGY_ENV_PATHã‹ã‚‰å–å¾—
            bucket_name (str): GCSãƒã‚±ãƒƒãƒˆå
        """
        if base_dir is None:
            energy_env_path = os.getenv('ENERGY_ENV_PATH')
            if energy_env_path is None:
                raise ValueError("ENERGY_ENV_PATH environment variable is not set")
            base_dir = os.path.join(energy_env_path, 'data', 'raw')
        
        self.base_dir = Path(base_dir)
        self.bucket_name = bucket_name
        
        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self.downloader = PowerDataDownloader(str(self.base_dir))
        self.uploader = GCSUploader(bucket_name)
        
        logger.info(f"MainETLPipeline initialized: {base_dir} â†’ gs://{bucket_name}")
    
    def run_etl_for_days(self, days=5):
        """
        æ—¥æ•°æŒ‡å®šã§ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        
        Args:
            days (int): ä»Šæ—¥ã‹ã‚‰é¡ã‚‹æ—¥æ•°
            
        Returns:
            dict: å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼
        """
        logger.info(f"Starting ETL pipeline for {days} days")
        
        # Phase 1: Extract (ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)
        logger.info("Phase 1: Extracting data from TEPCO website")
        download_results = self.downloader.download_for_days(days)
        
        if not download_results['success']:
            logger.warning("No data downloaded successfully. Skipping upload phase.")
            return {
                'download_results': download_results,
                'upload_results': {'success': [], 'failed': []},
                'summary': 'Failed: No data downloaded'
            }
        
        # Phase 2: Load (GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰)
        logger.info("Phase 2: Loading data to Google Cloud Storage")
        upload_results = self._upload_downloaded_data(download_results['success'])
        
        # çµæœã‚µãƒãƒªãƒ¼ä½œæˆ
        summary = self._create_summary(download_results, upload_results)
        logger.info(f"ETL pipeline completed: {summary}")
        
        return {
            'download_results': download_results,
            'upload_results': upload_results,
            'summary': summary
        }
    
    def run_etl_for_month(self, yyyymm):
        """
        æœˆæŒ‡å®šã§ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        
        Args:
            yyyymm (str): å¹´æœˆ (YYYYMMå½¢å¼)
            
        Returns:
            dict: å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼
        """
        logger.info(f"Starting ETL pipeline for month {yyyymm}")
        
        # Phase 1: Extract
        logger.info("Phase 1: Extracting data from TEPCO website")
        download_results = self.downloader.download_for_month(yyyymm)
        
        if not download_results['success']:
            logger.warning(f"No data downloaded for month {yyyymm}. Skipping upload phase.")
            return {
                'download_results': download_results,
                'upload_results': {'success': [], 'failed': []},
                'summary': f'Failed: No data downloaded for {yyyymm}'
            }
        
        # Phase 2: Load
        logger.info("Phase 2: Loading data to Google Cloud Storage")
        upload_results = self._upload_downloaded_data(download_results['success'])
        
        # çµæœã‚µãƒãƒªãƒ¼ä½œæˆ
        summary = self._create_summary(download_results, upload_results)
        logger.info(f"ETL pipeline completed: {summary}")
        
        return {
            'download_results': download_results,
            'upload_results': upload_results,
            'summary': summary
        }
    
    def run_etl_for_date(self, date_str):
        """
        ç‰¹å®šæ—¥ã§ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        
        Args:
            date_str (str): æ—¥ä»˜æ–‡å­—åˆ— (YYYYMMDDå½¢å¼)
            
        Returns:
            dict: å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼
        """
        logger.info(f"Starting ETL pipeline for date {date_str}")
        
        # Phase 1: Extract
        logger.info("Phase 1: Extracting data from TEPCO website")
        download_results = self.downloader.download_for_date(date_str)
        
        if not download_results['success']:
            logger.warning(f"No data downloaded for date {date_str}. Skipping upload phase.")
            return {
                'download_results': download_results,
                'upload_results': {'success': [], 'failed': []},
                'summary': f'Failed: No data downloaded for {date_str}'
            }
        
        # Phase 2: Load
        logger.info("Phase 2: Loading data to Google Cloud Storage")
        upload_results = self._upload_downloaded_data(download_results['success'])
        
        # çµæœã‚µãƒãƒªãƒ¼ä½œæˆ
        summary = self._create_summary(download_results, upload_results)
        logger.info(f"ETL pipeline completed: {summary}")
        
        return {
            'download_results': download_results,
            'upload_results': upload_results,
            'summary': summary
        }
    
    def _upload_downloaded_data(self, successful_months):
        """
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        
        Args:
            successful_months (list): ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸã—ãŸæœˆã®ãƒªã‚¹ãƒˆ
            
        Returns:
            dict: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
        """
        upload_results = {'success': [], 'failed': []}
        
        for month in successful_months:
            try:
                month_dir = self.base_dir / month
                
                # CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
                csv_files = list(month_dir.glob("*.csv"))
                
                if not csv_files:
                    logger.warning(f"No CSV files found in {month_dir}")
                    upload_results['failed'].append({
                        'month': month,
                        'error': 'No CSV files found'
                    })
                    continue
                
                # å„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                month_success = True
                for csv_file in csv_files:
                    try:
                        gcs_file_name = f"raw_data/{month}/{csv_file.name}"
                        uri = self.uploader.upload_file(str(csv_file), gcs_file_name)
                        logger.info(f"Successfully uploaded {csv_file.name} to {uri}")
                    except Exception as e:
                        logger.error(f"Failed to upload {csv_file}: {e}")
                        month_success = False
                
                if month_success:
                    upload_results['success'].append(month)
                else:
                    upload_results['failed'].append({
                        'month': month,
                        'error': 'One or more CSV files failed to upload'
                    })
                    
            except Exception as e:
                logger.error(f"Failed to process month {month}: {e}")
                upload_results['failed'].append({
                    'month': month,
                    'error': str(e)
                })
        
        return upload_results
    
    def _create_summary(self, download_results, upload_results):
        """
        å®Ÿè¡Œçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        
        Args:
            download_results (dict): ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
            upload_results (dict): ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
            
        Returns:
            str: ã‚µãƒãƒªãƒ¼æ–‡å­—åˆ—
        """
        download_success = len(download_results['success'])
        download_failed = len(download_results['failed'])
        upload_success = len(upload_results['success'])
        upload_failed = len(upload_results['failed'])
        
        if download_failed == 0 and upload_failed == 0:
            return f"Success: {download_success} months downloaded and uploaded"
        elif download_failed > 0 and upload_failed == 0:
            return f"Partial Success: {download_success} downloaded, {download_failed} download failed"
        elif download_failed == 0 and upload_failed > 0:
            return f"Partial Success: {download_success} downloaded, {upload_failed} upload failed"
        else:
            return f"Partial Success: {download_success} completed, {download_failed} download failed, {upload_failed} upload failed"


def print_results(results):
    """å®Ÿè¡Œçµæœã‚’è¡¨ç¤º"""
    print(f"\n{'='*60}")
    print("ğŸ“Š ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œçµæœ")
    print('='*60)
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
    download_results = results['download_results']
    print("\nğŸ“¥ ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ:")
    if download_results['success']:
        print(f"  âœ… æˆåŠŸ: {', '.join(download_results['success'])}")
    if download_results['failed']:
        print(f"  âŒ å¤±æ•—: {', '.join(download_results['failed'])}")
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ
    upload_results = results['upload_results']
    print("\nğŸ“¤ GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ:")
    if upload_results['success']:
        print(f"  âœ… æˆåŠŸ: {', '.join(upload_results['success'])}")
    if upload_results['failed']:
        print("  âŒ å¤±æ•—:")
        for item in upload_results['failed']:
            if isinstance(item, dict):
                print(f"    {item['month']}: {item['error']}")
            else:
                print(f"    {item}")
    
    # ã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“ˆ ç·åˆçµæœ: {results['summary']}")
    print('='*60)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    # ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–
    setup_logging()
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®base_dirã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    energy_env_path = os.getenv('ENERGY_ENV_PATH')
    if energy_env_path is None:
        default_base_dir = 'data/raw'  # ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print("âš ï¸  è­¦å‘Š: ENERGY_ENV_PATHç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç›¸å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    else:
        default_base_dir = os.path.join(energy_env_path, 'data', 'raw')
    
    parser = argparse.ArgumentParser(description='ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³')
    parser.add_argument('--days', type=int, default=5,
                       help='ä»Šæ—¥ã‹ã‚‰é¡ã‚‹æ—¥æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)')
    parser.add_argument('--month', type=str,
                       help='æŒ‡å®šæœˆã‚’å‡¦ç† (YYYYMMå½¢å¼)')
    parser.add_argument('--date', type=str,
                       help='ç‰¹å®šæ—¥ã‚’å‡¦ç† (YYYYMMDDå½¢å¼)')
    parser.add_argument('--base-dir', type=str, default=default_base_dir,
                       help=f'ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {default_base_dir})')
    parser.add_argument('--bucket', type=str, default='energy-env-data',
                       help='GCSãƒã‚±ãƒƒãƒˆå (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: energy-env-data)')
    
    args = parser.parse_args()
    
    # å¼•æ•°ã®æ’ä»–ãƒã‚§ãƒƒã‚¯
    specified_args = [
        bool(args.month),
        bool(args.date),
        args.days != 5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä»¥å¤–ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
    ]
    
    if sum(specified_args) > 1:
        print("âŒ ã‚¨ãƒ©ãƒ¼: --days, --month, --date ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
        print("   1ã¤ã®å®Ÿè¡Œã§1ã¤ã®å‡¦ç†ã®ã¿å¯èƒ½ã§ã™")
        return
    
    # ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    try:
        pipeline = MainETLPipeline(args.base_dir, args.bucket)
    except ValueError as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print("   ENERGY_ENV_PATHç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return
    
    print("ğŸš€ ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
    print(f"ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆ: {pipeline.base_dir}")
    print(f"â˜ï¸  GCSãƒã‚±ãƒƒãƒˆ: gs://{pipeline.bucket_name}")
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ¤å®šã¨å‡¦ç†å®Ÿè¡Œ
    try:
        if args.month:
            print(f"ğŸ“… æŒ‡å®šæœˆãƒ¢ãƒ¼ãƒ‰: {args.month}")
            results = pipeline.run_etl_for_month(args.month)
        elif args.date:
            print(f"ğŸ“… ç‰¹å®šæ—¥ãƒ¢ãƒ¼ãƒ‰: {args.date}")
            results = pipeline.run_etl_for_date(args.date)
        else:
            print(f"ğŸ“… æ—¥æ•°æŒ‡å®šãƒ¢ãƒ¼ãƒ‰: éå»{args.days}æ—¥åˆ†")
            results = pipeline.run_etl_for_days(args.days)
        
        # çµæœè¡¨ç¤º
        print_results(results)
        
    except Exception as e:
        logger.error(f"ETL pipeline failed: {e}")
        print(f"ğŸ’¥ ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    print("ğŸ ãƒ¡ã‚¤ãƒ³ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")


if __name__ == "__main__":
    main()