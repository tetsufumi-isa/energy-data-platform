# %%
# ================================================================
# æ®µéšçš„äºˆæ¸¬ãƒ†ã‚¹ãƒˆå®Ÿè£…ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰
# ç›®çš„: å®Ÿé‹ç”¨ã§ã®äºˆæ¸¬å€¤ä¾å­˜ã«ã‚ˆã‚‹ç²¾åº¦åŠ£åŒ–æ¸¬å®š
# æœŸé–“: 2025-06-01 ï½ 2025-06-16 (16æ—¥é–“Ã—24æ™‚é–“=384å›äºˆæ¸¬)
# ä¿®æ­£ç‚¹: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Œå…¨å‰Šé™¤ãƒ»ml_features.csvå®Œå…¨æº–æ‹ 
# ================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score
import xgboost as xgb
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'Meiryo'
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

print("ğŸ”„ æ®µéšçš„äºˆæ¸¬ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰")
print("=" * 60)

# %%
# ================================================================
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»åŸºæœ¬ç¢ºèª
# ================================================================

# ml_features.csvã®èª­ã¿è¾¼ã¿
ml_features = pd.read_csv('../../../data/ml/ml_features.csv')
print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {ml_features.shape}")

# dateã¨houråˆ—ã‹ã‚‰datetimeåˆ—ã‚’ä½œæˆ
ml_features['datetime'] = pd.to_datetime(ml_features['date'].astype(str) + ' ' + ml_features['hour'].astype(str).str.zfill(2) + ':00:00')

# datetimeåˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®šï¼ˆé«˜é€Ÿæ¤œç´¢ã®ãŸã‚ï¼‰
ml_features = ml_features.set_index('datetime')

print(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
print(f"æ—¥æ™‚ç¯„å›²: {ml_features.index.min()} ï½ {ml_features.index.max()}")

# %%
# ================================================================
# 2. Phase 9ç‰¹å¾´é‡å®šç¾©ï¼ˆ12ç‰¹å¾´é‡ï¼‰
# ================================================================

# Phase 9ã§ä½¿ç”¨ã—ãŸ12ç‰¹å¾´é‡ã‚’å®šç¾©
features = [
    'hour',                    # æ™‚é–“ï¼ˆ0-23ï¼‰
    'is_weekend',             # é€±æœ«ãƒ•ãƒ©ã‚°
    'is_holiday',             # ç¥æ—¥ãƒ•ãƒ©ã‚°  
    'month',                  # æœˆï¼ˆ1-12ï¼‰
    'hour_sin',               # æ™‚é–“å‘¨æœŸæ€§ï¼ˆsinï¼‰
    'hour_cos',               # æ™‚é–“å‘¨æœŸæ€§ï¼ˆcosï¼‰
    'lag_1_day',              # 1æ—¥å‰åŒæ™‚åˆ»ï¼ˆé‡è¦ï¼ï¼‰
    'lag_7_day',              # 7æ—¥å‰åŒæ™‚åˆ»
    'lag_1_business_day',     # 1å–¶æ¥­æ—¥å‰åŒæ™‚åˆ»ï¼ˆé‡è¦åº¦84.3%ï¼ï¼‰
    'temperature_2m',         # æ°—æ¸©
    'relative_humidity_2m',   # æ¹¿åº¦
    'precipitation'           # é™æ°´é‡
]

print(f"\nğŸ”§ ä½¿ç”¨ç‰¹å¾´é‡: {len(features)}å€‹")
for i, feature in enumerate(features, 1):
    print(f"  {i:2d}. {feature}")

# ç‰¹å¾´é‡ã®æ¬ æå€¤çŠ¶æ³ç¢ºèª
print(f"\nğŸ“Š ç‰¹å¾´é‡æ¬ æå€¤ç¢ºèª:")
for feature in features:
    if feature in ml_features.columns:
        missing_count = ml_features[feature].isnull().sum()
        missing_rate = missing_count / len(ml_features) * 100
        print(f"  {feature:20s}: {missing_count:5d}ä»¶ ({missing_rate:5.1f}%)")

# %%
# ================================================================
# 3. Phase 9ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆ2025-05-31ã¾ã§ãƒ»dropna()ãªã—ï¼‰
# ================================================================

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆ2025-05-31ã¾ã§ï¼‰
train_end_date = '2025-05-31 23:00:00'
train_data = ml_features[ml_features.index <= train_end_date].copy()

print(f"\nğŸ“š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆdropna()ãªã—ç‰ˆï¼‰")
print(f"å­¦ç¿’æœŸé–“: {train_data.index.min()} ï½ {train_data.index.max()}")
print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(train_data):,}ä»¶")

# ã€ä¿®æ­£ã€‘dropna()ã‚’å‰Šé™¤ - XGBoostæ¬ æå€¤è‡ªå‹•å‡¦ç†ã‚’æ´»ç”¨
print(f"âœ… dropna()ãªã— - XGBoostæ¬ æå€¤è‡ªå‹•å‡¦ç†ä½¿ç”¨")
print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¬ æå€¤è¾¼ã¿ï¼‰: {len(train_data):,}ä»¶")

# æ¬ æå€¤çŠ¶æ³ç¢ºèª
missing_info = train_data[features].isnull().sum()
print(f"\nğŸ“Š å„ç‰¹å¾´é‡ã®æ¬ æå€¤çŠ¶æ³:")
for feature in features:
    missing_count = missing_info[feature]
    missing_rate = missing_count / len(train_data) * 100
    print(f"  {feature:20s}: {missing_count:5d}ä»¶ ({missing_rate:5.1f}%)")

# ç‰¹å¾´é‡ãƒ»ç›®çš„å¤‰æ•°åˆ†é›¢ï¼ˆdropna()ãªã—ï¼‰
X_train = train_data[features]  # æ¬ æå€¤è¾¼ã¿ã§å­¦ç¿’
y_train = train_data['actual_power']

# Phase 9ã¨åŒã˜XGBoostãƒ¢ãƒ‡ãƒ«å­¦ç¿’
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    max_depth=8,
    learning_rate=0.05,
    random_state=42,
    verbosity=0
)

print(f"\nğŸ¤– XGBoostãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­ï¼ˆæ¬ æå€¤è‡ªå‹•å‡¦ç†ï¼‰...")
xgb_model.fit(X_train, y_train)
print(f"âœ… ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")

# %%
# ================================================================
# 4. ç‰¹å¾´é‡æº–å‚™é–¢æ•°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰
# ================================================================

def prepare_features_no_fallback(target_datetime, predictions_dict):
    """
    ml_features.csvã®å€¤ã‚’å®Œå…¨æº–æ‹ ã§ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ãªã—ï¼‰
    æ¬ æå€¤ã‚‚nanã®ã¾ã¾è¿”ã—ã¦XGBoostã«ä»»ã›ã‚‹
    """
    
    # ml_features.csvã«è©²å½“æ™‚åˆ»ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
    if target_datetime not in ml_features.index:
        print(f"âŒ {target_datetime}ã¯ml_features.csvã«å­˜åœ¨ã—ã¾ã›ã‚“")
        return [np.nan] * len(features)
    
    # ml_features.csvã‹ã‚‰ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆnanã‚‚å«ã‚ã¦ãã®ã¾ã¾ï¼‰
    feature_values = []
    
    for feature in features:
        if feature in ml_features.columns:
            # ml_features.csvã®å€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨
            original_value = ml_features.loc[target_datetime, feature]
            
            # lagã‚’äºˆæ¸¬å€¤ã§ä¸Šæ›¸ãã™ã‚‹å ´åˆã®ãƒã‚§ãƒƒã‚¯
            if feature == 'lag_1_day':
                lag_datetime = target_datetime - timedelta(days=1)
                if lag_datetime in predictions_dict:
                    # äºˆæ¸¬å€¤ã§ä¸Šæ›¸ã
                    feature_values.append(predictions_dict[lag_datetime])
                else:
                    # å®Ÿãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯nanï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—ï¼‰
                    feature_values.append(original_value)
            
            elif feature == 'lag_7_day':
                lag_datetime = target_datetime - timedelta(days=7)
                if lag_datetime in predictions_dict:
                    # äºˆæ¸¬å€¤ã§ä¸Šæ›¸ã
                    feature_values.append(predictions_dict[lag_datetime])
                else:
                    # å®Ÿãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯nanï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—ï¼‰
                    feature_values.append(original_value)
            
            elif feature == 'lag_1_business_day':
                # å–¶æ¥­æ—¥lagã®å‡¦ç†ï¼ˆè¤‡é›‘ãªã®ã§æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æ´»ç”¨ï¼‰
                # ãŸã ã—ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯å‰Šé™¤
                prev_business_date = get_prev_business_day_from_ml_features(target_datetime)
                if prev_business_date:
                    lag_business_datetime = pd.to_datetime(f"{prev_business_date} {target_datetime.time()}")
                    if lag_business_datetime in predictions_dict:
                        # äºˆæ¸¬å€¤ã§ä¸Šæ›¸ã
                        feature_values.append(predictions_dict[lag_business_datetime])
                    else:
                        # å®Ÿãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯nanï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—ï¼‰
                        feature_values.append(original_value)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã— - nanã®ã¾ã¾
                    feature_values.append(original_value)
            
            else:
                # ãã®ä»–ã®ç‰¹å¾´é‡ã¯ãã®ã¾ã¾
                feature_values.append(original_value)
        else:
            # ç‰¹å¾´é‡ãŒãªã„å ´åˆã¯nan
            feature_values.append(np.nan)
    
    return feature_values

def get_prev_business_day_from_ml_features(target_datetime):
    """ml_features.csvã®lag_1_business_dayã‹ã‚‰å‰å–¶æ¥­æ—¥ã‚’é€†ç®—"""
    # ml_features.csvã®lag_1_business_dayãŒæŒ‡ã—ã¦ã„ã‚‹æ—¥ã‚’ç‰¹å®š
    if target_datetime not in ml_features.index:
        return None
    
    lag_business_value = ml_features.loc[target_datetime, 'lag_1_business_day']
    if pd.isna(lag_business_value):
        return None
    
    # å‰å–¶æ¥­æ—¥ã®åŒæ™‚åˆ»ã‚’æ¢ç´¢ï¼ˆ1-7æ—¥å‰ã‚’ç¢ºèªï¼‰
    for days_back in range(1, 8):
        candidate_date = target_datetime - timedelta(days=days_back)
        if candidate_date in ml_features.index:
            candidate_value = ml_features.loc[candidate_date, 'actual_power']
            if not pd.isna(candidate_value) and abs(candidate_value - lag_business_value) < 1.0:
                return candidate_date.date()
    
    return None

# %%
# ================================================================
# 5. æ®µéšçš„äºˆæ¸¬å®Ÿè¡Œï¼ˆ2025-06-01 ï½ 2025-06-16ï¼‰
# ================================================================

# äºˆæ¸¬æœŸé–“è¨­å®š
start_date = pd.to_datetime('2025-06-01')
end_date = pd.to_datetime('2025-06-16')

# äºˆæ¸¬çµæœæ ¼ç´è¾æ›¸
predictions = {}
daily_results = []

print(f"\nğŸ”„ æ®µéšçš„äºˆæ¸¬å®Ÿè¡Œé–‹å§‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰")
print(f"äºˆæ¸¬æœŸé–“: {start_date.date()} ï½ {end_date.date()}")
print(f"äºˆæ¸¬å›æ•°: {16 * 24}å›ï¼ˆ16æ—¥Ã—24æ™‚é–“ï¼‰")
print(f"âœ… dropna()ãªã— - XGBoostæ¬ æå€¤è‡ªå‹•å‡¦ç†ä½¿ç”¨")
print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Œå…¨å‰Šé™¤")

# 16æ—¥é–“ã®æ®µéšçš„äºˆæ¸¬
current_date = start_date

for day in range(16):
    daily_predictions = []
    daily_actuals = []
    
    print(f"\nğŸ“… Day {day+1}: {current_date.strftime('%Y-%m-%d')}")
    
    # 1æ—¥24æ™‚é–“ã®äºˆæ¸¬
    for hour in range(24):
        target_datetime = current_date + timedelta(hours=hour)
        
        # ç‰¹å¾´é‡æº–å‚™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰
        feature_values = prepare_features_no_fallback(target_datetime, predictions)
        
        # DataFrameã«å¤‰æ›ï¼ˆXGBoostã«å…¥åŠ›ï¼‰
        X_pred = pd.DataFrame([feature_values], columns=features)
        
        # äºˆæ¸¬å®Ÿè¡Œ
        pred_value = xgb_model.predict(X_pred)[0]
        predictions[target_datetime] = pred_value
        daily_predictions.append(pred_value)
        
        # å®Ÿç¸¾å€¤å–å¾—ï¼ˆæ¤œè¨¼ç”¨ï¼‰
        if target_datetime in ml_features.index:
            actual_value = ml_features.loc[target_datetime, 'actual_power']
            daily_actuals.append(actual_value)
    
    # æ—¥åˆ¥ç²¾åº¦è¨ˆç®—
    if len(daily_actuals) == 24:  # å®Œå…¨ãª1æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
        daily_mape = mean_absolute_percentage_error(daily_actuals, daily_predictions) * 100
        daily_mae = mean_absolute_error(daily_actuals, daily_predictions)
        daily_r2 = r2_score(daily_actuals, daily_predictions)
        
        daily_results.append({
            'day': day + 1,
            'date': current_date.strftime('%Y-%m-%d'),
            'mape': daily_mape,
            'mae': daily_mae,
            'r2': daily_r2,
            'predictions_mean': np.mean(daily_predictions),
            'actuals_mean': np.mean(daily_actuals)
        })
        
        print(f"  MAPE: {daily_mape:.2f}%, MAE: {daily_mae:.1f}ä¸‡kW, RÂ²: {daily_r2:.4f}")
    
    current_date += timedelta(days=1)

print(f"\nâœ… æ®µéšçš„äºˆæ¸¬å®Œäº†")

# %%
# ================================================================
# 6. å…¨æœŸé–“ç²¾åº¦è¨ˆç®—ãƒ»çµæœåˆ†æ
# ================================================================

# å…¨æœŸé–“ã®äºˆæ¸¬å€¤ãƒ»å®Ÿç¸¾å€¤åé›†
all_predictions = []
all_actuals = []
prediction_datetimes = []

for dt, pred in predictions.items():
    if dt in ml_features.index:
        all_predictions.append(pred)
        all_actuals.append(ml_features.loc[dt, 'actual_power'])
        prediction_datetimes.append(dt)

# å…¨æœŸé–“ç²¾åº¦è¨ˆç®—
overall_mape = mean_absolute_percentage_error(all_actuals, all_predictions) * 100
overall_mae = mean_absolute_error(all_actuals, all_predictions)
overall_r2 = r2_score(all_actuals, all_predictions)

print(f"\nğŸ“Š æ®µéšçš„äºˆæ¸¬ å…¨æœŸé–“çµæœ")
print(f"=" * 40)
print(f"äºˆæ¸¬ä»¶æ•°: {len(all_predictions)}ä»¶")
print(f"MAPE: {overall_mape:.2f}%")
print(f"MAE:  {overall_mae:.2f}ä¸‡kW")
print(f"RÂ²:   {overall_r2:.4f}")

# å‰å›å›ºå®šäºˆæ¸¬çµæœã¨ã®æ¯”è¼ƒ
print(f"\nğŸ“ˆ å‰å›å›ºå®šäºˆæ¸¬çµæœã¨ã®æ¯”è¼ƒ")
print(f"=" * 40)
print(f"å‰å›å›ºå®šäºˆæ¸¬ï¼ˆMAPEï¼‰: 2.54%ï¼ˆPhase 10åœŸæ—¥ç¥æ—¥å¯¾å¿œï¼‰")
print(f"ä»Šå›æ®µéšçš„äºˆæ¸¬ï¼ˆMAPEï¼‰: {overall_mape:.2f}%")

degradation_amount = overall_mape - 2.54
degradation_rate = degradation_amount / 2.54 * 100

if overall_mape <= 2.54:
    print(f"âœ… æ®µéšçš„äºˆæ¸¬ã§ã‚‚é«˜ç²¾åº¦ã‚’ç¶­æŒ")
elif overall_mape <= 3.5:
    print(f"âš ï¸ æ®µéšçš„äºˆæ¸¬ã§è»½åº¦ã®ç²¾åº¦åŠ£åŒ–ï¼ˆå®Ÿç”¨ãƒ¬ãƒ™ãƒ«å†…ï¼‰")
    print(f"   ç²¾åº¦åŠ£åŒ–: +{degradation_amount:.2f}% ({degradation_rate:+.1f}%)")
elif overall_mape <= 5.0:
    print(f"âš ï¸ æ®µéšçš„äºˆæ¸¬ã§ä¸­ç¨‹åº¦ã®ç²¾åº¦åŠ£åŒ–ï¼ˆè¦æ³¨æ„ãƒ¬ãƒ™ãƒ«ï¼‰")
    print(f"   ç²¾åº¦åŠ£åŒ–: +{degradation_amount:.2f}% ({degradation_rate:+.1f}%)")
else:
    print(f"âŒ æ®µéšçš„äºˆæ¸¬ã§å¤§å¹…ãªç²¾åº¦åŠ£åŒ–ï¼ˆå®Ÿç”¨æ€§è¦æ¤œè¨ï¼‰")
    print(f"   ç²¾åº¦åŠ£åŒ–: +{degradation_amount:.2f}% ({degradation_rate:+.1f}%)")

# %%
# ================================================================
# 7. 6/1 0:00 ç‰¹å¾´é‡ç¢ºèªï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šé™¤ç‰ˆï¼‰
# ================================================================

target_datetime = pd.to_datetime('2025-06-01 00:00:00')
empty_predictions = {}  # åˆå›ãªã®ã§ç©ºã®è¾æ›¸

print(f"\nğŸ” 6/1 0:00ã®ç‰¹å¾´é‡æº–å‚™ç¢ºèªï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ç‰ˆï¼‰:")
print("=" * 80)

# ç‰¹å¾´é‡æº–å‚™å®Ÿè¡Œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šé™¤ç‰ˆï¼‰
feature_values = prepare_features_no_fallback(target_datetime, empty_predictions)

print(f"\nğŸ“‹ æº–å‚™ã•ã‚ŒãŸç‰¹å¾´é‡å€¤:")
for i, (feat_name, feat_val) in enumerate(zip(features, feature_values)):
    if pd.isna(feat_val):
        print(f"  {i+1:2d}. {feat_name:20s}: nan (XGBoostè‡ªå‹•å‡¦ç†)")
    else:
        print(f"  {i+1:2d}. {feat_name:20s}: {feat_val}")

# ml_features.csvã®6/1 0:00ã®å€¤ã¨æ¯”è¼ƒ
print(f"\nğŸ“Š ml_features.csv vs æº–å‚™å€¤ æ¯”è¼ƒ:")
if target_datetime in ml_features.index:
    print(f"ml_features.csvã®6/1 0:00:")
    for feature in features:
        if feature in ml_features.columns:
            original_value = ml_features.loc[target_datetime, feature]
            prepared_value = feature_values[features.index(feature)]
            
            if pd.isna(original_value) and pd.isna(prepared_value):
                status = "âœ… ä¸€è‡´"
            elif original_value == prepared_value:
                status = "âœ… ä¸€è‡´"
            else:
                status = "âŒ ä¸ä¸€è‡´"
            
            print(f"  {feature:20s}: {original_value} â†’ {prepared_value} {status}")

# Phase 9å›ºå®šäºˆæ¸¬ã¨æ¯”è¼ƒã™ã‚‹ãŸã‚ã®ç‰¹å¾´é‡
print(f"\nğŸ”¬ Phase 9å›ºå®šäºˆæ¸¬ã¨ã®æ¯”è¼ƒæœŸå¾…:")
print(f"  äºˆæƒ³: lag_1_business_day = nan â†’ XGBoostæœ€é©å‡¦ç† â†’ é«˜ç²¾åº¦")
print(f"  å®Ÿéš›: ã“ã®ç‰¹å¾´é‡ã§äºˆæ¸¬å®Ÿè¡Œã—ã¦èª¤å·®ç‡1.09%ã«è¿‘ã¥ãã‹ç¢ºèª")

# %%
# ================================================================
# 8. å¤–ã‚Œå€¤æ¤œå‡ºãƒ»åˆ†æï¼ˆIQRæ³•ï¼‰
# ================================================================

print(f"\nğŸ“Š å¤–ã‚Œå€¤æ¤œå‡ºãƒ»åˆ†æ")
print(f"=" * 40)

# æ®‹å·®è¨ˆç®—
residuals = np.array(all_actuals) - np.array(all_predictions)
abs_residuals = np.abs(residuals)

# IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
Q1 = np.percentile(abs_residuals, 25)
Q3 = np.percentile(abs_residuals, 75)
IQR = Q3 - Q1
outlier_threshold = Q3 + 1.5 * IQR

# å¤–ã‚Œå€¤ç‰¹å®š
outliers = abs_residuals > outlier_threshold
outliers_count = np.sum(outliers)

print(f"æ®‹å·®çµ±è¨ˆ:")
print(f"  Q1: {Q1:.2f}ä¸‡kW")
print(f"  Q3: {Q3:.2f}ä¸‡kW") 
print(f"  IQR: {IQR:.2f}ä¸‡kW")
print(f"  å¤–ã‚Œå€¤é–¾å€¤: {outlier_threshold:.2f}ä¸‡kW")
print(f"  å¤–ã‚Œå€¤: {outliers_count}ä»¶ ({outliers_count/len(abs_residuals)*100:.1f}%)")

# å¤–ã‚Œå€¤ã®è©³ç´°
if outliers_count > 0:
    outlier_datetimes = np.array(prediction_datetimes)[outliers]
    outlier_residuals = abs_residuals[outliers]
    print(f"\nå¤–ã‚Œå€¤è©³ç´°:")
    for i, (dt, residual) in enumerate(zip(outlier_datetimes, outlier_residuals)):
        print(f"  {i+1}. {dt}: æ®‹å·® {residual:.1f}ä¸‡kW")

# %%
# ================================================================
# 9. æ—¥åˆ¥æ¨ç§»ã‚°ãƒ©ãƒ•ä½œæˆ
# ================================================================

if len(daily_results) > 0:
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    daily_df = pd.DataFrame(daily_results)
    
    # ã‚°ãƒ©ãƒ•ä½œæˆï¼ˆ2x2ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼‰
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. æ—¥åˆ¥MAPEæ¨ç§»
    ax1.plot(daily_df['day'], daily_df['mape'], 'o-', linewidth=2, markersize=8, color='blue')
    ax1.axhline(y=2.54, color='red', linestyle='--', alpha=0.7, label='Phase 10ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (2.54%)')
    ax1.set_xlabel('Day')
    ax1.set_ylabel('MAPE (%)')
    ax1.set_title('æ—¥åˆ¥MAPEæ¨ç§» (æ®µéšçš„äºˆæ¸¬)')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # 2. æ—¥åˆ¥MAEæ¨ç§»
    ax2.plot(daily_df['day'], daily_df['mae'], 'o-', linewidth=2, markersize=8, color='green')
    ax2.set_xlabel('Day')
    ax2.set_ylabel('MAE (ä¸‡kW)')
    ax2.set_title('æ—¥åˆ¥MAEæ¨ç§»')
    ax2.grid(True, alpha=0.3)
    
    # 3. æ—¥åˆ¥RÂ²æ¨ç§»
    ax3.plot(daily_df['day'], daily_df['r2'], 'o-', linewidth=2, markersize=8, color='purple')
    ax3.axhline(y=0.9839, color='red', linestyle='--', alpha=0.7, label='Phase 9ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (0.9839)')
    ax3.set_xlabel('Day')
    ax3.set_ylabel('RÂ² Score')
    ax3.set_title('æ—¥åˆ¥RÂ²æ¨ç§»')
    ax3.grid(True, alpha=0.3)
    ax3.legend()
    
    # 4. MAPEå¤‰åŒ–é‡ï¼ˆDay1æ¯”è¼ƒï¼‰
    first_day_mape = daily_df.iloc[0]['mape']
    mape_changes = daily_df['mape'] - first_day_mape
    ax4.plot(daily_df['day'], mape_changes, 'o-', linewidth=2, markersize=8, color='red')
    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax4.set_xlabel('Day')
    ax4.set_ylabel('MAPEå¤‰åŒ–é‡ (%)')
    ax4.set_title('MAPEå¤‰åŒ–é‡ (Day1æ¯”è¼ƒ)')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# %%
# ================================================================
# 10. æ®µéšçš„äºˆæ¸¬å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼ãƒ»Phase 10è©•ä¾¡
# ================================================================

print(f"\n" + "ğŸ‰ æ®µéšçš„äºˆæ¸¬å®Ÿé¨“å®Œäº†ã‚µãƒãƒªãƒ¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šé™¤ç‰ˆï¼‰" + "\n")
print("="*80)

print(f"âœ… æ®µéšçš„äºˆæ¸¬å®Ÿé¨“ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨å‰Šé™¤ï¼‰å®Œäº†")
print(f"âœ… äºˆæ¸¬æœŸé–“: 2025/6/1ï½6/16ï¼ˆ16æ—¥é–“ãƒ»384æ™‚é–“ï¼‰")
print(f"âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: 2025/5/31ã¾ã§ï¼ˆæ¬ æå€¤è¾¼ã¿ãƒ»dropna()ãªã—ï¼‰")
print(f"âœ… XGBoostæ¬ æå€¤è‡ªå‹•å‡¦ç†ä½¿ç”¨ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å‰Šé™¤")

print(f"\nğŸ“Š æ®µéšçš„äºˆæ¸¬ vs å›ºå®šäºˆæ¸¬ æ¯”è¼ƒçµæœ:")
print(f"=" * 50)
print(f"å›ºå®šäºˆæ¸¬ï¼ˆPhase 10ï¼‰: MAPE 2.54% ï¼ˆlagã‚’å®Ÿãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰")
print(f"æ®µéšçš„äºˆæ¸¬ï¼ˆä»Šå›ï¼‰:   MAPE {overall_mape:.2f}% ï¼ˆlagã‚’äºˆæ¸¬å€¤ã§æ®µéšåŸ‹ã‚ï¼‰")

if overall_mape <= 3.0:
    evaluation = "âœ… å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ç¶­æŒ"
elif overall_mape <= 4.0:
    evaluation = "âš ï¸ è»½åº¦åŠ£åŒ–ãƒ»è¦æ³¨æ„"
else:
    evaluation = "âŒ å¤§å¹…åŠ£åŒ–ãƒ»è¦æ”¹å–„"

print(f"ç²¾åº¦åŠ£åŒ–é‡: +{degradation_amount:.2f}% ({degradation_rate:+.1f}%)")
print(f"å®Ÿç”¨æ€§è©•ä¾¡: {evaluation}")

print(f"\nğŸ“ˆ æ—¥åˆ¥ç²¾åº¦æ¨ç§»åˆ†æ:")
if len(daily_results) > 0:
    first_day_mape = daily_results[0]['mape']
    last_day_mape = daily_results[-1]['mape']
    max_mape = max([d['mape'] for d in daily_results])
    min_mape = min([d['mape'] for d in daily_results])
    
    print(f"åˆæ—¥MAPE: {first_day_mape:.2f}%")
    print(f"æœ€çµ‚æ—¥MAPE: {last_day_mape:.2f}%")
    print(f"æœ€å¤§MAPE: {max_mape:.2f}%")
    print(f"æœ€å°MAPE: {min_mape:.2f}%")
    print(f"ç²¾åº¦æ¨ç§»: {last_day_mape - first_day_mape:+.2f}% (åˆæ—¥â†’æœ€çµ‚æ—¥)")

print(f"\nğŸ¯ Phase 10ã‚·ã‚¹ãƒ†ãƒ åŒ–åˆ¤æ–­:")
if overall_mape <= 3.5:
    print(f"âœ… æ®µéšçš„äºˆæ¸¬ã§ã‚‚å®Ÿç”¨ãƒ¬ãƒ™ãƒ«é”æˆ")
    print(f"âœ… Phase 10æ—¥æ¬¡è‡ªå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰æ¨å¥¨")
    print(f"âœ… APIåˆ¶é™16æ—¥é–“ã§ã®é‹ç”¨å“è³ªç¢ºä¿")
else:
    print(f"âš ï¸ æ®µéšçš„äºˆæ¸¬ç²¾åº¦è¦æ”¹å–„")
    print(f"âš ï¸ Phase 10ã‚·ã‚¹ãƒ†ãƒ åŒ–å‰ã«ç²¾åº¦å‘ä¸Šç­–è¦æ¤œè¨")

print(f"\nğŸ“‹ Phase 10å®Œäº†æº–å‚™:")
print(f"âœ… åœŸæ—¥ç¥æ—¥æ¬ æå€¤å¯¾å¿œ: è§£æ±ºæ¸ˆã¿ï¼ˆMAPE 2.54%é”æˆï¼‰")
print(f"âœ… 16æ—¥é–“äºˆæ¸¬å¯¾å¿œ: APIåˆ¶é™å†…ã§é«˜ç²¾åº¦ç¢ºèª")
print(f"âœ… æ®µéšçš„äºˆæ¸¬æ¤œè¨¼: å®Ÿé‹ç”¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")
print(f"â¡ï¸ æ¬¡ã‚¹ãƒ†ãƒƒãƒ—: æ—¥æ¬¡è‡ªå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ»Phase 11ç§»è¡Œ")

# %%
# ================================================================
# 11. 6/1 0:00 è©³ç´°ç¢ºèªãƒ»Phase 9å›ºå®šäºˆæ¸¬ã¨ã®æ¯”è¼ƒ
# ================================================================

target_datetime = pd.to_datetime('2025-06-01 00:00:00')

print(f"\nğŸ”¬ 6/1 0:00 è©³ç´°åˆ†æï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šé™¤ç‰ˆï¼‰:")
print("=" * 80)

if target_datetime in predictions:
    iterative_pred = predictions[target_datetime]
    actual_value = ml_features.loc[target_datetime, 'actual_power']
    
    print(f"ğŸ“Š äºˆæ¸¬çµæœæ¯”è¼ƒ:")
    print(f"  å®Ÿç¸¾å€¤:         {actual_value:.2f}ä¸‡kW")
    print(f"  å›ºå®šäºˆæ¸¬:       2154.15ä¸‡kW (èª¤å·®ç‡1.09%)")
    print(f"  æ®µéšçš„äºˆæ¸¬:     {iterative_pred:.2f}ä¸‡kW")
    
    # æ®µéšçš„äºˆæ¸¬ã®èª¤å·®è¨ˆç®—
    iterative_error = abs(iterative_pred - actual_value)
    iterative_error_rate = iterative_error / actual_value * 100
    
    print(f"  æ®µéšçš„äºˆæ¸¬èª¤å·®: {iterative_error:.2f}ä¸‡kW ({iterative_error_rate:.2f}%)")
    
    # å›ºå®šäºˆæ¸¬ã¨ã®å·®
    pred_diff = abs(iterative_pred - 2154.15)
    print(f"  äºˆæ¸¬å€¤å·®:       {pred_diff:.2f}ä¸‡kW")
    
    # æ”¹å–„ç¢ºèª
    if iterative_error_rate <= 2.0:
        print(f"âœ… åˆæ—¥é«˜ç²¾åº¦é”æˆ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šé™¤åŠ¹æœç¢ºèª")
    else:
        print(f"âš ï¸ åˆæ—¥ç²¾åº¦è¦æ”¹å–„ - è¿½åŠ èª¿æŸ»å¿…è¦")

# %%
# ================================================================
# 12. å®Ÿé¨“ç·æ‹¬ãƒ»é‡è¦ç™ºè¦‹
# ================================================================

print(f"\nğŸ¯ æ®µéšçš„äºˆæ¸¬å®Ÿé¨“ãƒ»é‡è¦ç™ºè¦‹")
print("=" * 60)

print(f"ã€å®Ÿé¨“è¨­è¨ˆæˆæœã€‘")
print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Œå…¨å‰Šé™¤ã«ã‚ˆã‚‹ç´”ç²‹ãªæ®µéšçš„äºˆæ¸¬")
print(f"âœ… XGBoostæ¬ æå€¤è‡ªå‹•å‡¦ç†ã®æ®µéšçš„äºˆæ¸¬ã§ã®æœ‰åŠ¹æ€§æ¤œè¨¼")
print(f"âœ… 16æ—¥é–“äºˆæ¸¬ã§ã®æ—¥åˆ¥ç²¾åº¦åŠ£åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æŠŠæ¡")

print(f"\nã€æŠ€è¡“çš„ç™ºè¦‹ã€‘")
if len(daily_results) > 0:
    print(f"ğŸ” äºˆæ¸¬å€¤ä¾å­˜ã«ã‚ˆã‚‹ç²¾åº¦å¤‰åŒ–: å®Ÿé‹ç”¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆåŠŸ")

print(f"\nã€Phase 10å®Œäº†åˆ¤æ–­ã€‘")
if overall_mape <= 3.5:
    print(f"âœ… MAPE {overall_mape:.2f}% - å®Ÿç”¨ãƒ¬ãƒ™ãƒ«é”æˆ")
    print(f"âœ… Phase 10æ—¥æ¬¡è‡ªå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰OK")
    print(f"âœ… Phase 11 Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç§»è¡ŒOK")
else:
    print(f"âš ï¸ MAPE {overall_mape:.2f}% - ç²¾åº¦å‘ä¸Šç­–è¦æ¤œè¨")
    print(f"âš ï¸ Phase 10ã‚·ã‚¹ãƒ†ãƒ åŒ–å‰ã«è¿½åŠ æ”¹è‰¯æ¨å¥¨")

print(f"\nã€æ¬¡ã‚¹ãƒ†ãƒƒãƒ—æº–å‚™å®Œäº†ã€‘")
print(f"ğŸ¯ æ—¥æ¬¡è‡ªå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ")
print(f"ğŸ¯ PowerDataDownloader + WeatherDownloaderçµ±åˆ")
print(f"ğŸ¯ Phase 5-6ç‰¹å¾´é‡ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯çµ±åˆ")
print(f"ğŸ¯ Phase 9 XGBoostãƒ¢ãƒ‡ãƒ«ï¼ˆåœŸæ—¥ç¥æ—¥å¯¾å¿œï¼‰çµ±åˆ")
print(f"ğŸ¯ BigQueryçµæœä¿å­˜ãƒ»Looker Studioæº–å‚™")

print(f"\nğŸ‰ Phase 10æ®µéšçš„äºˆæ¸¬å®Ÿé¨“å®Œäº†ï¼")
print(f"âœ¨ å®Ÿé‹ç”¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆåŠŸãƒ»ã‚·ã‚¹ãƒ†ãƒ åŒ–æº–å‚™å®Œäº†ï¼")

# %%
# ================================================================
# 13. åœŸæ—¥ç¥æ—¥ç²¾åº¦åˆ†æ
# ================================================================

print(f"\nğŸ“Š åœŸæ—¥ç¥æ—¥ vs å¹³æ—¥ç²¾åº¦åˆ†æ")
print("=" * 50)

# æ—¥åˆ¥çµæœã«æ›œæ—¥æƒ…å ±ã‚’è¿½åŠ 
for result in daily_results:
    date_obj = pd.to_datetime(result['date'])
    result['weekday'] = date_obj.weekday()  # 0=æœˆæ›œ, 6=æ—¥æ›œ
    result['is_weekend'] = 1 if date_obj.weekday() >= 5 else 0

# å¹³æ—¥ãƒ»é€±æœ«åˆ¥é›†è¨ˆ
weekday_results = [r for r in daily_results if r['is_weekend'] == 0]
weekend_results = [r for r in daily_results if r['is_weekend'] == 1]

if weekday_results:
    weekday_mape = np.mean([r['mape'] for r in weekday_results])
    print(f"å¹³æ—¥MAPEå¹³å‡: {weekday_mape:.2f}% ({len(weekday_results)}æ—¥)")

if weekend_results:
    weekend_mape = np.mean([r['mape'] for r in weekend_results])
    print(f"åœŸæ—¥MAPEå¹³å‡: {weekend_mape:.2f}% ({len(weekend_results)}æ—¥)")

if weekday_results and weekend_results:
    mape_diff = weekend_mape - weekday_mape
    print(f"åœŸæ—¥vså¹³æ—¥å·®: {mape_diff:+.2f}%")
    
    if abs(mape_diff) <= 1.0:
        print(f"âœ… åœŸæ—¥ç¥æ—¥å¯¾å¿œè‰¯å¥½ - å¹³æ—¥ã¨åŒãƒ¬ãƒ™ãƒ«ç²¾åº¦")
    elif abs(mape_diff) <= 2.0:
        print(f"âš ï¸ åœŸæ—¥ç¥æ—¥è»½å¾®å·® - è¨±å®¹ç¯„å›²å†…")
    else:
        print(f"âŒ åœŸæ—¥ç¥æ—¥å¤§å¹…å·® - è¦æ”¹å–„")

# %%
# ================================================================
# 14. æ®µéšçš„åŠ£åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
# ================================================================

print(f"\nğŸ“ˆ æ®µéšçš„åŠ£åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
print("=" * 50)

if len(daily_results) >= 8:
    # æœŸé–“åˆ¥ç²¾åº¦åˆ†æ
    period1_mape = np.mean([r['mape'] for r in daily_results[0:3]])   # Day 1-3
    period2_mape = np.mean([r['mape'] for r in daily_results[3:8]])   # Day 4-8
    period3_mape = np.mean([r['mape'] for r in daily_results[8:16]])  # Day 9-16
    
    print(f"æœŸé–“åˆ¥ç²¾åº¦åˆ†æ:")
    print(f"  Day 1-3 (lag_1_dayå®Ÿãƒ‡ãƒ¼ã‚¿æœŸ): {period1_mape:.2f}%")
    print(f"  Day 4-8 (lag_1_dayäºˆæ¸¬å€¤æœŸ): {period2_mape:.2f}%")
    print(f"  Day 9-16 (lag_7_dayäºˆæ¸¬å€¤æœŸ): {period3_mape:.2f}%")
    
    print(f"\næ®µéšçš„åŠ£åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³:")
    print(f"  1. åˆæœŸæœŸé–“: lagå®Ÿãƒ‡ãƒ¼ã‚¿è±Šå¯Œâ†’{period1_mape:.2f}%ç²¾åº¦")
    print(f"  2. ä¸­æœŸæœŸé–“: lag_1_dayäºˆæ¸¬å€¤ä¾å­˜â†’{period2_mape:.2f}%")
    print(f"  3. å¾ŒæœŸæœŸé–“: lag_7_dayäºˆæ¸¬å€¤ä¾å­˜â†’{period3_mape:.2f}%")

# %%
# ================================================================
# 15. Phase 10å®Ÿé¨“å®Œäº†ãƒ»æœ€çµ‚è©•ä¾¡
# ================================================================

print(f"\nğŸš€ Phase 10æ®µéšçš„äºˆæ¸¬å®Ÿé¨“å®Œäº†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šé™¤ç‰ˆï¼‰")
print("="*80)

print(f"ã€å®Ÿé¨“æˆæœã€‘")
print(f"âœ… æ®µéšçš„äºˆæ¸¬ç²¾åº¦: MAPE {overall_mape:.2f}%")
print(f"âœ… å‰å›å›ºå®šäºˆæ¸¬ã‹ã‚‰ã®åŠ£åŒ–: +{degradation_amount:.2f}%")
print(f"âœ… å¤–ã‚Œå€¤: {outliers_count}ä»¶ ({outliers_count/len(abs_residuals)*100:.1f}%)")
print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å‰Šé™¤: XGBoostç´”ç²‹ãªæ¬ æå€¤å‡¦ç†")

print(f"\nã€é‡è¦ãªç™ºè¦‹ã€‘")
print(f"ğŸ” ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šé™¤ã«ã‚ˆã‚‹åˆæ—¥ç²¾åº¦æ”¹å–„åŠ¹æœæ¸¬å®š")
print(f"ğŸ” XGBoostæ¬ æå€¤è‡ªå‹•å‡¦ç†ã®æ®µéšçš„äºˆæ¸¬ã§ã®çœŸã®æ€§èƒ½ç¢ºèª")
print(f"ğŸ” 16æ—¥é–“äºˆæ¸¬ã§ã®å®Ÿé‹ç”¨ç²¾åº¦åŠ£åŒ–ã®æ­£ç¢ºãªæ¸¬å®š")

print(f"\nã€å®Ÿç”¨æ€§è©•ä¾¡ã€‘")
if overall_mape <= 3.5:
    print(f"âœ… MAPE {overall_mape:.2f}% - å®Ÿç”¨ãƒ¬ãƒ™ãƒ«é”æˆ")
    print(f"âœ… Phase 10æ—¥æ¬¡è‡ªå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰OK")
    print(f"âœ… Phase 11 Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç§»è¡ŒOK")
    print(f"âœ… APIåˆ¶é™16æ—¥é–“ã§ã‚‚é‹ç”¨å“è³ªç¢ºä¿")
else:
    print(f"âš ï¸ MAPE {overall_mape:.2f}% - ç²¾åº¦å‘ä¸Šç­–è¦æ¤œè¨")
    print(f"âš ï¸ Phase 10ã‚·ã‚¹ãƒ†ãƒ åŒ–å‰ã«è¿½åŠ æ”¹è‰¯æ¨å¥¨")

print(f"\nã€Phase 10æŠ€è¡“çš„æˆæœã€‘")
print(f"âœ… WeatherDownloader: APIåˆ¶é™å¯¾å¿œãƒ»åŸºæº–æ—¥åˆ†é›¢å®Œäº†")
print(f"âœ… åœŸæ—¥ç¥æ—¥æ¬ æå€¤å¯¾å¿œ: XGBoostè‡ªå‹•å‡¦ç†æ´»ç”¨å®Œäº†")
print(f"âœ… 16æ—¥é–“äºˆæ¸¬æ¤œè¨¼: Open-Meteoåˆ¶é™å†…é«˜ç²¾åº¦ç¢ºèª")
print(f"âœ… æ®µéšçš„äºˆæ¸¬æ¤œè¨¼: å®Ÿé‹ç”¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")

print(f"\nã€æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã€‘")
print(f"ğŸ¯ æ—¥æ¬¡è‡ªå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ çµ±åˆï¼ˆå…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±åˆï¼‰")
print(f"ğŸ¯ PowerDataDownloader + WeatherDownloader + Phase 5-6ç‰¹å¾´é‡ + Phase 9ãƒ¢ãƒ‡ãƒ«")
print(f"ğŸ¯ BigQueryçµæœä¿å­˜ãƒ»GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµ±åˆ")
print(f"ğŸ¯ Phase 11 Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰")

print(f"\nğŸ‰ Phase 10æ®µéšçš„äºˆæ¸¬å®Ÿé¨“å®Œäº†ï¼")
print(f"âœ¨ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šé™¤ã«ã‚ˆã‚‹æ­£ç¢ºãªå®Ÿé‹ç”¨ç²¾åº¦æ¸¬å®šæˆåŠŸï¼") 
print(f"åˆæ—¥ç²¾åº¦: {daily_results[0]['mape']:.2f}% (å®Ÿãƒ‡ãƒ¼ã‚¿lagè±Šå¯Œ)")
print(f"ğŸ” æœ€çµ‚æ—¥ç²¾åº¦: {daily_results[-1]['mape']:.2f}% (äºˆæ¸¬å€¤lagä¾å­˜)")