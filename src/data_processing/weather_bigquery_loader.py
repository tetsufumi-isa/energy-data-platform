"""
æ°—è±¡ãƒ‡ãƒ¼ã‚¿BigQueryæŠ•å…¥ã‚·ã‚¹ãƒ†ãƒ 

å®Ÿè¡Œæ–¹æ³•:
    python -m src.data_processing.weather_bigquery_loader                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: forecast
    python -m src.data_processing.weather_bigquery_loader --data-type historical  # éå»ãƒ‡ãƒ¼ã‚¿
    python -m src.data_processing.weather_bigquery_loader --data-type forecast    # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
"""

import argparse
from datetime import datetime
from pathlib import Path
from logging import getLogger
from google.cloud import bigquery
from google.cloud import storage

from src.utils.logging_config import setup_logging

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å°‚ç”¨ã®ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—
logger = getLogger('energy_env.data_processing.weather_bigquery_loader')

class WeatherBigQueryLoader:
    """æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«æŠ•å…¥ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_id="energy-env", bucket_name="energy-env-data"):
        """
        åˆæœŸåŒ–
        
        Args:
            project_id (str): GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
            bucket_name (str): GCSãƒã‚±ãƒƒãƒˆå
        """
        self.project_id = project_id
        self.bucket_name = bucket_name
        self.dataset_id = "dev_energy_data"
        self.table_id = "weather_data"
        
        # BigQueryã¨GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.bq_client = bigquery.Client(project=project_id)
        self.gcs_client = storage.Client(project=project_id)
        self.bucket = self.gcs_client.bucket(bucket_name)
        
        logger.info(f"WeatherBigQueryLoader initialized: {project_id}")
    
    def get_unprocessed_files(self, data_type="forecast"):
        """
        æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã‚’å–å¾—
        
        Args:
            data_type (str): "historical" | "forecast"
            
        Returns:
            list: æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã®GCS URIãƒªã‚¹ãƒˆ
            
        Raises:
            ValueError: CSVä»¥å¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
        """
        prefix = f"weather_processed/{data_type}/"
        
        # delimiter="/" ã§ç›´ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å–å¾—
        blobs = self.gcs_client.list_blobs(
            self.bucket_name, 
            prefix=prefix, 
            delimiter="/"
        )
        
        unprocessed_files = []
        for blob in blobs:
            # CSVä»¥å¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã‚¨ãƒ©ãƒ¼
            if not blob.name.endswith('.csv'):
                raise ValueError(f"Unexpected non-CSV file found: {blob.name}")
            
            unprocessed_files.append(f"gs://{self.bucket_name}/{blob.name}")
        
        logger.info(f"Found {len(unprocessed_files)} unprocessed files in {data_type}")
        return unprocessed_files
    
    def create_external_table(self, file_uris):
        """
        EXTERNAL TABLEã‚’ä½œæˆï¼ˆç”ŸSQLå®Ÿè¡Œï¼‰
        
        Args:
            file_uris (list): å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®GCS URIãƒªã‚¹ãƒˆ
        """
        if not file_uris:
            logger.warning("No files to process")
            return
        
        # GCS URIã‚’SQLé…åˆ—å½¢å¼ã«å¤‰æ›
        uris_str = "', '".join(file_uris)
        
        create_sql = f"""
        CREATE OR REPLACE EXTERNAL TABLE `{self.project_id}.{self.dataset_id}.temp_weather_external`
        (
            prefecture STRING,
            date STRING,
            hour STRING,
            temperature_2m FLOAT64,
            relative_humidity_2m FLOAT64,
            precipitation FLOAT64,
            weather_code INT64
        )
        OPTIONS (
            format = 'CSV',
            uris = ['{uris_str}'],
            skip_leading_rows = 1
        );
        """
        
        # ç›´æ¥SQLæ–‡ã‚’å®Ÿè¡Œ
        job = self.bq_client.query(create_sql)
        job.result()  # å®Œäº†ã¾ã§å¾…æ©Ÿ
        
        logger.info(f"Created external table: {self.project_id}.{self.dataset_id}.temp_weather_external with {len(file_uris)} files")
    
    def delete_duplicate_data(self):
        """
        é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³çµã‚Šè¾¼ã¿ + CONCATæ–¹å¼ï¼‰
        """
        delete_query = f"""
        DELETE FROM `{self.project_id}.{self.dataset_id}.{self.table_id}` 
        WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
          AND date <= CURRENT_DATE()
          AND CONCAT(prefecture, '|', CAST(date AS STRING), '|', hour) IN (
              SELECT CONCAT(prefecture, '|', CAST(PARSE_DATE('%Y-%m-%d', date) AS STRING), '|', hour)
              FROM `{self.project_id}.{self.dataset_id}.temp_weather_external`
              WHERE PARSE_DATE('%Y-%m-%d', date) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
          )
        """
        
        job = self.bq_client.query(delete_query)
        result = job.result()
        
        logger.info(f"Deleted duplicate data: {job.num_dml_affected_rows} rows")
    
    def insert_weather_data(self):
        """
        æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«æŠ•å…¥
        """
        insert_query = f"""
        INSERT INTO `{self.project_id}.{self.dataset_id}.{self.table_id}`
        (prefecture, date, hour, temperature_2m, relative_humidity_2m, precipitation, weather_code, created_at)
        SELECT 
            prefecture,
            PARSE_DATE('%Y-%m-%d', date) as date,
            hour,
            temperature_2m,
            relative_humidity_2m,
            precipitation,
            weather_code,
            CURRENT_TIMESTAMP() as created_at
        FROM `{self.project_id}.{self.dataset_id}.temp_weather_external`
        """
        
        job = self.bq_client.query(insert_query)
        result = job.result()
        
        logger.info(f"Inserted weather data: {job.num_dml_affected_rows} rows")
        return job.num_dml_affected_rows
    
    def drop_external_table(self):
        """
        EXTERNAL TABLEã‚’å‰Šé™¤
        """
        external_table_id = f"{self.project_id}.{self.dataset_id}.temp_weather_external"
        
        try:
            self.bq_client.delete_table(external_table_id)
            logger.info(f"Dropped external table: {external_table_id}")
        except Exception as e:
            logger.warning(f"Failed to drop external table: {e}")
    
    def move_processed_files(self, processed_uris, data_type="forecast"):
        """
        å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç§»å‹•
        
        Args:
            processed_uris (list): å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®GCS URIãƒªã‚¹ãƒˆ
            data_type (str): "historical" | "forecast"
        """
        moved_count = 0
        
        for uri in processed_uris:
            try:
                # GCS URIã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŠ½å‡º
                file_path = uri.replace(f"gs://{self.bucket_name}/", "")
                file_name = Path(file_path).name
                
                # ç§»å‹•å…ˆãƒ‘ã‚¹
                destination_path = f"weather_processed/{data_type}/insert_completed/{file_name}"
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
                source_blob = self.bucket.blob(file_path)
                destination_blob = self.bucket.blob(destination_path)
                
                # ã‚³ãƒ”ãƒ¼å®Ÿè¡Œ
                destination_blob.upload_from_string(source_blob.download_as_text())
                
                # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                source_blob.delete()
                
                moved_count += 1
                logger.info(f"Moved processed file: {file_name}")
                
            except Exception as e:
                logger.error(f"Failed to move file {uri}: {e}")
        
        logger.info(f"Moved {moved_count} processed files to insert_completed/")
    
    def load_weather_data(self, data_type="forecast"):
        """
        æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«æŠ•å…¥ã™ã‚‹ãƒ¡ã‚¤ãƒ³å‡¦ç†
        
        Args:
            data_type (str): "historical" | "forecast"
            
        Returns:
            dict: å‡¦ç†çµæœ
        """
        logger.info(f"Starting weather data load: {data_type}")
        
        try:
            # 1. æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            unprocessed_files = self.get_unprocessed_files(data_type)
            
            if not unprocessed_files:
                logger.info("No unprocessed files found")
                return {
                    'status': 'success',
                    'message': 'No files to process',
                    'files_processed': 0,
                    'rows_inserted': 0
                }
            
            # 2. EXTERNAL TABLEä½œæˆ
            self.create_external_table(unprocessed_files)
            
            # 3. é‡è¤‡ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
            self.delete_duplicate_data()
            
            # 4. ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
            rows_inserted = self.insert_weather_data()
            
            # 5. EXTERNAL TABLEå‰Šé™¤
            self.drop_external_table()
            
            # 6. æˆåŠŸæ™‚ã®ã¿ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•
            self.move_processed_files(unprocessed_files, data_type)
            
            logger.info(f"Weather data load completed: {len(unprocessed_files)} files, {rows_inserted} rows")
            
            return {
                'status': 'success',
                'message': f'Successfully loaded {len(unprocessed_files)} files',
                'files_processed': len(unprocessed_files),
                'rows_inserted': rows_inserted
            }
            
        except Exception as e:
            logger.error(f"Weather data load failed: {e}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯EXTERNAL TABLEã‚’å‰Šé™¤
            try:
                self.drop_external_table()
            except:
                pass
            
            return {
                'status': 'failed',
                'message': f'Load failed: {str(e)}',
                'files_processed': 0,
                'rows_inserted': 0
            }


def print_load_results(results):
    """æŠ•å…¥çµæœã‚’è¡¨ç¤º"""
    print(f"\n{'='*60}")
    print("ğŸ“Š æ°—è±¡ãƒ‡ãƒ¼ã‚¿BigQueryæŠ•å…¥çµæœ")
    print('='*60)
    
    status_emoji = {
        'success': 'âœ…',
        'failed': 'âŒ'
    }
    
    print(f"\n{status_emoji[results['status']]} å‡¦ç†çµæœ")
    print(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {results['message']}")
    print(f"å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {results['files_processed']}")
    print(f"æŠ•å…¥ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {results['rows_inserted']}")
    print('='*60)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    # ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–
    setup_logging()
    
    parser = argparse.ArgumentParser(description='æ°—è±¡ãƒ‡ãƒ¼ã‚¿BigQueryæŠ•å…¥ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--data-type', type=str, default='forecast',
                       choices=['historical', 'forecast'],
                       help='ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— (historical: éå»ãƒ‡ãƒ¼ã‚¿, forecast: äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿)')
    parser.add_argument('--project-id', type=str, default='energy-env',
                       help='GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID')
    parser.add_argument('--bucket', type=str, default='energy-env-data',
                       help='GCSãƒã‚±ãƒƒãƒˆå')
    
    args = parser.parse_args()
    
    print("ğŸš€ æ°—è±¡ãƒ‡ãƒ¼ã‚¿BigQueryæŠ•å…¥ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {args.data_type}")
    print(f"â˜ï¸  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {args.project_id}")
    print(f"ğŸ“‚ ãƒã‚±ãƒƒãƒˆ: {args.bucket}")
    
    # æŠ•å…¥å‡¦ç†å®Ÿè¡Œ
    loader = WeatherBigQueryLoader(args.project_id, args.bucket)
    results = loader.load_weather_data(args.data_type)
    
    # çµæœè¡¨ç¤º
    print_load_results(results)
    
    print("ğŸ å‡¦ç†å®Œäº†")


if __name__ == "__main__":
    main()